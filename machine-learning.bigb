= Machine learning
{wiki}

= Machine learn
{synonym}

The main reason <Ciro Santilli> never touched it is that it feels that every public data set has already been fully mined or has already had the most interesting algorithms developed for it, so you can't do much outside of big companies.

This is why Ciro started <Ciro's 2D reinforcement learning games> to generate synthetic data and thus reduce the cost of data.

The other reason is that it is ugly.

\Include[artificial-intelligence]

= Cluster analysis
{parent=Machine learning}
{wiki}

= Clustering
{synonym}

= Deepfake
{parent=Machine learning}
{wiki}

= Deepfakes
{synonym}

= Dimentionality reduction
{parent=Machine learning}
{tag=Descriptive statistics}
{wiki}

= Principal component analysis
{parent=Dimentionality reduction}
{title2=PCA}
{wiki}

Given a bunch of points in $n$ dimensions, PCA maps those points to a new $p$ dimensional space with $p \le n$.

$p$ is a <hyperparameter>, $p=1$ and $p=2$ are common choices when doing dataset exploration, as they can be easily visualized on a planar plot.

The mapping is done by projecting all points to a $p$ dimensional <hyperplane>. PCA is an algorithm for choosing this hyperplane and the coordinate system within this hyperplane.

The hyperplane choice is done as follows:
* the <hyperplane> will have origin at the <mean> point
* the first axis is picked along the direction of greatest <variance>, i.e. where points are the most spread out.

  Intuitively, if we pick an axis of small variation, that would be bad, because all the points are very close to one another on that axis, so it doesn't contain as much information that helps us differentiate the points.
* then we pick a second axis, orthogonal to the first one, and on the direction of second largest variance
* and so on until $p$ orthogonal axes are taken

https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186 provides an OK-ish example with a concrete context. In there, each point is a country, and the input data is the consumption of different kinds of foods per year, e.g.:
* flour
* dry codfish
* olive oil
* sausage
so in this example, we would have input points in 4D.

The question is then: we want to be able to identify the country by what they eat.

Suppose that every country consumes the same amount of flour every year. Then, that number doesn't tell us much about which country each point represents (has the least <variance>), and the first PCA axes would basically never point anywhere near that direction.

Another cool thing is that PCA seems to automatically account for linear dependencies in the data, so it skips selecting highly correlated axes multiple times. For example, suppose that dry codfish and olive oil consumption are very high in Portugal and Spain, but very low in Germany and Poland. Therefore, the variation is very high in those two parameters, and contains a lot of information.

However, suppose that dry codfish consumption is also directly proportional to olive oil consumption. Because of this, it would be kind of wasteful if we selected:
* dry codfish as the first axis
* olive oil as the second axis
since the information about codfish already tells us the olive oil. PCA apparently recognizes this, and instead picks the first axis at a 45 degree angle to both dry codfish and olive oil, and then moves on to something else for the second axis.

We can see that much like the rest of <machine learning>, PCA can <Machine learning as a form of data compression>[be seen as a form of compression].

= Hyperparameter
{parent=Machine learning}
{tag=AI complete}
{wiki=Hyperparameter_(machine_learning)}

A parameter that you choose which determines how the algorithm will perform.

In the case of <machine learning> in particular, it is not part of the <training data set>.

Hyperparameters can also be considered in domains outside of <machine learning> however, e.g. the step size in <partial differential equation solver> is entirely independent from the problem itself and could be considered a hyperparamter. One difference from machine learning however is that step size hyperparameters in <numerical analysis> are clearly better if smaller at a higher computational cost. In machine learning however, there is often an optimum somewhere, beyond which <overfitting> becomes excessive.

= Overfitting
{parent=Hyperparameter}
{wiki}

= Machine learning as a form of data compression
{parent=Overfitting}
{tag=Overfitting}

Philosophically, machine learning can be seen as a form of <lossy compression>.

And if we make it too <lossless>, then we are basically <overfitting>.

Bibliography:
* https://bair.berkeley.edu/blog/2019/09/19/bit-swap/
* https://www.eecs.tufts.edu/~dsculley/papers/compressionAndVectors.pdf
* https://arxiv.org/abs/2202.06533
* https://towardsdatascience.com/ai-based-image-compression-the-state-of-the-art-fb5aa6042bfa

= Natural language processing
{parent=Machine learning}
{tag=AI complete}
{wiki}

An impossible <AI complete> dream.

It is impossible to understand speech, and take meaningful actions from it, if you don't understand what is being talked about.

And without doubt, "understanding what is being talked about" comes down to understanding (efficiently representing) the geometry of the 3D world with a time component.

Not from hearing sounds alone.

= Recommender system
{parent=Machine learning}
{wiki}

* https://analyticsindiamag.com/5-open-source-recommender-systems-you-should-try-for-your-next-project/ 5 Open-Source Recommender Systems You Should Try For Your Next Project (2019)

= Supervised and unsupervised learning
{parent=Machine learning}
{wiki}

= Supervised learning
{parent=Supervised and unsupervised learning}
{wiki}

= k-nearest neighbors algorithm
{c}
{parent=Supervised learning}
{wiki}

= k-NN
{c}
{title2}
{synonym}

One of the most simply classification algorithm one can think of: just see whatever kind of point your new point seems to be closer to, and say it is also of that type! Then it is just a question of defining "close".

<Scikit-learn> implementation https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html at \a[python/sklearn/knn.py]

= Training, validation, and test data sets
{parent=Supervised learning}
{wiki}

https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set

= Training data set
{parent=Training, validation, and test data sets}

= Validation data set
{parent=Training, validation, and test data sets}

= Test data set
{parent=Training, validation, and test data sets}

= Training and inference
{parent=Supervised learning}
{wiki}

This is the first thing you have to know about <supervised learning>:
* training is when you learn model parameters from input. This literally means learning the best value we can for a bunch of number input numbers of the model. This can easily be on the hundreds of thousands.
* inference is when we take a trained model (i.e. with the parameters determined), and apply it to new inputs
Both of those already have <hardware acceleration> available as of the 2010s.

= Inference
{disambiguate=ML}
{parent=Training and inference}

= Training
{disambiguate=ML}
{parent=Training and inference}

= Unsupervised learning
{parent=Supervised and unsupervised learning}
{wiki}

= Machine learning architecture
{parent=Machine learning}

= Symbolic artificial intelligence
{parent=Machine learning architecture}
{wiki}

= Symbolic AI
{synonym}

= Neuro-symbolic AI
{c}
{parent=Symbolic artificial intelligence}

https://researcher.watson.ibm.com/researcher/view_group.php?id=10518

An <IBM> made/pushed term, but that matches <Ciro Santilli>'s general view of how we should move forward <AGI>.

Ciro's motivation/push for this can be seen e.g. at: <Ciro's 2D reinforcement learning games>.

= Neural network
{parent=Machine learning}
{wiki}

= Artificial neural network
{parent=Neural network}
{wiki}

= Trained artificial neural network
{parent=Artificial neural network}
{tag=Training (ML)}

= Deep learning
{parent=Artificial neural network}
{wiki}

Deep learning is the name <artificial neural networks> basically converged to in the 2010s/2020s.

It is a bit of an unfortunate as it suggests something like "deep understanding", but at least it sounds good.

= Backpropagation
{parent=Deep learning}
{wiki}

\Video[https://www.youtube.com/watch?v=Ilg3gGewQ5U]
{title=What is backpropagation really doing? by <3Blue1Brown> (2017)}
{description=Good <hand wave> intuition, but does not describe the exact <algorithm>.}

= Convolutional neural network
{parent=Deep learning}
{wiki}

= YOLO model
{c}
{parent=Convolutional neural network}
{wiki}

Object detection model.

You can get some really sweet pre-trained versions of this, typically trained on the <COCO dataset>.

= Deep learning benchmark
{parent=Deep learning}

= MLperf
{c}
{parent=Deep learning benchmark}

https://mlcommons.org/en/ Their homepage is not amazingly organized, but it does the job.

Benchmark focused on <deep learning>. It has two parts:
* <training (ML)>: produces a trained network
* <inference (ML)>: uses the trained network
Furthermore, a specific network model is specified for each benchmark in the closed category: so it goes beyond just specifying the dataset.

Results can be seen e.g. at:
* training: https://mlcommons.org/en/training-normal-21/
* inference: https://mlcommons.org/en/inference-datacenter-21/

And there are also separate repositories for each:
* https://github.com/mlcommons/inference
* https://github.com/mlcommons/training

E.g. on https://mlcommons.org/en/training-normal-21/ we can see what the the benchmarks are:

|| Dataset
|| Model

| <ImageNet>
| ResNet

| KiTS19
| 3D U-Net

| <Open Images Dataset>[OpenImages]
| RetinaNet

| <COCO dataset>
| Mask R-CNN

| LibriSpeech
| RNN-T

| Wikipedia
| BERT

| 1TB Clickthrough
| DLRM

| <Go (game)>
| <MiniGo>

= Deep learning is mostly matrix multiplication
{parent=Deep learning}

* https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/
* https://math.stackexchange.com/questions/41706/practical-uses-of-matrix-multiplication/4647422#4647422

= Deep learning framework
{parent=Deep learning}
{wiki}

= PyTorch
{c}
{parent=Deep learning framework}
{wiki}

= python/pytorch/matmul.py
{file}
{parent=PyTorch}

<Matrix multiplication> example.

Fundamental since <deep learning is mostly matrix multiplication>.

<NumPy> does not automatically use the <GPU> for it: https://stackoverflow.com/questions/49605231/does-numpy-automatically-detect-and-use-gpu[], and PyTorch is one of the most notable compatible implementations, as it uses the same memory structure as NumPy arrays.

Sample runs on <Ciro Santilli's hardware/P51> to observe the <GPU> speedup:
``
$ time ./matmul.py g 10000 1000 10000 100
real    0m22.980s
user    0m22.679s
sys     0m1.129s
$ time ./matmul.py c 10000 1000 10000 100
real    1m9.924s
user    4m16.213s
sys     0m17.293s
``

= TensorFlow
{c}
{parent=Deep learning framework}
{wiki}

= TensorFlow quantum
{c}
{parent=TensorFlow}
{tag=Quantum computing}

Version of TensorFlow with a <Cirq> backend that can run in either <quantum computers> or <classical computer> simulations, with the goal of potentially speeding up <deep learning> applications on a <quantum computer> some day.

= Computer vision
{parent=Machine learning}
{wiki}

= Pre-trained computer vision model
{parent=Computer vision}

= Pre-trained computer vision model CLI
{parent=Pre-trained computer vision model}

= yolov5-pip
{c}
{parent=Pre-trained computer vision model CLI}
{tag=YOLO model}

https://github.com/fcakyon/yolov5-pip

OK, now we're talking, two liner and you get a window showing <bounding box> object detection from your <webcam> feed!
``
python -m pip install -U yolov5==7.0.9
yolov5 detect --source 0
``
The accuracy is crap for anything but people. But still. Well done. Tested on <Ubuntu 22.10>, <Ciro Santilli's hardware/P51>.

\Video[https://www.youtube.com/watch?v=1MD3Wn7e6OE]

= Computer vision dataset
{parent=Computer vision}

= MNIST database
{c}
{parent=Computer vision dataset}
{title2=1998}
{wiki}

= MNIST
{c}
{synonym}

60,000 28x28 grayscale images of hand-written digits 0-9, i.e. 10 categories.

This is THE "<OG>" <computer vision dataset>.

Playing with it is the de-facto <computer vision> <hello world>.

But it is important to note that as of the 2010's, the benchmark had become too easy for many application.

The dataset can be downloaded from http://yann.lecun.com/exdb/mnist/[], e.g. training:
``
wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
``
but doing so is kind of pointless as both files use some crazy single-file custom binary format to store all images and labels. OMG!

OK-ish data explorer: https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=mnist

= Fashion MNIST
{c}
{parent=MNIST database}
{title2=2017}
{wiki}

Same style as <MNIST>, but with clothes. Designed to be much harder, and more representative of modern applications, while still retaining the low resolution of <MNIST> for simplicity of training.

= ImageNet
{c}
{parent=Computer vision dataset}
{tag=Closed standard}
{title2=2009}
{wiki}

14 million images, more than 20k categories, typically denoting prominent objects in the image, either common daily objects, or a wild range of animals. About 1 million of them also have <bounding boxes> for the objects.

Each image appears to have a single label associated to it. Care must have been taken somehow with categories, since some images contain severl possible objects, e.g. a person and some object.

In practice however, the <ILSVRC> subset is more commonly used.

Official project page: https://www.image-net.org/

The data license is restrictive and forbids commercial usage: https://www.image-net.org/download.php[].

https://datascience.stackexchange.com/questions/111756/where-can-i-view-the-imagenet-classes-as-a-hierarchy-on-wordnet

The categories are all part of <WordNet>, which means that there are several parent/child categories such as dog vs type of dog available. <ImageNet1k> only appears to have leaf nodes however (i.e. no "dog" label, just specific types of dog).

= ImageNet Large Scale Visual Recognition Challenge dataset
{c}
{parent=ImageNet}

= ILSVRC
{c}
{synonym}
{title2}

= ImageNet1k
{c}
{synonym}
{title2}

Subset of <ImageNet>. TODO size. Apparently 1k categories, and about 1,281,167 images, so this dataset is also known as ImageNet1k: https://datascience.stackexchange.com/questions/47458/what-is-the-difference-between-imagenet-and-imagenet1k-how-to-download-it

The official page: https://www.image-net.org/challenges/LSVRC/index.php points to a download link on <Kaggle>: https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data Kaggle says that the size is 167.62 GB!

Another download location appears to be: https://huggingface.co/datasets/imagenet-1k on <Hugging Face>, but you have to login due to their license terms. Once you login you have a very basic data explorer available: https://huggingface.co/datasets/imagenet-1k/viewer/default/train[].

https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a lists all 1k labels as a plaintext file.

= COCO dataset
{c}
{parent=Computer vision dataset}
{title2=2014}

https://cocodataset.org

From https://cocodataset.org/[]:
* 330K images (>200K labeled)
* 1.5 million object instances
* 80 object categories
* 91 stuff categories
* 5 captions per image. A caption is a short textual description of the image.

So they have relatively few object labels, but their focus seems to be putting a bunch of objects on the same image. E.g. they have 13 cat plus pizza photos. Searching for such weird combinations is kind of fun.

Their official dataset explorer is actually good: https://cocodataset.org/#explore

And the objects don't just have bounding boxes, but detailed polygons.

Also, images have captions describing the relation between objects:
\Q[a black and white cat standing on a table next to a pizza.]
Epic.

This dataset is kind of cool.

Original 2014 <paper> by <Microsoft>: https://arxiv.org/abs/1405.0312

= Open Images dataset
{c}
{parent=Computer vision dataset}
{title2=2014}

https://storage.googleapis.com/openimages/web/index.html

TODO vs <COCO dataset>.

As of v7:
* ~9M images
* 600 object classes
* <bounding boxes>
* visual relatoinships are really hard: https://storage.googleapis.com/openimages/web/factsfigures_v7.html#visual-relationships e.g. "person kicking ball": https://storage.googleapis.com/openimages/web/visualizer/index.html?type=relationships&set=train&c=kick
* https://google.github.io/localized-narratives/ localized narratives is ludicrous, you can actually hear the (<Indian> women mostly) annotators describing the image while hovering their mouses to point what they are talking about). They are clearly bored out of their minds the poor people!

The images and annotations are both under <CC BY>, with <Google> as the copyright holder.

= Optical character recognition
{parent=Computer vision}
{wiki}

= OCR
{c}
{synonym}
{title2}

= Image generation
{parent=Machine learning}
{wiki}

= Face Generator
{parent=Image generation}
{wiki}

Very useful for idiotic websites that require real photos!

* https://thispersondoesnotexist.com/ holy fuck, the images are so photorealistic, that <uncanny valley>[when there's a slight fail, it is really, really scary]

= Text to image generation
{parent=Image generation}
{wiki}

* https://deepai.org/machine-learning-model/text2img
* https://openai.com/blog/dall-e/

= Machine learning company
{parent=Machine learning}

= Hugging Face
{c}
{parent=Machine learning company}
{wiki}

https://huggingface.co/

= Ontology
{parent=Machine learning company}
{wiki}

= Hyponymy and hypernymy
{parent=Ontology}
{title2=is a}
{wiki}

= Meronymy and holonymy
{parent=Ontology}
{title2=is part of}
{wiki}

= List of ontologies
{parent=Ontology}

= WordNet
{c}
{parent=List of ontologies}
{wiki}

Groups concepts by <hyponymy and hypernymy> and <meronymy and holonymy>. That actually makes a lot of sense!

= Machine learning bibliography
{parent=Machine learning}

= Machine learning YouTube channel
{parent=Machine learning bibliography}

= Two Minute Papers
{parent=Machine learning bibliography}

https://www.youtube.com/@TwoMinutePapers

The approach of this channel of exposing recent research papers is a "honking good idea" that should be taken to other areas beyond just <machine learning>. It takes a very direct stab at <the missing link between basic and advanced>!
