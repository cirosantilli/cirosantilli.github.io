= Machine learning
{wiki}

= Machine learn
{synonym}

The main reason <Ciro Santilli> never touched it is that it feels that every public data set has already been fully mined or has already had the most interesting algorithms developed for it, so you can't do much outside of big companies.

This is why Ciro started <Ciro's 2D reinforcement learning games> to generate synthetic data and thus reduce the cost of data.

The other reason is that it is ugly.

\Include[artificial-intelligence]

= Cluster analysis
{parent=Machine learning}
{wiki}

= Clustering
{synonym}

= Deepfake
{parent=Machine learning}
{wiki}

= Deepfakes
{synonym}

= Dimentionality reduction
{parent=Machine learning}
{tag=Descriptive statistics}
{wiki}

= Principal component analysis
{parent=Dimentionality reduction}
{title2=PCA}
{wiki}

Given a bunch of points in $n$ dimensions, PCA maps those points to a new $p$ dimensional space with $p \le n$.

$p$ is a <hyperparameter>, $p=1$ and $p=2$ are common choices when doing dataset exploration, as they can be easily visualized on a planar plot.

The mapping is done by projecting all points to a $p$ dimensional <hyperplane>. PCA is an algorithm for choosing this hyperplane and the coordinate system within this hyperplane.

The hyperplane choice is done as follows:
* the <hyperplane> will have origin at the <mean> point
* the first axis is picked along the direction of greatest <variance>, i.e. where points are the most spread out.

  Intuitively, if we pick an axis of small variation, that would be bad, because all the points are very close to one another on that axis, so it doesn't contain as much information that helps us differentiate the points.
* then we pick a second axis, orthogonal to the first one, and on the direction of second largest variance
* and so on until $p$ orthogonal axes are taken

https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186 provides an OK-ish example with a concrete context. In there, we each point is a country, and the input data is the consumption of different kinds of foods per year, e.g.:
* flour
* dry codfish
* olive oil
* sausage
so in this example, we would have input points in 4D.

Suppose that every country consumes the same amount of flour every year. Then, that number doesn't tell us much about which country each point represents (has the least <variance>), and the first PCA axes would basically never point anywhere near that direction.

Another cool thing is that PCA seems to automatically account for linear dependencies in the data, so it skips selecting highly correlated axes multiple times. For example, suppose that dry codfish and olive oil consumption are very high in Portugal and Spain, but very low in Germany and Poland. Therefore, the variation is very high in those two parameters, and contains a lot of information.

However, suppose that dry codfish consumption is also directly proportional to olive oil consumption. Because of this, it would be kind of wasteful if we selected:
* dry codfish as the first axis
* olive oil as the second axis
since the information about codfish already tells us the olive oil. PCA apparently recognizes this, and instead picks the first axis at a 45 degree angle to both dry codfish and olive oil, and then moves on to something else for the second axis.

= Hyperparameter
{parent=Machine learning}
{tag=AI complete}
{wiki=Hyperparameter_(machine_learning)}

A parameter that you choose which determines how the algorithm will perform. In particular, it is not part of the <training data set>.

= Natural language processing
{parent=Machine learning}
{tag=AI complete}
{wiki}

An impossible <AI complete> dream.

It is impossible to understand speech, and take meaningful actions from it, if you don't understand what is being talked about.

And without doubt, "understanding what is being talked about" comes down to understanding (efficiently representing) the geometry of the 3D world with a time component.

Not from hearing sounds alone.

= Recommender system
{parent=Machine learning}
{wiki}

* https://analyticsindiamag.com/5-open-source-recommender-systems-you-should-try-for-your-next-project/ 5 Open-Source Recommender Systems You Should Try For Your Next Project (2019)

= Supervised and unsupervised learning
{parent=Machine learning}
{wiki}

= Supervised learning
{parent=Supervised and unsupervised learning}
{wiki}

= k-nearest neighbors algorithm
{c}
{parent=Supervised learning}
{wiki}

= k-NN
{c}
{title2}
{synonym}

One of the most simply classification algorithm one can think of: just see whatever kind of point your new point seems to be closer to, and say it is also of that type! Then it is just a question of defining "close".

<Scikit-learn> implementation https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html at \a[python/sklearn/knn.py]

= Training, validation, and test data sets
{parent=Supervised learning}
{wiki}

https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set

= Training data set
{parent=Training, validation, and test data sets}

= Validation data set
{parent=Training, validation, and test data sets}

= Test data set
{parent=Training, validation, and test data sets}

= Training and inference
{parent=Supervised learning}
{wiki}

This is the first thing you have to know about <supervised learning>:
* training is when you learn model parameters from input. This literally means learning the best value we can for a bunch of number input numbers of the model. This can easily be on the hundreds of thousands.
* inference is when we take a trained model (i.e. with the parameters determined), and apply it to new inputs
Both of those already have <hardware acceleration> available as of the 2010s.

= Inference
{disambiguate=ML}
{parent=Training and inference}

= Training
{disambiguate=ML}
{parent=Training and inference}

= Unsupervised learning
{parent=Supervised and unsupervised learning}
{wiki}

= Machine learning architecture
{parent=Machine learning}

= Symbolic artificial intelligence
{parent=Machine learning architecture}
{wiki}

= Symbolic AI
{synonym}

= Neuro-symbolic AI
{c}
{parent=Symbolic artificial intelligence}

https://researcher.watson.ibm.com/researcher/view_group.php?id=10518

An <IBM> made/pushed term, but that matches <Ciro Santilli>'s general view of how we should move forward <AGI>.

Ciro's motivation/push for this can be seen e.g. at: <Ciro's 2D reinforcement learning games>.

= Neural network
{parent=Machine learning}
{wiki}

= Artificial neural network
{parent=Neural network}
{wiki}

= Deep learning
{parent=Artificial neural network}
{wiki}

= Computer vision
{parent=Machine learning}
{wiki}

= Optical character recognition
{parent=Computer vision}
{wiki}

= OCR
{c}
{synonym}
{title2}

= Image generation
{parent=Machine learning}
{wiki}

= Face Generator
{parent=Image generation}
{wiki}

Very useful for idiotic websites that require real photos!

* https://thispersondoesnotexist.com/ holy fuck, the images are so photorealistic, that <uncanny valley>[when there's a slight fail, it is really, really scary]

= Text to image generation
{parent=Image generation}
{wiki}

* https://deepai.org/machine-learning-model/text2img
* https://openai.com/blog/dall-e/
