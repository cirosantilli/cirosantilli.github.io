= Machine learning
{wiki}

= Machine learn
{synonym}

The main reason <Ciro Santilli> never touched it is that it feels that every public data set has already been fully mined or has already had the most interesting algorithms developed for it, so you can't do much outside of big companies.

This is why Ciro started <Ciro's 2D reinforcement learning games> to generate synthetic data and thus reduce the cost of data.

The other reason is that it is ugly.

\Include[artificial-intelligence]{parent=machine-learning}

= Cluster analysis
{parent=Machine learning}
{wiki}

= Clustering
{synonym}

= Deepfake
{parent=Machine learning}
{wiki}

= Deepfakes
{synonym}

= Natural language processing
{parent=Machine learning}
{tag=AI complete}
{wiki}

An impossible <AI complete> dream.

It is impossible to understand speech, and take meaningful actions from it, if you don't understand what is being talked about.

And without doubt, "understanding what is being talked about" comes down to understanding (efficiently representing) the geometry of the 3D world with a time component.

Not from hearing sounds alone.

= Recommender system
{parent=Machine learning}
{wiki}

* https://analyticsindiamag.com/5-open-source-recommender-systems-you-should-try-for-your-next-project/ 5 Open-Source Recommender Systems You Should Try For Your Next Project (2019)

= Supervised and unsupervised learning
{parent=Machine learning}
{wiki}

= Supervised learning
{parent=Supervised and unsupervised learning}
{wiki}

= k-nearest neighbors algorithm
{c}
{parent=Supervised learning}
{wiki}

= k-NN
{c}
{title2}
{synonym}

One of the most simply classification algorithm one can think of: just see whatever kind of point your new point seems to be closer to, and say it is also of that type! Then it is just a question of defining "close".

<Scikit-learn> implementation https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html at \a[python/sklearn/knn.py]

= Training and inference
{parent=Supervised learning}
{wiki}

This is the first thing you have to know about <supervised learning>:
* training is when you learn model parameters from input. This literally means learning the best value we can for a bunch of number input numbers of the model. This can easily be on the hundreds of thousands.
* inference is when we take a trained model (i.e. with the parameters determined), and apply it to new inputs
Both of those already have <hardware acceleration> available as of the 2010s.

= Unsupervised learning
{parent=Supervised and unsupervised learning}
{wiki}

= Inference
{disambiguate=ML}
{parent=Training and inference}

= Training
{disambiguate=ML}
{parent=Training and inference}

= Machine learning architecture
{parent=Machine learning}

= Symbolic artificial intelligence
{parent=Machine learning architecture}
{wiki}

= Symbolic AI
{synonym}

= Neuro-symbolic AI
{c}

https://researcher.watson.ibm.com/researcher/view_group.php?id=10518

An <IBM> made/pushed term, but that matches <Ciro Santilli>'s general view of how we should move forward <AGI>.

Ciro's motivation/push for this can be seen e.g. at: <Ciro's 2D reinforcement learning games>.

= Neural network
{parent=Machine learning}
{wiki}

= Artificial neural network
{parent=Neural network}
{wiki}

= Deep learning
{parent=Artificial neural network}
{wiki}
