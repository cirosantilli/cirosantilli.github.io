= Mathematics
{wiki}

= Mathematical
{synonym}

= Mathematically
{synonym}

= Math
{synonym}

= Maths
{synonym}

= Mathy
{synonym}

The proper precise definition of mathematics can be found at: \x[formalization-of-mathematics]{full}.

The most beautiful things in mathematics are described at: \x[the-beauty-of-mathematics].

= The beauty of mathematics
{parent=mathematics}

\x[ciro-santilli] intends to move his beauty list here little by little: https://github.com/cirosantilli/mathematics/blob/master/beauty.md

The most beautiful things in mathematics are results that are:
* simple to state and understand (\x[k-12], lower \x[undergrad]), but extremely hard to prove, e.g. Fermat's Last Theorem
* surprising results: we had intuitive reasons to believe something as possible or not, but a theorem shatters that conviction and brings us on our knees, sometimes via \x[pathological-mathematics] counter-examples. General surprise themes include:
  * \x[classification-mathematics] of potentially \x[infinite] sets like: compact \x[manifold]{p}, etc.
    * \x[classification-of-finite-simple-groups]
    * \x[classification-of-regular-polytopes]
    * \x[classification-of-closed-surfaces], and more \x[generalized-poincare-conjecture]{p}
    * classification of \x[wallpaper-group]{p} and \x[space-group]{p}
  * problems that are more complicated in low dimensions than high like:
    * \x[generalized-poincare-conjecture]{p}. It is also fun to see how in many cases complexity peaks out at 4 dimensions.
    * \x[classification-of-regular-polytopes]
  * unpredictable magic constants:
    * why is the lowest dimension for an \x[exotic-sphere] 7?
    * why is 4 the largest degree of an equation with explicit solution? \x[abel-ruffini-theorem]
* applications: make life easier and/or modeling some phenomena well, e.g. in \x[physics]. See also: \x[how-to-teach/explain-how-to-make-money-with-the-lesson]

Good lists of such problems \x[lists-of-mathematical-problems].

Specific examples:
* from \x[computer-science]:
  * the existence of \x[undecidable-problem]{p}, especially simple to state ones, e.g. \x[mortal-matrix-problem]

Whenever \x[ciro-santilli] learns a bit of \x[mathematics], he always wonders to himself:
\Q[Am I achieving insight, or am I just memorizing definitions?]
Unfortunately, due to how man books are written, it is not really possible to reach insight without first doing a bit of memorization. The better the book, the more insight is spread out, and less you have to learn before reaching each insight.

= The Hundred Greatest Theorems by Paul and Jack Abad (1999)
{parent=the-beauty-of-mathematics}

Randomly reproduced at: http://web.archive.org/web/20080105074243/http://personal.stevens.edu/~nkahl/Top100Theorems.html

= Classification
{disambiguate=mathematics}
{parent=the-beauty-of-mathematics}
{wiki}

= Classifying
{disambiguate=mathematics}
{synonym}

In \x[mathematics], a "classification" means making a list of all possible objects of a given type.

Classification results are some of \x[ciro-santilli]'s favorite: \x[the-beauty-of-mathematics]{full}.

Examples:
* \x[classification-of-finite-simple-groups]{child}
* \x[classification-of-regular-polytopes]{child}
* \x[classification-of-closed-surfaces]{child}, and more generalized \x[generalized-poincare-conjecture]{child}{p}
* \x[classification-of-associative-real-division-algebras]{child}
* \x[classification-of-finite-fields]{child}
* \x[classification-of-simple-lie-groups]{child}
* classification of the \x[wallpaper-group]{p} and the \x[space-group]{p}

= Pathological
{disambiguate=mathematics}
{parent=the-beauty-of-mathematics}
{wiki}

= Exceptional object
{parent=pathological-mathematics}
{wiki}

Oh, and the dude who created the https://en.wikipedia.org/wiki/Exceptional_object \x[wikipedia] page won an Oscar: https://www.youtube.com/watch?v=oF_FLN-TmCY[], Dan Piponi, aka `@sigfpe`. Cool dude.

List:
* \x[sporadic-group]{child}

= Exceptional isomorphism
{parent=exceptional-object}
{wiki}

= Lists of mathematical problems
{c}
{parent=the-beauty-of-mathematics}
{wiki}

Good place to hunt for \x[the-beauty-of-mathematics].

= Hilbert's problems
{c}
{parent=lists-of-mathematical-problems}
{wiki}

He's a bit overly obsessed with \x[polynomial]{p} for the taste of modern maths, but it's still fun.

= Millennium Prize Problems
{c}
{parent=lists-of-mathematical-problems}
{wiki}

\x[ciro-santilli] would like to fully understand the statements and motivations of each the problems!

Easy to understand the motivation:
* \x[navier-stokes-existence-and-smoothness] is basically the only problem that is really easy to understand the statement and motivation :-)
* \x[p-versus-np-problem]

Hard to understand the motivation!
* \x[riemann-hypothesis]: a bunch of results on prime numbers, and therefore possible applications to \x[cryptography]

  Of course, everything of interest has already been proved conditionally on it, and the likely "true" result will in itself not have any immediate applications.

  As is often the case, the only usefulness would be possible new ideas from the proof technique, and people being more willing to prove stuff based on it without the risk of the hypothesis being false.
* \x[yang-mills-existence-and-mass-gap]: this one has to do with findind/proving the existence of a more decent formalization of \x[quantum-field-theory] that does not resort to tricks like \x[perturbation-theory] and \x[effective-field-theory] with a random cutoff value

  This is important because the best theory of light and electrons (and therefore chemistry and material science) that we have today, \x[quantum-electrodynamics], is a \x[quantum-field-theory].

= Birch and Swinnerton-Dyer Conjecture
{c}
{parent=millennium-prize-problems}
{wiki}

https://www.youtube.com/watch?v=R9FKN9MIHlE&t=938s Birch and Swinnerton-Dyer Conjecture (Millennium Prize Problem!) by Kinertia (2020)

= Formalization of Mathematics
{parent=mathematics}

= Foundation of mathematics
{synonym}

Mathematics is a \x[art][beautiful game] played on https://en.wikipedia.org/wiki/String_(computer_science[strings], which \x[mathematician]{p} call https://en.wikipedia.org/wiki/Theorem["theorems"].

Here is a more understandable description of the semi-satire that follows: https://math.stackexchange.com/questions/53969/what-does-formal-mean/3297537#3297537

You start with a very small list of:
* certain arbitrarily chosen initial strings, which mathematicians call "\x[axiom]{p}"
* rules of how to obtain new strings from old strings, called https://en.wikipedia.org/wiki/Rule_of_inference["rules of inference"] Every transformation rule is very simple, and can be verified by a computer.

Using those rules, you choose a target string that you want to reach, and then try to reach it. Before the target string is reached, mathematicians call it a https://en.wikipedia.org/wiki/Conjecture["conjecture"].

Mathematicians call the list of transformation rules used to reach a string a https://en.wikipedia.org/wiki/Mathematical_proof["proof"].

Since every step of the proof is very simple and can be verified by a computer automatically, the entire proof can also be automatically verified by a computer very easily.

Finding proofs however is undoubtedly an \x[computable-problem][uncomputable problem].

Most mathematicians can't code or deal with the real world in general however, so they haven't created the obviously necessary: \x[website-front-end-for-a-mathematical-formal-proof-system].

The fact that Mathematics happens to be the best way to describe \x[physics] and that humans can use physical intuition heuristics to reach the NP-hard proofs of mathematics is one of the great miracles of the universe.

Once we have mathematics formally modelled, one of the coolest results is https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems[Gödel's incompleteness theorems], which states that for any reasonable proof system, there are necessarily theorems that cannot be proven neither true nor false starting from any given set of axioms: those theorems are independent from those axioms. Therefore, there are three possible outcomes for any hypothesis: true, false or independent!

Some famous theorems have even been proven to be independent of some famous \x[axiom]{p}. One of the most notable is that the http://en.wikipedia.org/wiki/Continuum_hypothesis[Continuum Hypothesis] is \x[independent-mathematical-logic] from \x[zermelo-fraenkel-set-theory]! Such independence proofs rely on modelling the proof system inside another proof system, and https://en.wikipedia.org/wiki/Forcing_(mathematics)[forcing] is one of the main techniques used for this.

\Image[https://web.archive.org/web/20190430151331im_/http://abstrusegoose.com/strips/i_dont_give_a_shit_about_your_mountain.png]
{title=The landscape of modern Mathematics comic by Abstruse Goose.}
{description=This comic shows that Mathematics is one of the most diversified areas of \x[art][useless] human knowledge.}
{source=https://abstrusegoose.com/211}

= Proof assistant
{parent=formalization-of-mathematics}
{wiki}

Much of this section will be dumped at \x[website-front-end-for-a-mathematical-formal-proof-system]{full} instead.

= QED manifesto
{c}
{parent=proof-assistant}
{wiki}

If \x[ciro-santilli] ever becomes rich, he's going to solve this with: \x[website-front-end-for-a-mathematical-formal-proof-system], promise.

= List of proof assistants
{parent=proof-assistant}

= Lean
{disambiguate=proof assistant}
{parent=list-of-proof-assistants}
{tag=microsoft-product}
{tag=open-source-software}
{wiki}

* https://github.com/leanprover/lean
* https://github.com/leanprover/lean4

= Formal proof
{parent=formalization-of-mathematics}
{wiki}

A proof in some system for the \x[formalization-of-mathematics].

= Mathematical proof
{parent=formal-proof}
{wiki}

= Formal system
{parent=formal-proof}

= Formal proof system
{synonym}

= Zermelo-Fraenkel set theory
{c}
{parent=formal-system}
{title2=ZF}
{wiki=Zermelo–Fraenkel_set_theory}

One of the first \x[formal-proof-system]{p}. This is actually understandable!

This is \x[ciro-santilli]-2020 definition of the \x[foundation-of-mathematics] (and the only one he had any patience to study at all).

TODO what are its limitations? Why were other systems created?

= Set theory
{parent=zermelo-fraenkel-set-theory}
{wiki}

When \x[ciro-santilli] says \x[set-theory], he basically means. \x[zermelo-fraenkel-set-theory].

= Metamath
{c}
{parent=zermelo-fraenkel-set-theory}
{wiki}

http://metamath.org/

It seems to implement \x[zermelo-fraenkel-set-theory].

= Axiom
{parent=formal-proof}
{wiki}

= Consistency
{parent=axiom}
{wiki}

= Consistent
{synonym}

A set of \x[axiom]{p} is consistent if they don't lead to any contradictions.

When a set of axioms is not consistent, false can be proven, and then everything is true, making the set of axioms useless.

= Independence
{disambiguate=mathematical logic}
{parent=axiom}
{wiki}

= Independent
{disambiguate=mathematical logic}
{synonym}

A \x[theorem] is said to be independent from a set of \x[axiom]{p} if it cannot be proven neither true nor false from those axioms.

It or its negation could therefore be arbitrarily added to the set of axioms.

= Conjecture
{parent=formal-proof}
{wiki}

= Theorem
{parent=formal-proof}
{wiki}

= Corollary
{parent=theorem}
{wiki}

An easy to prove theorem that follows from a harder to prove theorem.

= Set
{disambiguate=mathematics}
{parent=formalization-of-mathematics}

= Set
{synonym}

Intuitively: unordered container where all the values are unique, just like C++ `std::set`.

More precisely for set theory \x[formalization-of-mathematics]:
* everything is a set, including the elements of sets
* string manipulation wise:
  * `{}` is an empty set. The natural number `0` is defined as `{}` as well.
  * `{{}}` is a set that contains an empty set
  * `{{}, {{}}}` is a set that contains two sets: `{}` and `{{}}`
  * `{{}, {}}` is not well formed, because it contains `{}` twice

= Union
{disambiguate=set theory}
{parent=set-mathematics}
{title2=$A \bigcup B$}
{wiki}

= Set union
{synonym}

= Cardinality
{parent=set-mathematics}

The size of a set.

For \x[finite] sizes, the definition is simple, and the intuitive name "size" matches well.

But for infinity, things are messier, e.g. the size of the \x[real-number]{p} is strictly larger than the size of the \x[integer]{p} as shown by \x[cantor-s-diagonal-argument], which is kind of what justifies a fancier word "cardinality" to distinguish it from the more normal word "size".

The key idea is to compare set sizes with \x[bijection]{p}.

= Function
{disambiguate=mathematics}
{parent=formalization-of-mathematics}
{wiki}

= Function
{synonym}

\x[set-mathematics]{c} of \x[ordered-pair]{p}. That's it! This is illustrated at: https://math.stackexchange.com/questions/1480651/is-fx-x-1-x-2-a-function/1481099#1481099

= Cartesian product
{c}
{parent=function-mathematics}
{wiki}

A function that maps two \x[set]{p} to a third set.

= Direct product
{c}
{parent=function-mathematics}
{wiki}

A \x[cartesian-product] that carries over some extra structure of the input groups.

E.g. the \x[direct-product-of-groups] carries over \x[group] structure on both sides.

= Domain, codomain and image
{parent=function-mathematics}

= Bijection
{parent=domain-codomain-and-image}
{wiki}

= Bijective
{synonym}

= One-to-one
{synonym}

= 1-to-1
{synonym}

= Injective function
{parent=bijection}
{wiki}

= Injection
{synonym}

= Injective
{synonym}

Mnemonic: in means into. So we are going into a \x[codomain] that is large enough so that we can have a different image for every input.

= Surjective function
{parent=bijection}
{wiki}

= Surjection
{synonym}

= Surjective
{synonym}

Mnemonic: sur means over. So we are going over the \x[codomain], and covering it entirely.

= Domain of a function
{parent=domain-codomain-and-image}
{wiki}

= Domain
{disambiguate=function}
{synonym}

= Codomain
{parent=domain-codomain-and-image}
{wiki}

Vs: \x[image-mathematics]: the codomain is the set that the function might reach.

The \x[image-mathematics] is the exact set that it actually reaches.

E.g. the function:
$$
f(x) = x^2
$$
could have:
* codomain $\R$
* image $\R_{+}$

Note that the definition of the codomain is somewhat arbitrary, e.g. $x^2$ could as well technically have codomain:
$$
\R \bigcup \R^2
$$
even though it will obviously never reach any value in $\R^2$.

The exact image is in general therefore harder to characterize.

= Endofunction
{parent=codomain}

A \x[function] where the \x[domain-function] is the same as the \x[codomain].

= Image
{disambiguate=mathematics}
{parent=domain-codomain-and-image}
{wiki}

= Image of a function
{synonym}

= Image
{disambiguate=function}
{synonym}

= Addition
{parent=function-mathematics}
{wiki}

= Subtraction
{parent=function-mathematics}
{wiki}

= Multiplication
{parent=function-mathematics}
{wiki}

= Division
{parent=function-mathematics}
{wiki}

= Exponentiation
{parent=function-mathematics}
{title2=$x^y$}
{wiki}

= nth root
{parent=exponentiation}
{wiki}

= Square root
{parent=nth-root}
{wiki}

= Exponentiation functional equation
{parent=exponentiation}
{tag=functional-equation}

We define this as the \x[functional-equation]:
$$
f(x, y) = f(x)f(y)
$$
It is a bit like \x[cauchy-s-functional-equation] but with \x[multiplication] instead of \x[addition].

= Exponential function
{parent=exponentiation}
{title2=$e^x$}
{wiki}

= Exponential function differential equation
{parent=exponential-function}

The \x[differential-equation] that is solved by the \x[exponential-function]:
$$
y'(x) = y(x)
$$
with \x[initial-condition]:
$$
y(0) = 1
$$

TODO find better name for it, "\x[linear-differential-equation][linear] homogenous differential equation of degree one" almost fully constrainst it except for the exponent constant and initial value.

= Definition of the exponential function
{parent=exponential-function}
{wiki=Characterizations_of_the_exponential_function}

= Taylor expansion definition of the exponential function
{c}
{parent=definition-of-the-exponential-function}

The \x[taylor-series] expansion is the most direct definition of the expontial as it obviously satisfies the \x[exponential-function-differential-equation]:
* the first constant term dies
* each other term gets converted to the one before
* because we have \x[infinite] many terms, we get what we started with!
$$
e^x = \sum_{n=0}^\infty \frac{x^n}{n!} = 1 + \frac{x}{1} + \frac{x^2}{2} + \frac{x^3}{2 \times 3} + \frac{x^4}{2 \times 3 \times 4} + \ldots
$$

= Product definition of the exponential function
{parent=definition-of-the-exponential-function}

$$
e^x = \lim_{n\to\infty} \left(1+\frac x n \right)^n
$$

The basic intuition for this is to start from the origin and make small changes to the function based on its known derivative at the origin.

More precisely, we know that for any base b, \x[exponentiation] satisfies:
* $b^{x + y} = b^x b^y$.
* $b^{0} = 1$.
And we also know that for $b = e$ in particular that we satisfy the \x[exponential-function-differential-equation] and so:
$$
\dv{e^x}{x}(0) = 1
$$
One interesting fact is that the only thing we use from the \x[exponential-function-differential-equation] is the value around $x = 0$, which is quite little information! This idea is basically what is behind the importance of the ralationship between \x[lie-group-lie-algebra-correspondence] via the \x[exponential-map]. In the more general settings of groups and \x[manifold]{p}, restricting ourselves to be near the origin is a huge advantage.

Now suppose that we want to calculate $e^1$. The idea is to start from $e^0$ and then then to use the first order of the \x[taylor-series] to extend the known value of $e^0$ to $e^1$.

E.g., if we split into 2 parts, we know that:
$$
e^1 = e^{1/2}e^{1/2}
$$
or in three parts:
$$
e^1 = e^{1/3}e^{1/3}e^{1/3}
$$
so we can just use arbitrarily many parts $e^{1/n}$ that are arbitrarily close to $x = 0$:
$$
e^1 = (e^{1/n})^n
$$
and more generally for any $x$ we have:
$$
e^x = (e^{x/n})^n
$$

Let's see what happens with the Taylor series. We have near $y = 0$ in \x[little-o-notation]:
$$
e^y = 1 + y + o(y)
$$
Therefore, for $y = x/n$, which is near $y = 0$ for any fixed $x$:
$$
e^{x/n} = 1 + x/n + o(1/n)
$$
and therefore:
$$
e^x = (e^{x/n})^n = (1 + x/n + o(1/n))^n
$$
which is basically the formula tha we wanted. We just have to convince ourselves that at $\lim_{n \to \infty}$, the $o(1/n)$ disappears, i.e.:
$$
(1 + x/n + o(1/n))^n = (1 + x/n)^n
$$

To do that, let's multiply $e^y$ by itself once:
$$
e^y e^y = (1 + y + o(y))(1 + y + o(y)) = 1 + 2y + o(y)
$$
and multiplying a third time:
$$
e^y e^y e^y = (1 + 2y + o(y))(1 + y + o(y)) = 1 + 3y + o(y)
$$
TODO conclude.

= Exponential
{synonym}

= Logarithm
{parent=exponential-function}
{wiki}

= Matrix exponential
{parent=exponential-function}
{wiki}

Is the solution to a \x[system-of-linear-ordinary-differential-equations], the \x[exponential-function] is just a 1-dimensional subcase.

Note that more generally, the \x[matrix-exponential] can be defined on any \x[ring-mathematics].

The matrix exponential is of particular interest in the study of \x[lie-group]{p}, because in the case of the \x[lie-algebra-of-a-matrix-lie-group], it provides the correct \x[exponential-map].

\Video[https://www.youtube.com/watch?v=O85OWBJ2ayo]
{title=How (and why) to raise e to the power of a matrix by \x[3blue1brown] (2021)}

= Logarithm of a matrix
{parent=matrix-exponential}
{wiki}

= Matrix logarithm
{synonym}

= Existence of the matrix logarithm
{parent=logarithm-of-a-matrix}

= The matrix exponential is not surjective
{synonym}

https://en.wikipedia.org/wiki/Logarithm_of_a_matrix#Existence mentions it always exists for all \x[invertible] \x[complex] matrices. But the \x[real] condition is more complicated. Notable counter example: -1 cannot be reached by any real $e^{tk}$.

The \x[lie-algebra-exponential-covering-problem] can be seen as a generalized version of this problem, because
* \x[lie-algebra] of \x[gl-n] is just the entire \x[m-n]
* we can immediately exclude non-invertible matrices from being the result of the exponential, because $e^{tM}$ has inverse $e^{-tM}$, so we already know that non-invertible matrices are not reachable

= Function space
{parent=formalization-of-mathematics}
{wiki}

Most notable example: \x[l2].

= Integer
{parent=formalization-of-mathematics}
{title2=$\Z$}
{wiki}

= Rational number
{parent=formalization-of-mathematics}
{title2=$\Q$}
{wiki}

= Fraction
{parent=rational-number}
{wiki}

= Real number
{parent=formalization-of-mathematics}
{title2=$\R$}
{wiki}

= Real
{synonym}

A good definition is by using \x[dedekind-cut]{p}.

= Dedekind cut
{c}
{parent=real-number}
{wiki}

= Cardinality of the continuum
{parent=real-number}

= Countable set
{parent=cardinality-of-the-continuum}
{wiki}

= Cantor's diagonal argument
{c}
{parent=cardinality-of-the-continuum}
{wiki}

= Mathematical constant
{parent=real-number}
{wiki}

= Pi
{parent=mathematical-constant}
{title2=$\pi$}
{wiki}

= Complex number
{parent=formalization-of-mathematics}
{wiki}

= Complex
{synonym}

An \x[ordered-pair] of two \x[real-number]{p} with the complex addition and multiplication defined.

Forms both a:
* \x[division-algebra] if thought of \x[r-2] with complex multiplication as the bilinear map of the \x[algebra-over-a-field][algebra]
* \x[field-mathematics]

= Complex conjugate
{parent=complex-number}
{title2=$\overline{x}$}
{wiki}

= Imaginary number
{parent=complex-number}
{wiki}

= Imaginary unit
{parent=complex-number}
{title2=$i$}
{wiki}

= Cayley-Dickson construction
{c}
{parent=complex-number}
{wiki=Cayley–Dickson construction}

Constructs the \x[quaternion]{p} from \x[complex-number]{p}, \x[octonion]{p} from \x[quaternion]{p}, and keeps doubling like this indefinitely.

= Quaternion
{parent=cayley-dickson-construction}
{title2=$\H$, 4}
{wiki}

Kind of extends the \x[complex-number]{p}.

Some facts that make them stand out:
* one of the only three real \x[associative] \x[division-algebra]{p} in addition to the \x[real-number]{p} and \x[complex-number]{p}, according to the \x[classification-of-associative-real-division-algebras]
* the simplest non-\x[commutative] \x[division-algebra]. Contrast for example with \x[complex-number]{p} where multiplication is \x[commutative]

= Octonion
{parent=cayley-dickson-construction}
{title2=8}
{wiki}

Unlike the \x[quaternion]{p}, it is non-\x[associative].

= Sedenion
{parent=cayley-dickson-construction}
{title2=16}
{wiki}

= Ordered pair
{parent=formalization-of-mathematics}
{wiki}

\x[set]{p} are unordered, but we can use them to create ordered objects, which are of fundamental importance. Notably, they are used in the definition of \x[function]{p}.
= Logic
{parent=formalization-of-mathematics}
{wiki}

= Propositional logic
{parent=logic}
{wiki}

This is the part of the \x[formalization-of-mathematics] that deals only with the propositions.

In some systems, e.g. including \x[metamath], \x[modus-ponens] alone tends to be enough, everything else can be defined based on it.

= Modus ponens
{parent=propositional-logic}
{wiki}

= If and only if
{parent=propositional-logic}
{wiki}

= Iff
{synonym}
{title2}

= First-order logic
{parent=logic}
{wiki}

Builds on top of \x[propositional-logic], adding notably \x[existential-quantification].

= Existential quantification
{parent=first-order-logic}
{title2=$\exists$}
{wiki}

Models \x[existence] in the context of the \x[formalization-of-mathematics].

= Existence and uniqueness
{parent=existential-quantification}
{wiki}

Existence and uniqueness results are fundamental in \x[mathematics] because we often define objects by their properties, and then start calling them "the object", which is fantastically convenient.

But calling something "the object" only makes sense if there exists exactly one, and only one, object that satisfies the properties.

One particular context where these come up very explicitly is in solutions to \x[differential-equation]{p}, e.g. \x[existence-and-uniqueness-of-solutions-of-partial-differential-equations]{child}.

= Existence
{parent=existence-and-uniqueness}

= Exists
{synonym}

= Exist
{synonym}

= Uniqueness
{parent=existence-and-uniqueness}

= Unique
{synonym}

= Universal quantification
{parent=first-order-logic}
{title2=$\forall$}
{wiki}

= Algebra
{parent=mathematics}
{wiki}

= Algebraic
{synonym}

Not to be confused with \x[algebra-over-a-field], which is a particular \x[algebraic-structure] studied within algebra.

= Abstract algebra
{parent=algebra}
{wiki}

We just use "Abstract algebra" as a synonym for \x[algebra].

= Algebraic structure
{parent=algebra}
{wiki}

A \x[set-mathematics] $S$ plus any number of functions $f_i : S \times S \to S$, such that each $f_i$ satisfies some properties of choice.

Key examples:
* \x[group]: one function
* \x[field-mathematics]: two functions
* \x[ring-mathematics]: also two functions, but with less restrictive properties

= Commutator
{parent=algebraic-structure}
{wiki}

= Identity element
{parent=algebraic-structure}
{wiki}

= Inverse element
{parent=identity-element}
{wiki}

= Inverse
{synonym}

Some specific examples:
* \x[invertible-matrix]

= Invertible
{parent=inverse-element}

= Order
{disambiguate=algebra}
{parent=algebraic-structure}
{wiki}

The order of a \x[algebraic-structure] is just its \x[cardinality].

Sometimes, especially in the case of structures with an \x[infinite] number of elements, it is often more convenient to talk in terms of some parameter that characterizes the structure, and that parameter is usually called the \x[degree-algebra].

= Degree
{disambiguate=algebra}
{parent=order-algebra}
{wiki}

The degree of some \x[algebraic-structure] is some parameter that describes the structure. There is no universal definition valid for all structures, it is a per structure type thing.

This is particularly useful when talking about structures with an \x[infinite] number of elements, but it is sometimes also used for finite structures.

Examples:
* the \x[dihedral-group] of degree n acts on n elements, and has order 2n
* the parameter $n$ that characterizes the size of the \x[general-linear-group] $GL(n)$ is called the degree of that group, i.e. the dimension of the underlying matrices

= Finite algebraic structure
{parent=order-algebra}

Examples:
* \x[finite-group]{child}
* \x[finite-field]{child}

= Linear algebra
{parent=algebra}
{wiki}

= Linear function
{parent=linear-algebra}
{wiki}

= Linear
{synonym}

= Linearly
{synonym}

The term is not very clear, as it could either mean:
* a \x[real-number] function whose graph is a line, i.e.:
  $$f(x) = ax + b$$
  or for higher dimensions, a \x[hyperplane]:
  $$f(x_1, x_2, \ldots, x_n) = c_1 x_1 + c_2 x_2 + \ldots + c_n x_n + b$$
* a \x[linear-map]. Note that the above linear functions are not linear maps unless $b = 0$ (known as the homogeneous case), because e.g.:
  $$f(x + y) = ax + ay + b$$
  but
  $$f(x) + f(y) = ax + b + ay + b$$
  For this reason, it is better never to refer to linear maps as linear functions.

= Linear map
{parent=linear-algebra}
{title2=linear operator}
{wiki}

A linear map is a function $f : V_1(F) \to V_2(F)$ where $V_1(F)$ and $V_2(F)$ are two vector spaces over \x[underlying-field-of-a-vector-space][underlying fields] $F$ such that:
$$
\forall v_{1}, v_{2} \in V_1, c_{1}, c_{2} \in F \\
f(c_{1} v_{1} + c_{2} v_{2}) = c_{1} f(v_{1}) + c_{2} f(v_{2})
$$

A common case is $F = \R$, $V_1 = \R_m$ and $V_2 = \R_n$.

One thing that makes such functions particularly simple is that they can be fully specified by specifyin how they act on all possible combinations of input basis vectors: they are therefore specified by only a finite number of elements of $F$.

Every linear map in \x[finite-dimension] can be represented by a \x[matrix], the points of the \x[domain-function] being represented as \x[vector]{p}.

As such, when we say "linear map", we can think of a generalization of \x[matrix-multiplication] that makes sense in \x[infinite-dimensional] spaces like \x[hilbert-space]{p}, since calling such infinite dimensional maps "matrices" is stretching it a bit, since we would need to specify infinitely many rows and columns.

The prototypical building block of \x[infinite-dimensional] linear map is the \x[derivative]. In that case, the vectors being operated upon are \x[function]{p}, which cannot therefore be specified by a finite number of parameters, e.g. 

For example, the left side of the \x[time-independent-schrodinger-equation] is a linear map. And the \x[time-independent-schrodinger-equation] can be seen as a \x[eigenvalue] problem.

= Form
{disambiguate=mathematics}
{parent=linear-map}

A form is a \x[function] from a \x[vector-space] to elements of the \x[underlying-field-of-the-vector-space].

Examples:
* \x[linear-form]
* \x[bilinear-form]
* \x[multilinear-form]

= Linear form
{parent=linear-map}
{wiki}

A \x[linear-map]{c} where the \x[image-mathematics] is the \x[underlying-field-of-the-vector-space], e.g. $\R^n \to \R$.

The set of all \x[linear-form]{p} over a \x[vector-space] forms another vector space called the \x[dual-space].

= Matrix representation of a linear form
{parent=linear-form}

For the typical case of a linear form over \x[r-n], the form can be seen just as a row vector with n elements, the full form being specified by the value of each of the \x[basis-vector]{p}.

= Dual space
{parent=linear-form}
{title2=$V^*$}
{wiki}

The dual space of a \x[vector-space] $V$, sometimes denoted $V^*$, is the vector space of all \x[linear-form]{p} over $V$ with the obvious addition and scalar multiplication operations defined.

Since a linear form is completely determined by how it acts on a \x[basis], and since for each basis element it is specified by a scalar, at least in finite dimension, the dimension of the dual space is the same as the $V$, and so they are isomorphic because \x[all-vector-spaces-of-the-same-dimension-on-a-given-field-are-isomorphic], and so the dual is quite a boring concept in the context of finite dimension.

Infinite dimension seems more interesting however, see: https://en.wikipedia.org/w/index.php?title=Dual_space&oldid=1046421278#Infinite-dimensional_case

One place where duals are different from the non-duals however is when dealing with \x[tensor]{p}, because they transform differently than vectors from the base space $V$.

= Dual vector
{parent=dual-space}
{title2=$e^i$}

Dual vectors are the members of a \x[dual-space].

In the context of \x[tensor]{p} , we use raised indices to refer to members of the dual basis vs the underlying basis:
$$
\begin{aligned}
e_1 & \in V \\
e_2 & \in V \\
e_3 & \in V \\
e^1 & \in V^* \\
e^2 & \in V^* \\
e^3 & \in V^* \\
\end{aligned}
$$
The dual basis vectors $e^i$ are defined to "pick the corresponding coordinate" out of elements of V. E.g.:
$$
\begin{aligned}
e^1 (4, -3, 6) & =  4 \\
e^2 (4, -3, 6) & = -3 \\
e^3 (4, -3, 6) & =  6 \\
\end{aligned}
$$
By expanding into the basis, we can put this more succinctly with the \x[kronecker-delta] as:
$$
e^i(e_j) = \delta_{ij}
$$

Note that in \x[einstein-notation], the components of a dual vector have lower indices. This works well with the upper case indices of the dual vectors, allowing us to write a dual vector $f$ as:
$$
f = f_i e^i
$$

In the context of \x[quantum-mechanics], the \x[bra-ket][bra] notation is also used for dual vectors.

= Linear operator
{parent=linear-map}
{wiki}

= Operator
{synonym}

We define it as a \x[linear-map] where the \x[domain-function] is the same as the \x[image-mathematics], i.e. an \x[endofunction].

Examples:
* a 2x2 matrix can represent a \x[linear-map] from \x[r-2] to \x[r-2], so which is a linear operator
* the \x[derivative] is a \x[linear-map] from \x[c-infty] to \x[c-infty], so which is also a linear operator

= Adjoint operator
{parent=linear-operator}
{title2=$A^\dagger$}

Given a \x[linear-operator] $A$ over a space $S$ that has a \x[inner-product] defined, we define the adjoint operator $A^\dagger$ (the $\dagger$ symbol is called "dagger") as the unique operator that satisfies:
$$
\forall v, w \in S, <Av, w> = <v, A^{\dagger} w>
$$

= Self-adjoint operator
{parent=linear-map}
{wiki}

= Self-adjoint
{synonym}

= Multilinear map
{parent=linear-map}
{wiki}

= Bilinear map
{parent=multilinear-map}
{wiki}

= Bilinear product
{synonym}

\x[linear-map]{c} of two variables.

More formally, given 3 \x[vector-space]{p} X, Y, Z over a single \x[field-mathematics], a bilinear map is a function from:
$$f : X \times Y \to Z$$
that is linear on the first two arguments from X and Y, i.e.:
$$f(a_1\vec{x_1} + a_2\vec{x_2}, \vec{y}) = a_1f(\vec{x_1}, \vec{y}) + a_2f(\vec{x_2}, \vec{y})$$
Note that the definition only makes sense if all three vector spaces are over the same field, because linearity can mix up each of them.

The most important example by far is the \x[dot-product] from $\R^n \times \R^n \to \R$, which is more specifically also a \x[symmetric-bilinear-form].

= Bilinear form
{parent=multilinear-map}
{title2=$B(x, y)$}
{wiki}

Analogous to a \x[linear-form], a bilinear form is a \x[bilinear-map]{c} where the \x[image-mathematics] is the \x[underlying-field-of-the-vector-space], e.g. $\R^n \times \R^m \to \R$.

Some definitions require both of the input spaces to be the same, e.g. $\R^n \times \R^n \to \R$, but it doesn't make much different in general.

The most important example of a bilinear form is the \x[dot-product]. It is only defined if both the input spaces are the same.

= Matrix representation of a bilinear form
{parent=bilinear-form}

As usual, it is useful to think about how a \x[bilinear-form] looks like in terms of \x[vector]{p} and \x[matrix]{p}.

Unlike a \x[linear-form], which \x[matrix-representation-of-a-linear-form][was a vector], because it has two inputs, the bilinear form is represented by a matrix $M$ which encodes the value for each possible pair of \x[basis-vector]{p}.

In terms of that \x[matrix], the form $B(x,y)$ is then given by:
$$
B(x,y) = x^T M y
$$

= Effect of a change of basis on the matrix of a bilinear form
{parent=matrix-representation-of-a-bilinear-form}
{title2=$B_2 = C^T B C$}

If $C$ is the \x[change-of-basis-matrix], then the \x[matrix-representation-of-a-bilinear-form] $M$ that looked like:
$$
B(x,y) = x^T M y
$$
then the matrix in the new basis is:
$$
C^T M C
$$
\x[sylvester-s-law-of-inertia] then tells us that the number of positive, negative and 0 eigenvalues of both of those matrices is the same.

Proof: the value of a given bilinear form cannot change due to a \x[change-of-basis], since the bilinear form is just a \x[function-mathematics], and does not depend on the choice of basis. The only thing that change is the matrix representation of the form. Therefore, we must have:
$$
x^T M y = x_{new}^T M_{new} y_{new}
$$
and in the new basis:
$$
x = C x_{new} \\
y = C y_{new} \\
x_{new}^T M_{new} y_{new} = x^T M y =  (Cx_{new})^T M (Cy_{new}) = x_{new}^T (C^T M C) y_{new} \\
$$
and so since:
$$
\forall x_{new}, y_{new} x_{new}^T M_{new} y_{new} = x_{new}^T (C^T M C) y_{new} \implies M_{new} = C^T M C \\
$$

Related:
* https://proofwiki.org/wiki/Matrix_of_Bilinear_Form_Under_Change_of_Basis

= Multilinear form
{parent=multilinear-map}
{wiki}

See \x[form-mathematics].

Analogous to a \x[linear-form], a multilinear form is a \x[multilinear-map]{c} where the \x[image-mathematics] is the \x[underlying-field-of-the-vector-space], e.g. $\R^{n_1} \times \R^{n_2} \times \R^{n_2} \to \R$.

= Symmetric bilinear map
{parent=multilinear-map}
{wiki}

Subcase of \x[symmetric-multilinear-map]:
$$f(x, y) = f(y, x)$$

Requires the two inputs $x$ and $y$ to be in the same \x[vector-space] of course.

The most important example is the \x[dot-product], which is also a \x[positive-definite-symmetric-bilinear-form].

= Symmetric bilinear form
{parent=symmetric-bilinear-map}
{wiki}

\x[symmetric-bilinear-map]{p} that is also a \x[bilinear-form].

= Matrix representation of a symmetric bilinear form
{parent=symmetric-bilinear-form}

= Matrix representation of the symmetric bilinear form
{synonym}

Like the \x[matrix-representation-of-a-bilinear-form], it is a \x[matrix], but now the matrix has to be a \x[symmetric-matrix].

We can then immediately see that the matrix is symmetric, then so is the form. We have:
$$
B(x,y) = x^T M y
$$
But because $B(x,y)$ is a \x[scalar], we have:
$$
B(x,y) = B(x,y)^T
$$
and:
$$
B(x,y) = B(x,y)^T = (x^T M y)^T = y^T M^T x = y^T M^T x = y^T M x = B(y,x)
$$

= Hermitian form
{c}
{parent=symmetric-bilinear-map}
{wiki}

The \x[complex-number] analogue of a \x[symmetric-bilinear-form].

The prototypical example of it is the \x[complex-dot-product].

Note that this form is neither strictly \x[symmetric-bilinear-map][symmetric], it satisfies:
$$
<x, y> = \overline{<y, x>}
$$
where the over bar indicates the \x[complex-conjugate], nor is it linear for complex scalar multiplication on the second argument.

Bibliography:
* https://mathworld.wolfram.com/HermitianForm.html

= Matrix representation of a Hermitian form
{parent=hermitian-form};

A \x[hermitian-matrix].

= Quadratic form
{parent=symmetric-bilinear-map}
{wiki}

\x[multivariate-polynomial]{c} where each term has degree 2, e.g.:
$$
f(x,y) = 2y^2 + 10yx + x^2
$$
is a quadratic form because each term has degree 2:
* $y^2$
* $xy$
* $x^2$
but e.g.:
$$
f(x,y) = 2y^2 + 10yx + x^3
$$
is not because the term $x^3$ has degree 3.

There is a \x[1-to-1] relationship between \x[quadratic-form]{p} and \x[symmetric-bilinear-form]{p}. In matrix representation, this can be written as:
$$
\vec{x}^T B \vec{x}
$$
where $\vec{x}$ contains each of the variabes of the form, e.g. for 2 variables:
$$
\vec{x} = [x, y]
$$

Strictly speaking, the associated \x[bilinear-form] would not need to be a \x[symmetric-bilinear-form], at least for the \x[real-number]{p} or \x[complex-number]{p} which are \x[commutative]. E.g.:
$$
\begin{bmatrix}x y\end{bmatrix}
\begin{bmatrix}0 & 1 \\ 2 & 0 \\ \end{bmatrix}
\begin{bmatrix}x \\ y \\ \end{bmatrix}
=
\begin{bmatrix}x y\end{bmatrix}
\begin{bmatrix}y \\ 2x \\\end{bmatrix}
= xy + 2yx
= 3xy
$$
But that same matrix could also be written in symmetric form as:
$$\begin{bmatrix}0 & 1.5 \\ 1.5 & 0 \\ \end{bmatrix}$$
so why not I guess, its simpler/more restricted.

= Positive definite symmetric bilinear form
{parent=symmetric-bilinear-map}
{wiki}

\x[symmetric-bilinear-form]{c} that is also positive definite, i.e.:
$$
\forall x, B(x, x) > 0
$$

= Matrix representation of a positive definite symmetric bilinear form
{parent=positive-definite-symmetric-bilinear-form}

A \x[positive-definite-matrix] that is also a \x[symmetric-matrix].

= Skew-symmetric bilinear map
{parent=symmetric-bilinear-map}

= Antisymmetric bilinear map
{synonym}

Subcase of \x[antisymmetric-multilinear-map]:
$$f(x, y) = -f(y, x)$$

= Skew-symmetric bilinear form
{parent=symmetric-bilinear-map}
{wiki}

\x[skew-symmetric-bilinear-map]{c} that is also a \x[bilinear-form].

= Symmetric multilinear map
{parent=multilinear-map}

Same value if you swap any input arguments.

= Antisymmetric multilinear map
{parent=symmetric-multilinear-map}

Change sign if you swap two input values.

= Alternating multilinear map
{parent=multilinear-map}

Implies \x[antisymmetric-multilinear-map].

= Dot product
{parent=linear-algebra}
{wiki}

The definition of the "dot product" of a general space varies quite a lot with different contexts.

Most definitions tend to be \x[bilinear-form]{p}.

We use the unqualified generally refers to the dot product of \x[real-coordinate-space]{p}, which is a \x[positive-definite-symmetric-bilinear-form]. Other important examples include:
* the \x[complex-dot-product], which is not strictly \x[symmetric-bilinear-map][symmetric] nor \x[linear], but it is \x[positive-definite]
* \x[minkowski-inner-product], sometimes called" "Minkowski dot product is not \x[positive-definite]
The rest of this section is about the \x[r-n] case.

The \x[positive-definite] part of the definition likely comes in because we are so familiar with \x[metric-space]{p}, which requires a positive \x[norm] in the \x[norm-induced-by-an-inner-product].

The default \x[euclidean-space] definition, we use the \x[matrix-representation-of-a-symmetric-bilinear-form] as the identity matrix, e.g. in \x[r-3]:
$$
M =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
$$
so that:
$$
\vec{x} \cdot \vec{y}
=
\begin{bmatrix}
x_1 & x_2 & x_3 \\
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
\end{bmatrix}
=
x_1y_1 + x_2y_2 + x_3y_3
$$

= Orthogonality
{parent=dot-product}
{wiki}

= Orthogonal
{synonym}

= Orthonormality
{parent=orthogonality}
{wiki}

= Orthonormal
{synonym}

= Angle
{parent=dot-product}
{wiki}

= Cross product
{parent=linear-algebra}
{title2=$\va{x} \times \va{y}$}
{wiki}

= Jacobi identity
{c}
{parent=cross-product}
{wiki}

= Index picking function
{parent=linear-algebra}

= Kronecker delta
{c}
{parent=index-picking-function}
{title2=$\delta_{ij}$}
{wiki}

= Levi-Civita symbol
{c}
{parent=index-picking-function}
{title2=$\varepsilon$}
{wiki}

Denoted by the \x[greek-letter-epsilon] with `\varepsilon` encoding in \x[latex].

Definition:
* \x[odd-permutation]: -1
* \x[even-permutation]: 1
* not a \x[permutation]: 0. This happens iff two more more indices are repeated

= Levi-Civita symbol as a tensor
{parent=levi-civita-symbol}

\x[an-introduction-to-tensors-and-group-theory-for-physicists-by-nadir-jeevanjee-2011]{c} shows that this is a \x[tensor] that represents the \x[volume-of-a-parallelepiped].

It takes as input three vectors, and outputs one real number, the volume. And it is linear on each vector. This perfectly satisfied the definition of a tensor of \x[order-of-a-tensor][order] (3,0).

Given a basis $(e_i, e_j, e_k)$ and a function that return the volume of a parallelepiped given by three vectors $V(v_1, v_2, v_3)$, $\varepsilon_{ikj} = V(e_i, e_j, e_k)$.

= Projection
{disambiguate=mathematics}
{parent=linear-algebra}

= Matrix
{parent=linear-algebra}
{wiki}

= Matrix operation
{parent=matrix}
{wiki}

= Determinant
{parent=matrix-operation}
{title2=$det$}
{wiki}

Name origin: likely because it "determines" if a matrix is \x[invertible-matrix][invertible] or not, as a matrix is invertible iff determinant is not zero.

= Matrix inverse
{parent=matrix-operation}
{title2=$M^{-1}$}

When it exists, which is not for all matrices, only \x[invertible-matrix], the inverse is denoted:
$$
M^{-1}
$$

= Invertible matrix
{parent=matrix-inverse}
{tag=named-matrix}
{wiki}

The set of all \x[invertible-matrix]{p} forms a \x[group]: the \x[general-linear-group] with \x[matrix-multiplication]. Non-invertible matrices don't form a group due to the lack of inverse.

= Transpose
{parent=matrix-operation}
{title2=$M^T$}
{wiki}

= Transpose of a matrix multiplication
{parent=transpose}
{wiki}

When it distributes it inverts the order of the \x[matrix-multiplication]:
$$(MN)^T = N^T M^T$$

= Inverse of the transpose
{parent=transpose}
{wiki}

The \x[transpose] and \x[matrix-inverse] commute:
$$
(M^T)-1 = (M^{-1})^T
$$

= Matrix multiplication
{parent=matrix}
{wiki}

= Matrix product
{synonym}

Since a \x[matrix] $M$ can be seen as a \x[linear-map] $f_M(\vec{x})$, the product of two matrices $MN$ can be seen as the composition of two \x[linear-map]{p}:
$$f_M(f_N(\vec{x}))$$
One cool thing about linear functions is that we can easily pre-calculate this product only once to obtain a new matrix, and so we don't have to do both multiplications separately each time.

= Matrix multiplication algorithm
{parent=matrix-multiplication}
{tag=computational-problem}

https://math.stackexchange.com/questions/30330/fast-algorithm-for-solving-system-of-linear-equations/259372#259372

= Matrix decomposition
{parent=matrix-multiplication}
{wiki}

= Eigenvalues and eigenvectors
{parent=matrix}
{wiki}

= Applications of eigenvalues and eigenvectors
{parent=eigenvalues-and-eigenvectors}

* https://math.stackexchange.com/questions/23312/what-is-the-importance-of-eigenvalues-eigenvectors/3503875#3503875
* https://math.stackexchange.com/questions/1520832/real-life-examples-for-eigenvalues-eigenvectors
* https://matheducators.stackexchange.com/questions/520/what-is-a-good-motivation-showcase-for-a-student-for-the-study-of-eigenvalues

= Characteristic polynomial
{parent=eigenvalues-and-eigenvectors}
{wiki}

= Eigenvalue
{parent=eigenvalues-and-eigenvectors}

See: \x[eigenvalues-and-eigenvectors].

= Spectrum
{disambiguate=functional analysis}
{parent=eigenvalue}

Set of \x[eigenvalue]{p} of a \x[linear-operator].

= Continuous spectrum
{disambiguate=functional analysis}
{parent=spectrum-functional-analysis}

Unlike the simple case of a \x[matrix], in \x[infinite-dimensional] vector spaces, the spectrum may be continuous.

The quintessential example of that is the spectrum of the \x[position-operator] in \x[quantum-mechanics], in which any \x[real-number] is a possible \x[eigenvalue], since the particle may be found in any position. The associated \x[eigenvector]{p} are the corresponding \x[dirac-delta-function]{p}.

= Eigendecomposition of a matrix
{parent=eigenvalues-and-eigenvectors}
{wiki}

= Eigendecomposition
{synonym}

Every \x[invertible-matrix] $M$ can be written as:
$$
M = QDQ^{-1}
$$
where:
* $D$ is a \x[diagonal-matrix] containing the \x[eigenvalue]{p} of $M$
* columns of $Q$ are \x[eigenvector]{p} of $M$
Note therefore that this decomposition is unique up to swapping the order of eigenvectors. We could fix a canonical form by sorting eigenvectors from smallest to largest in the case of a \x[real-number].

Intuitively, Note that this is just the \x[change-of-basis] formula, and so:
* $Q^{-1}$ changes basis to align to the eigenvectors
* $D$ multiplies eigenvectors simply by eigenvalues
* $Q$ changes back to the original basis

= Eigendecomposition of a real symmetric matrix
{parent=eigendecomposition-of-a-matrix}

= The eigendecomposition of a real symmetric matrix is done with orthogonal matrices
{synonym}

The general result from \x[eigendecomposition-of-a-matrix]:
$$
M = QDQ^{-1}
$$
becomes:
$$
M = ODO^T
$$
where $O$ is an \x[orthogonal-matrix], and therefore has $O^{-1} = O^T$.

= Sylvester's law of inertia
{c}
{parent=eigendecomposition-of-a-matrix}
{wiki}

The main interest of this theorem is in \x[classifying-mathematics] the \x[indefinite-orthogonal-group]{p}, which in turn is fundamental because the \x[lorentz-group] is an \x[indefinite-orthogonal-group]{p}, see: \x[all-indefinite-orthogonal-groups-of-matrices-of-equal-metric-signature-are-isomorphic].

It also tells us that a \x[change-of-basis] does not the alter the \x[metric-signature] of a \x[bilinear-form], see \x[matrix-congruence-can-be-seen-as-the-change-of-basis-of-a-bilinear-form].

The theorem states that the number of 0, 1 and -1 in the \x[metric-signature] is the same for two \x[symmetric-matrix]{p} that are \x[congruent-matrix]{p}.

For example, consider:
$$
A = \begin{bmatrix}2 & \sqrt{2} \\ \sqrt{2} & 3 \\\end{bmatrix}
$$

The \x[eigenvalue]{p} of $A$ are $1$ and $4$, and the associated eigenvectors are:
$$
v_1 = [-\sqrt{2}, 1]^T
v_4 = [\sqrt{2}/2, 1]^T
$$
\x[sympy] code:
``
A = Matrix([[2, sqrt(2)], [sqrt(2), 3]])
A.eigenvects()
``
and from the \x[eigendecomposition-of-a-real-symmetric-matrix] we know that:
$$
A = PDP^T =
\begin{bmatrix}-\sqrt{2} & \sqrt{2}/2 \\ 1 & 1\\\end{bmatrix}
\begin{bmatrix}1 & 0 \\ 0 & 4\\\end{bmatrix}
\begin{bmatrix}-\sqrt{2} & 1 \\ \sqrt{2}/2 & 1\\\end{bmatrix}
$$

Now, instead of $P$, we could use $PE$, where $E$ is an arbitrary \x[diagonal-matrix] of type:
$$
\begin{bmatrix}e_1 & 0 \\ 0 & e_2\\\end{bmatrix}
$$
With this, would reach a new matrix $B$:
$$
B = (PE)D(PE)^T = P(EDE^T)P^T = P(EED)P^T
$$
Therefore, with this congruence, we are able to multiply the eigenvalues of $A$ by any positive number $e_1^2$ and $e_2^2$. Since we are multiplying by two arbitrary positive numbers, we cannot change the signs of the original eigenvalues, and so the \x[metric-signature] is maintained, but respecting that any value can be reached.

Note that the \x[matrix-congruence] relation looks a bit like the \x[eigendecomposition-of-a-matrix]:
$$
D = SMS^T
$$
but note that $D$ does not have to contain \x[eigenvalue]{p}, unlike the \x[eigendecomposition-of-a-matrix]. This is because here $S$ is not fixed to having \x[eigenvector]{p} in its columns.

But because the matrix is symmetric however, we could always choose $S$ to actually diagonalize as mentioned at \x[eigendecomposition-of-a-real-symmetric-matrix]. Therefore, the \x[metric-signature] can be seen directly from \x[eigenvalue]{p}.

Also, because $D$ is a \x[diagonal-matrix], and thus symmetric, it must be that:
$$
S^T = S^{-1}
$$

What this does represent, is a general \x[change-of-basis] that maintains the matrix a \x[symmetric-matrix].

Related:
* https://math.stackexchange.com/questions/1817906/sylvesters-law-of-inertia
* https://math.stackexchange.com/questions/1284601/what-is-the-lie-group-that-leaves-this-matrix-invariant
* https://physics.stackexchange.com/questions/24495/metric-signature-explanation

= Congruent matrix
{parent=sylvester-s-law-of-inertia}

= Matrix congruence
{synonym}

Two \x[symmetric-matrix]{p} $A$ and $B$ are defined to be congruent if there exists an $S$ in \x[gl-n] such that:
$$
A = S B S^T
$$

= Matrix congruence can be seen as the change of basis of a bilinear form
{parent=congruent-matrix}

From \x[effect-of-a-change-of-basis-on-the-matrix-of-a-bilinear-form], remember that a change of basis $C$ modifies the \x[matrix-representation-of-a-bilinear-form] as:
$$
C^T M C
$$

So, by taking $S = C^T$, we understand that two matrices being congruent means that they can both correspond to the same \x[bilinear-form] in different bases.

= Matrix similarity
{parent=sylvester-s-law-of-inertia}
{wiki}

= Similar matrix
{synonym}

= Metric signature
{parent=sylvester-s-law-of-inertia}
{wiki}

= Metric signature matrix
{parent=metric-signature}

= Eigenvector
{parent=eigenvalues-and-eigenvectors}

See: \x[eigenvalues-and-eigenvectors].

= Eigenvectors and eigenvalues of the identity matrix
{parent=eigenvalues-and-eigenvectors}

https://math.stackexchange.com/questions/1507290/linear-algebra-identity-matrix-and-its-relation-to-eigenvalues-and-eigenvectors/3934023#3934023

= Spectral theorem
{parent=eigenvalues-and-eigenvectors}
{wiki}

= Hermitian matrix
{c}
{parent=spectral-theorem}
{wiki}

= Hermitian operator
{c}
{parent=hermitian-matrix}

This is the possibly infinite dimensional version of a \x[hermitian-matrix], since \x[linear-operator]{p} are the possibly infinite dimensional version of \x[matrix]{p}.

There's a catch though: now we don't have explicit matrix indices here however in general, the generalized definition is shown at: https://en.wikipedia.org/w/index.php?title=Hermitian_adjoint&oldid=1032475701#Definition_for_bounded_operators_between_Hilbert_spaces

= Riesz representation theorem
{c}
{parent=hermitian-operator}

= Kronecker product
{c}
{parent=matrix}
{wiki}

= Named matrix
{parent=matrix}
{wiki=List_of_named_matrices}

= Diagonal matrix
{parent=named-matrix}
{wiki}

Forms a \x[normal-subgroup] of the \x[general-linear-group].

= Scalar matrix
{parent=diagonal-matrix}
{title2=$Z(V)$}
{wiki=Diagonal_matrix#Scalar_matrix}

Forms a \x[normal-subgroup] of the \x[general-linear-group].

= Identity matrix
{parent=scalar-matrix}
{title2=$I_n$}
{wiki}

= Square matrix
{parent=named-matrix}
{wiki}

= Matrix ring
{parent=square-matrix}
{wiki}

= Matrix ring of degree n
{title2}
{synonym}

= $M_n$
{title2}
{synonym}

= Set of all n-by-y square matrices
{title2}
{synonym}

The matrix ring of degree n $M_n$ is the set of all n-by-n square matrices together with the usual \x[vector-space] and \x[matrix-multiplication] operations.

This set forms a \x[ring-mathematics].

Related terminology:
* https://math.stackexchange.com/questions/412200/what-is-the-notation-for-the-set-of-all-m-times-n-matrices

= Orthogonal matrix
{parent=named-matrix}
{wiki}

Members of the \x[orthogonal-group].

= Unitary matrix
{parent=orthogonal-matrix}
{wiki}

\x[complex-number][Complex]{c} analogue of \x[orthogonal-matrix].

Applications:
* in \x[quantum-computer]{p} programming basically comes down to creating one big unitary matrix as explained at: \x[quantum-computing-is-just-matrix-multiplication]

= Triangular matrix
{parent=named-matrix}
{wiki}

= Symmetric matrix
{parent=named-matrix}
{wiki}

A \x[matrix] that equals its \x[transpose]:
$$
M = M^T
$$

Can represent a \x[symmetric-bilinear-form] as shown at \x[matrix-representation-of-a-symmetric-bilinear-form], or a \x[quadratic-form].

= Definite matrix
{parent=symmetric-matrix}
{wiki}

The definition implies that this is also a \x[symmetric-matrix].

= Positive definite matrix
{parent=definite-matrix}

= Positive definite
{synonym}

The \x[dot-product] is a \x[positive-definite-matrix], and so we see that those will have an important link to familiar \x[geometry].

= Skew-symmetric matrix
{parent=symmetric-matrix}
{wiki}

= Antisymmetric matrix
{title2}
{synonym}

WTF is a skew? "Antisymmetric" is just such a better name! And it also appears in other definitions such as \x[antisymmetric-multilinear-map].

= Skew-symmetric form
{parent=skew-symmetric-matrix}

= Vector space
{parent=linear-algebra}
{wiki}

= Basis
{disambiguate=linear algebra}
{parent=vector-space}
{wiki}

= Basis
{synonym}

= Basis vector
{synonym}

= Change of basis
{parent=basis-linear-algebra}
{wiki}

$$N = BMB^{-1}$$
where:
* $M$: matrix in the old basis
* $N$: matrix in the new basis
* $B$: change of basis matrix

= Change of basis matrix
{parent=change-of-basis}

The change of basis matrix $C$ is the matrix that allows us to express the new basis in an old basis:
$$x_{old} = Cx_{new}$$

Mnemonic is as follows: consider we have an initial basis $(x_{old}, y_{old})$. Now, we define the new basis in terms of the old basis, e.g.:
$$
\begin{aligned}
x_{new} &= 1x_{old} + 2y_{old} \\
y_{new} &= 3x_{old} + 4y_{old} \\
\end{aligned}
$$
which can be written in matrix form as:
$$
\begin{bmatrix}x_{new} \\ y_{new} \\\end{bmatrix} =
\begin{bmatrix}1 && 2 \\ 3 && 4 \\\end{bmatrix}
\begin{bmatrix}x_{old} \\ y_{old} \\\end{bmatrix}
$$
and so if we set:
$$
M = \begin{bmatrix}1 && 2 \\ 3 && 4 \\\end{bmatrix}
$$
we have:
$$
\vec{x_{new}} = M\vec{x_{old}}
$$

The usual question then is: given a vector in the new basis, how do we represent it in the old basis?

The answer is that we simply have to calculate the \x[matrix-inverse] of $M$:
$$
\vec{x_{old}} =  M^{-1}\vec{x_{new}}
$$

That $M^{-1}$ is the matrix inverse.

= Change of basis between symmetric matrices
{parent=change-of-basis}

When we have a \x[symmetric-matrix], a \x[change-of-basis] keeps symmetry iff it is done by an \x[orthogonal-matrix], in which case:
$$N = BMB^{-1} = OMO^T$$

= Linear independence
{parent=basis-linear-algebra}
{wiki}

= Linearly independent
{synonym}

= Classification of vector spaces
{parent=vector-space}

= All vector spaces of the same dimension on a given field are isomorphic
{synonym}

https://en.wikipedia.org/wiki/Dimension_(vector_space)#Facts

= Underlying field of a vector space
{parent=vector-space}

= Underlying field of the vector space
{synonym}

Every vector space is defined over a \x[field-mathematics].

E.g. in $\R^3$, the underlying \x[field-mathematics] is $\R$, the \x[real-number]{p}. And in $\C^2$ the underlying field is $\C$, the \x[complex-number]{p}.

Any field can be used, including \x[finite-field]. But the underlying thing has to be a field, because the definitions of a vector need all field properties to hold to make sense.

Elements of the underlying field of a vector space are known as \x[scalar-mathematics].

= Vector
{disambiguate=mathematics}
{parent=vector-space}
{wiki=Vector (mathematics and physics)}

= Vector
{synonym}

= Vectorized
{synonym}

= Scalar
{disambiguate=mathematics}
{parent=vector-mathematics}
{wiki}

= Scalar
{synonym}

A member of the \x[underlying-field-of-a-vector-space]. E.g. in $\R^3$, the underlying field is $\R$, and a scalar is a member of $\R$, i.e. a \x[real-number].

= Tensor
{parent=linear-algebra}
{wiki}

A \x[multilinear-form] with a \x[domain-function] that looks like:
$$
V^m \times {V*}^n \to \R
$$
where $V*$ is the \x[dual-space].

Because a tensor is a \x[multilinear-form], it can be fully specified by how it act on all combinations of basis sets, which can be done in terms of components. We refer to each component as:
$$
T_{i_1 \ldots i_m}^{j_1 \ldots j_n} = T(e_{i_1}, \ldots, e_{i_m}, e^{j_1}, \ldots, e^{j_m})
$$
where we remember that the raised indices refer \x[dual-vector].

Some examples:
* \x[levi-civita-symbol-as-a-tensor]{child}
* \x[a-linear-map-is-a-1-1-tensor]

= A linear map is a (1,1) tensor
{parent=tensor}

A linear map $A$ can be seen as a (1,1) \x[tensor] because:
$$
T(w, v*) = v* A w
$$
is a number, $v*$. is a \x[dual-vector], and \x[w] is a \x[vector]. Furthermoe, $T$ is linear in both $v*$ and $w$. All of this makes $T$ fullfill the definition of a (1,1) tensor.

= Tensor space
{parent=tensor}
{title2=$T^{(m, n)}$}

Bibliography:
* https://mathworld.wolfram.com/TensorSpace.html

= Order of a tensor
{parent=tensor-space}

$T^{(m, n)}$ has order $(m, n)$

= Einstein notation
{c}
{parent=tensor}
{wiki}

= Einstein summation convention
{c}
{synonym}
{title2}

The https://en.wikipedia.org/w/index.php?title=Einstein_notation&oldid=1021244532[Wikipedia page] of this article is basically a masterclass why \x[ourbigbook-com/wikipedia][Wikipedia is useless for learning technical subjects]. They are not even able to teach such a simple subject properly there!

Bibliography:
* https://www.maths.cam.ac.uk/postgrad/part-iii/files/misc/index-notation.pdf gives a definition that does not consider upper and lower indexes, it only counts how many times the indices appear

  Their definition of the \x[laplacian] is a bit wrong as only one $i$ appears in it, they likely meant to have written $\pdv{}{x_i}\pdv{F}{x_i}$ instead of $\pdv{^2 F}{x_i^2}$, related: 

= Raised and lowered indices
{parent=einstein-notation}

TODO what is the point of them? Why not just sum over every index that appears twice, regardless of where it is, as mentioned at: https://www.maths.cam.ac.uk/postgrad/part-iii/files/misc/index-notation.pdf[].

Vectors with the index on top such as $x^i$ are the "regular vectors", they are called \x[covariant-vector]{p}.

Those in indices on bottom are called \x[contravariant-vector]{p}.

It is possible to change between them by \x[raising-and-lowering-indices].

The values are different only when the \x[metric-signature-matrix] is different from the \x[identity-matrix].

= Raised index
{parent=raised-and-lowered-indices}

= Lowered index
{parent=raised-and-lowered-indices}

= Raising and lowering indices
{c}
{parent=raised-and-lowered-indices}
{wiki}

= Implicit metric signature in Einstein notation
{parent=einstein-notation}

Then a specific \x[metric] is involved, sometimes we want to automatically add it to products.

E.g., in a context considering the common \x[minkowski-inner-product-matrix] where the $\eta$ 4x4 matrix and $\mu$ is a vector in \x[r-4]
$$
x^{\mu} x_{\mu} = x^{\mu} \eta_{\mu \nu} x^{\nu} = -x_0^2 + x_1^2 + x_2^2 + x_3^2;
$$
which leads to the change of sign of some terms.

= Einstein notation for partial derivatives
{parent=einstein-notation}

The \x[einstein-summation-convention] works will with \x[partial-derivative], and this case is widely used in \x[particle-physics].

\x[partial-index-partial-derivative-notation]{c} is the \x[partial-derivative-notation] commonly used in this context, as we want to do operations by index rather than by labels such as $x$, $y$, $z$.

This notation also allows us to have \x[raised-and-lowered-indices] on the \x[partial-derivative-symbol] TODO how are they different?

= Divergence in Einstein notation
{parent=einstein-notation-for-partial-derivatives}

Given a vector function of three variables:
$$
F(x_0, x_1, x_2) = (F^0(x_0, x_1, x_2), F^1(x_0, x_1, x_2), F^2(x_0, x_1, x_2)) : \R^3 \to \R^3
$$
so note that we are denoting each component of $F$ as $F^i$ with a \x[raised-index].

Then, the \x[divergence] can be written in \x[einstein-notation] as:
$$
\div{F} = \partial_i F^i(x_0, x_1, x_2) = \pdv{F^i(x_0, x_1, x_2)}{x^i} = \pdv{F^0(x_0, x_1, x_2)}{x_0} + \pdv{F^1(x_0, x_1, x_2)}{x_1} + \pdv{F^2(x_0, x_1, x_2)}{x_2}
$$

It is common to just omit the variables of the function, so we tend to just say:
$$
\div{F} = \partial_i F^i
$$
or equivalently when referring just to the operation:
$$
\div{} = \partial_i
$$

= Laplacian in Einstein notation
{c}
{parent=einstein-notation-for-partial-derivatives}
{title2=$\partial_i \partial^i$}

Given a real function of three variables:
$$
F(x_0, x_1, x_2) = : \R^3 \to \R
$$
its \x[laplacian] can be written as:
$$
\laplacian{F(x_0, x_1, x_2)} = \partial_i \partial^i F(x_0, x_1, x_2) = \\
\partial_0 \partial^0 F(x_0, x_1, x_2) + \partial_1 \partial^1 F(x_0, x_1, x_2) + \partial_2 \partial^2 F(x_0, x_1, x_2) \\
\partial_0^2 F(x_0, x_1, x_2) + \partial_1^2 F(x_0, x_1, x_2) + \partial_2^2 F(x_0, x_1, x_2)
$$

It is common to just omit the variables of the function, so we tend to just say:
$$
\laplacian{F} = \partial_i \partial^i F
$$
or equivalently when referring just to the operation:
$$
\laplacian{} = \partial_i \partial^i
$$

= D'alembert operator in Einstein notation
{c}
{parent=laplacian-in-einstein-notation}
{title2=$\partial_i \partial^i$}

Given the function $\psi$:
$$
\psi : \R^4 \to \C
$$
the operator can be written in \x[planck-units] as:
$$
\partial_i \partial^i \psi(x_0, x_1, x_2, x_3) - m^2 \psi(x_0, x_1, x_2, x_3) = 0
$$
often written without function arguments as:
$$
\partial_i \partial^i \psi
$$
Note how this looks just like the \x[laplacian-in-einstein-notation],  since the \x[d-alembert-operator] is just a generalization of the \x[laplace-operator] to \x[minkowski-space].

= Klein-Gordon equation in Einstein notation
{parent=d-alembert-operator-in-einstein-notation}

The \x[klein-gordon-equation] can be written in terms of the \x[d-alembert-operator] as:
$$
\Box \psi + m^2 \psi = 0
$$
so we can expand the \x[d-alembert-operator-in-einstein-notation] to:
$$
\partial_i \partial^i \psi - m^2 \psi = 0
$$

= Covariance and contravariance of vectors
{parent=einstein-notation}
{wiki}

= Covariant vector
{parent=covariance-and-contravariance-of-vectors}

= Contravariant vector
{parent=covariance-and-contravariance-of-vectors}

= Linear algebra bibliography
{parent=linear-algebra}

https://textbooks.math.gatech.edu/ila/index.html Interactive Linear Algebra. Source: https://github.com/QBobWatson/ila[]. Written in \x[mathbook-xml].

= Group
{disambiguate=mathematics}
{parent=algebra}
{wiki}

= Group
{synonym}

= Center
{disambiguate=group theory}
{parent=group-mathematics}
{wiki}

= Center
{disambiguate=group}
{synonym}

= Commutative property
{parent=group-mathematics}
{wiki}

= Commutative
{synonym}

= Commutativity
{synonym}

= Abelian group
{c}
{parent=commutative-property}
{wiki}

= Abelian
{c}
{synonym}

Easily classified as the \x[direct-product-of-groups][direct product] of \x[cyclic-group]{p} of \x[prime-number][prime] order.

= Non-commutative
{parent=abelian-group}

= Symmetry
{parent=group-mathematics}
{wiki}

Directly modelled by \x[group-mathematics]{p}.

For \x[continuous-symmetry]{p}, see: \x[lie-group].

https://en.wikipedia.org/wiki/Generating_set_of_a_group

= Important mathematical group
{parent=group-mathematics}

= Important discrete mathematical group
{parent=important-mathematical-group}

= Cyclic group
{parent=important-discrete-mathematical-group}
{wiki}

= $C_n$
{synonym}
{title2}

= The direct product of two cyclic groups of coprime order is another cyclic group
{parent=important-discrete-mathematical-group}
{wiki}

You just map the value (1, 1) $C_m \times C_n$ to the value 1 of $C_{mn}$, and it works out. E.g. for $C_2 \times C_3$, the \x[generating-set-of-a-group][group generated by] of (1, 1) is:
``
0 = (0, 0)
1 = (1, 1)
2 = (0, 2)
3 = (1, 0)
4 = (0, 1)
5 = (1, 2)
6 = (0, 0) = 0
``

= Permutation
{parent=important-discrete-mathematical-group}
{wiki}

= Cycle notation
{parent=permutation}
{wiki}

A concise to describe a specific \x[permutation].

A permutation group can then be described in terms of the \x[generating-set-of-a-group] of specific elements given in cycle notation.

E.g. https://en.wikipedia.org/w/index.php?title=Mathieu_group&oldid=1034060469#Permutation_groups mentions that the \x[mathieu-group-m-11] is generated by three elements:
* (0123456789a)
* (0b)(1a)(25)(37)(48)(69)
* (26a7)(3945)
which feels quite compact for a \x[simple-group] with 95040 elements, doesn't it!

= Parity of a permutation
{parent=permutation}
{wiki}

= Odd permutation
{parent=parity-of-a-permutation}

= Even permutation
{parent=parity-of-a-permutation}

= Permutation group
{parent=permutation}
{wiki}

= Stabilizer
{disambiguate=group}
{parent=permutation-group}

Suppose we have a given \x[permutation-group] that acts on a set of n elements.

If we pick k elements of the set, the stabilizer subgroup of those k elements is a subgroup of the given permutation group that keeps those elements unchanged.

Note that an analogous definition can be given for non-finite groups. Also note that the case for all finite groups is covered by the permutation definition since \x[all-groups-are-isomorphic-to-a-subgroup-of-the-symmetric-group]

TODO existence and uniqueness. Existence is obvious for the identity permutation, but proper subgroup likely does not exist in general.

Bibliography:
* https://mathworld.wolfram.com/Stabilizer.html
* https://ncatlab.org/nlab/show/stabilizer+group from \x[nlab]

= Symmetric group
{parent=permutation-group}
{wiki}

\x[group]{c} of all \x[permutation]{p}.

= All groups are isomorphic to a subgroup of the symmetric group
{parent=symmetric-group}

Or in other words: \x[symmetric-group]{p} are boring, because they are basically everything already!

= Alternating group
{parent=permutation-group}
{wiki}

= $A_n$
{synonym}
{title2}

Group of \x[even-permutation]{p}.

Note that \x[odd-permutation]{p} don't form a \x[subgroup] of the \x[symmetric-group] like the even permutations do, because the composition of two odd permutations is an even permutation.

= Alternating group of degree 5
{parent=alternating-group}

= The alternating groups of degree 5 or greater are simple
{parent=alternating-group-of-degree-5}

https://www.youtube.com/watch?v=U_618kB6P1Q GT18.2. A_n is Simple (n ge 5) by \x[mathdoctorbob] (2012)

= Dihedral group
{parent=important-discrete-mathematical-group}
{title2=$D_n$}
{wiki}

Our notation: $D_n$, called "dihedral group of degree n", means the dihedral group of the \x[regular-polygon] with $n$ sides, and therefore has order $2n$ (all rotations + flips), called the "dihedral group of \x[order-algebra] 2n".

= Wallpaper group
{parent=important-discrete-mathematical-group}
{wiki}

17 of them.

= Space group
{parent=important-discrete-mathematical-group}
{tag=crystallography}
{wiki}

All possible repetitive crystal structures!

219 of them.

= Klein four-group
{c}
{parent=important-discrete-mathematical-group}
{wiki}

$C_2 \times C_2$

= Finite group
{parent=group-mathematics}

= Classification of finite groups
{parent=finite-group}

As shown in \x[video-simple-groups-abstract-algebra-by-socratica-2018], this can be split up into two steps:
* \x[classification-of-finite-simple-groups]: done
* \x[group-extension-problem]
This split is sometimes called the "Jordan-Hölder program" in reference to the authors of the \x[jordan-holder-theorem].

Good lists to start playing with:

History: https://math.stackexchange.com/questions/1587387/historical-notes-on-the-jordan-h%C3%B6lder-program

= List of finite groups
{parent=classification-of-finite-groups}

* https://en.wikipedia.org/wiki/List_of_small_groups

= GroupNames
{c}
{parent=list-of-finite-groups}
{wiki}

https://people.maths.bris.ac.uk/~matyd/GroupNames/index.html

This dude has done well.

= Classification of finite simple groups
{parent=classification-of-finite-groups}
{wiki}

= Classification of simple finite groups
{synonym}

\x[ciro-santilli] is very fond of this result: \x[the-beauty-of-mathematics].

How can so much complexity come out from so few rules?

How can the proof be so long (thousands of papers)?? Surprise!!

And to top if all off, the awesomely named \x[monster-group] could have a relationship with \x[string-theory] via the \x[monstrous-moonshine]?

\x[all-science-is-either-physics-or-stamp-collecting] comes to mind.

The classification contains:
* \x[cyclic-group]{p}: infinitely many, one for each \x[prime] order. Non-prime orders are not simple. These are the only \x[abelian] ones.
* \x[alternating-group]{p} of order 4 or greater: infinitely many
* \x[groups-of-lie-type]: a contains several infinite families
* \x[sporadic-group]{p}: 26 or 27 of them depending on definitions

\Video[https://www.youtube.com/watch?v=jhVMBXl5jTA]
{title=Simple Groups - Abstract Algebra by Socratica (2018)}
{description=Good quick overview.}

= Group of Lie type
{parent=classification-of-finite-simple-groups}
{wiki}

= Groups of Lie type
{synonym}

In the \x[classification-of-finite-simple-groups], groups of Lie type are a set of infinite families of simple lie groups. These are the other infinite families besides te \x[cyclic-group]{p} and \x[alternating-group]{p}.

A decent list at: https://en.wikipedia.org/wiki/List_of_finite_simple_groups[], https://en.wikipedia.org/wiki/Group_of_Lie_type[] is just too unclear. The groups of Lie type can be subdivided into:
* \x[chevalley-group]{child}{p}
* TODO the rest

The first in this family discovered were a subset of the \x[chevalley-groups-a-n-q] by \x[galois]: \x[psl-2-p], so it might be a good first one to try and understand what it looks like.

TODO understand intuitively why they are called of Lie type. Their names $A_n$, $B_n$ seem to correspond to the members of the \x[classification-of-simple-lie-groups] which are also named like that.

But they are of course related to \x[lie-group]{p}, and as suggested at \x[video-yang-mills-1-by-david-metzler-2011] part 2, the continuity actually simplifies things.

= Chevalley group
{c}
{parent=group-of-lie-type}
{wiki}

= Chevalley groups $A_n(q)$
{c}
{parent=chevalley-group}

They are the \x[finite-projective-special-linear-group]{p}.

This was the first infinite family of \x[simple-group]{p} discovered after the simple \x[cyclic-group]{p} and \x[alternating-group]{p}. The first case discovered was \x[psl-2-p] by \x[galois]. You should understand that one first.

= Sporadic group
{parent=classification-of-finite-simple-groups}
{wiki}

Examples of \x[exceptional-object]{p}{parent}.

= Mathieu group
{c}
{parent=sporadic-group}
{wiki}

Contains the first \x[sporadic-group]{p} discovered by far: 11 and 12 in 1861, and 22, 23 and 24 in 1973. And therefore presumably the simplest! The next sporadic ones discovered were the \x[janko-group]{p}, only in 1965!

Each \x[m-n] is a \x[permutation-group] on $n$ elements. There isn't an obvious algorithmic relationship between $n$ and the actual group.

TODO initial motivation? Why did Mathieu care about \x[k-transitive-group]{p}?

Their; \x[k-transitive-group] properties seem to be the main characterization, according to Wikipedia:
* 22 is 3-transitive but not 4-transitive.
* four of them (11, 12, 23 and 24) are the only \x[sporadic-group][sporadic] \x[k-transitive-group][4-transitive] groups as per the \x[classification-of-4-transitive-groups] (no known simpler proof as of 2021), which sounds like a reasonable characterization. Note that 12 and 25 are also 5 transitive.
Looking at the \x[classification-of-k-transitive-groups] we see that the Mathieu groups are the only families of 4 and 5 transitive groups other than \x[symmetric-group]{p} and \x[alternating-group]{p}. 3-transitive is not as nice, so let's just say it is the \x[stabilizer-group] of $M_23$ and be done with it.

= k-transitive group
{parent=mathieu-group}

TODO why do we care about this?

Note that if a group is k-transitive, then it is also k-1-transitive.

= Classification of k-transitive groups
{parent=k-transitive-group}

TODO this would give a better motivation for the \x[mathieu-group]

Higher transitivity: https://mathoverflow.net/questions/5993/highly-transitive-groups-without-assuming-the-classification-of-finite-simple-g

= 2-transitive group
{parent=classification-of-k-transitive-groups}
{wiki}

= Classification of 2-transitive groups
{parent=2-transitive-group}

= Classification of 3-transitive groups
{parent=classification-of-k-transitive-groups}

Might be a bit complex: https://math.stackexchange.com/questions/698327/classification-of-triply-transitive-finite-groups

= Classification of 4-transitive groups
{parent=classification-of-k-transitive-groups}

https://en.wikipedia.org/w/index.php?title=Mathieu_group&oldid=1034060469#Multiply_transitive_groups is a nice characterization of 4 of the \x[mathieu-group]{p}.

= Classification of 5-transitive groups
{parent=classification-of-k-transitive-groups}

Apparently only \x[mathieu-group-m-12] and \x[mathieu-group-m-24].

http://www.maths.qmul.ac.uk/~pjc/pps/pps9.pdf mentions:
\Q[
The automorphism group of the extended Golay code is the 54-transitive Mathieu group $M_{24}$. This is one of only two finite 5-transitive groups other than symmetric and alternating groups
]
Hmm, is that 54, or more likely 5 and 4?

https://scite.ai/reports/4-homogeneous-groups-EAKY21 quotes https://link.springer.com/article/10.1007%2FBF01111290 which suggests that is is also another one of the Mathieu groups, https://math.stackexchange.com/questions/698327/classification-of-triply-transitive-finite-groups#comment7650505_3721840 and https://en.wikipedia.org/wiki/Mathieu_group_M12 mentions M_12.

= Classification of 6-transitive groups
{parent=classification-of-k-transitive-groups}

https://math.stackexchange.com/questions/700235/is-there-an-easy-proof-for-the-classification-of-6-transitive-finite-groups says there aren't any non-boring ones.

= Mathieu group $M_11$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M11}

= Mathieu group $M_12$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M12}

= Mathieu group $M_22$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M22}

= Mathieu group $M_23$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M23}

= Mathieu group $M_24$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M24}

https://math.stackexchange.com/questions/698327/classification-of-triply-transitive-finite-groups

A master thesis reviewing its results: https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=5051&context=etd_theses

= Janko group
{c}
{parent=sporadic-group}
{wiki}

= Monster group
{parent=sporadic-group}
{wiki}

\Video[https://www.youtube.com/watch?v=mH0oCDa74tE]
{title=Group theory, abstraction, and the 196,883-dimensional monster by \x[3blue1brown] (2020)}
{description=Too basic, starts motivating groups themselves, therefore does not give anything new or rare.}

= Monstrous moonshine
{parent=monster-group}
{wiki}

TODO \x[clickbait], or is it that good?

= Jordan-Holder Theorem
{parent=classification-of-finite-simple-groups}
{wiki=Composition_series#Uniqueness:_Jordan–Hölder_theorem}

Uniqueness results for the \x[composition-series] of a group.

= Composition series
{parent=classification-of-finite-simple-groups}
{wiki}

= Group extension problem
{parent=classification-of-finite-groups}
{wiki=Group_extension}

Besides the understandable Wikipedia definition, \x[video-simple-groups-abstract-algebra-by-socratica-2018] gives an understandable one:
\Q[
Given a finite group $F$ and a simple group $S$, find all groups $G$ such that $N$ is a \x[normal-subgroup] of $G$ and $G/N = S$.
]

We don't really know how to make up larger groups from smaller simple groups, which would complete the \x[classification-of-finite-groups]:
* https://math.stackexchange.com/questions/25315/how-is-a-group-made-up-of-simple-groups

In particular, this is hard because you can't just take the \x[direct-product-of-groups] to retrieve the original group: \x[relationship-between-the-quotient-group-and-direct-products]{full}.

= Group operation
{parent=group-mathematics}

= Group product
{synonym}

= Group isomorphism
{parent=group-mathematics}
{wiki}

= Isomorphism
{parent=group-isomorphism}
{wiki}

= Isomorphic
{synonym}

Something analogous to a \x[group-isomorphism], but that preserves whatever properties the given algebraic object has. E.g. for a \x[field-mathematics], we also have to preserve multiplication in addition to addition.

Other common examples include isomorphisms of \x[vector-space]{p} and \x[field-mathematics]{p}. But since both of those two are much simpler than groups in \x[classification-mathematics], as they are both determined by number of elements/dimension alone, see:
* \x[classification-of-finite-fields]
* \x[all-vector-spaces-of-the-same-dimension-on-a-given-field-are-isomorphic]
we tend to not talk about isomorphisms so much in those contexts.

= Group homomorphism
{parent=group-isomorphism}
{wiki}

Like isomorphism, but does not have to be one-to-one: multiple different inputs can have the same output.

The image is as for any function smaller or equal in size as the domain of course.

This brings us to the key intuition about group homomorphisms: they are a way to split out a larger group into smaller groups that retains a subset of the original structure.

As shown by the \x[fundamental-theorem-on-homomorphisms], each group homomorphism is fully characterized by a \x[normal-subgroup] of the domain.

= Fundamental theorem on homomorphisms
{parent=group-homomorphism}
{wiki}

Ultimate explanation: https://math.stackexchange.com/questions/776039/intuition-behind-normal-subgroups/3732426#3732426

Links \x[group-homomorphism] and the \x[quotient-group] via \x[normal-subgroup]{p}.

= Kernel
{disambiguate=algebra}
{parent=group-homomorphism}
{wiki}

= Generating set of a group
{parent=group-mathematics}
{wiki}

= Cayley graph
{c}
{parent=generating-set-of-a-group}
{wiki}

You select a \x[generating-set-of-a-group], and then you name every node with them, and you specify:
* each node by a product of generators
* each edge by what happens when you apply a generator to each element

Not unique: different generating sets lead to different graphs, see e.g. two possible https://en.wikipedia.org/w/index.php?title=Cayley_graph&oldid=1028775401#Examples for the 

= Cycle graph
{disambiguate=algebra}
{parent=cayley-graph}
{wiki}

How to build it: https://math.stackexchange.com/questions/3137319/how-in-general-does-one-construct-a-cycle-graph-for-a-group/3162746#3162746 good answer with \x[ascii-art]. You basically just pick each element, and repeatedly apply it, and remove any path that has a longer version.

Immediately gives the \x[generating-set-of-a-group] by looking at elements adjacent to the origin, and more generally the \x[order-of-an-element-of-a-group][order of each element].

TODO \x[uniqueness]: can two different \x[group]{p} have the same cycle graph? It does not seem to tell us how every element interact with every other element, only with itself. This is in contrast with the \x[cayley-graph], which more accurately describes group structure (but does not give the order of elements as directly), so feels like it won't be unique.

= Cycle of an element of a group
{parent=generating-set-of-a-group}

Take the element and apply it to itself. Then again. And so on.

In the case of a \x[finite-group], you have to eventually reach the \x[identity-element] again sooner or later, giving you the \x[order-of-an-element-of-a-group].

The continuous analogue for the cycle of a group are the \x[one-parameter-subgroup]{p}. In the continuous case, you sometimes reach identity again and to around infinitely many times (which always happens in the finite case), but sometimes you don't.

= Order of an element of a group
{parent=cycle-of-an-element-of-a-group}

The length of its \x[cycle-of-an-element-of-a-group][cycle].

Bibliography:
* https://math.stackexchange.com/questions/972057/calculating-the-order-of-an-element-in-a-group

= Direct product of groups
{parent=group-mathematics}
{title2=$G \times H$}
{wiki}

= Product of group subsets
{parent=direct-product-of-groups}
{wiki}

= Semidirect product
{parent=direct-product-of-groups}
{title2=$N \rtimes H$}
{wiki}

As per https://en.wikipedia.org/w/index.php?title=Semidirect_product&oldid=1040813965#Properties[], unlike the \x[direct-product], the semidirect product of two goups is neither \x[unique], nor does it always \x[exist], and there is no known algorithmic way way to tell if one exists or not.

This is because reaching the "output" of the semidirect produt of two groups requires extra non-obvious information that might not exist. This is because the semi-direct product is based on the \x[product-of-group-subsets]. So you start with two small and completely independent groups, and it is not obvious how to join them up, i.e. how to define the group operation of the product group that is compatible with that of the two smaller input groups. Contrast this with the \x[direct-product], where the composition is simple: just use the group operation of each group on either side.

Product of group subsets

So in other words, it is not a \x[function-mathematics] like the \x[direct-product]. The semidiret product is therefore more like a property of three groups. 

The semidirect product is more general than the \x[direct-product-of-groups] when thinking about the \x[group-extension-problem], because with the \x[direct-product-of-groups], both subgroups of the larger group are necessarily also normal (trivial projection \x[group-homomorphism] on either side), while for the semidirect product, only one of them does.

Conversely, https://en.wikipedia.org/w/index.php?title=Semidirect_product&oldid=1040813965 explains that if $G = N \rtimes H$, and besides the implied requirement that N is normal, H is also normal, then $G = N \times H$.

Smallest example: $D_6 = C_3 \rtimes C_2$ where $D$ is a \x[dihedral-group] and $C$ are \x[cyclic-group]{p}. $C_3$ (the rotation) is a normal subgroup of $D_6$, but $C_2$ (the flip) is not.

Note that with the \x[direct-product] instead we get $C_6$ and not $D_6$, i.e. $C_3 \times C_2 = C_6$ as per \x[the-direct-product-of-two-cyclic-groups-of-coprime-order-is-another-cyclic-group].

TODO:
* why does one of the groups have to be normal in the definition?
* what is the smallest example of a non-\x[simple-group] that is neither a direct nor a semi-direct product of any two other groups?

Bibliography: https://math.stackexchange.com/questions/1726939/is-this-intuition-for-the-semidirect-product-of-groups-correct

= Subgroup
{parent=group-mathematics}
{wiki}

= Subgroup generated by a group
{parent=subgroup}

= Quotient group
{parent=subgroup}
{wiki}

Ultimate explanation: https://math.stackexchange.com/questions/776039/intuition-behind-normal-subgroups/3732426#3732426

Does not have to be isomorphic to a subgroup:
* https://www.mathcounterexamples.net/a-semi-continuous-function-with-a-dense-set-of-points-of-discontinuity/
* https://math.stackexchange.com/questions/2498922/is-a-quotient-group-a-subgroup
This is one of the reasons why the analogy between \x[simple-group]{p} of finite groups and \x[prime-number]{p} is limited.

= Subquotient
{parent=quotient-group}
{wiki}

Quotient of a subgroup H of G by a \x[normal-subgroup] of the subgroup H.

That \x[normal-subgroup] does not have have to be a normal subgroup of G.

As an overkill example, the happy family are subquotients of the \x[monster-group], but the monster group is simple.

= Relationship between the quotient group and direct products
{parent=quotient-group}

Although quotients look a bit real number division, there are some important differences with the "group analog of multiplication" of \x[direct-product-of-groups].

If a group is isomorphic to the \x[direct-product-of-groups], we can take a quotient of the product to retrieve one of the groups, which is somewhat analogous to division: https://math.stackexchange.com/questions/723707/how-is-the-quotient-group-related-to-the-direct-product-group

The "converse" is not always true however: a group does not need to be isomorphic to the product of one of its \x[normal-subgroup]{p} and the associated \x[quotient-group]. The wiki page provides an example:
\Q[
Given G and a normal subgroup N, then G is a group extension of G/N by N. One could ask whether this extension is trivial or split; in other words, one could ask whether G is a direct product or semidirect product of N and G/N. This is a special case of the extension problem. An example where the extension is not split is as follows: Let $G = Z4 = {0, 1, 2, 3}$, and $ = {0, 2}$ which is isomorphic to Z2. Then G/N is also isomorphic to Z2. But Z2 has only the trivial automorphism, so the only semi-direct product of N and G/N is the direct product. Since Z4 is different from Z2 × Z2, we conclude that G is not a semi-direct product of N and G/N.
]

TODO find a less minimal but possibly more important example.

This is also semi mentioned at: https://math.stackexchange.com/questions/1596500/when-is-a-group-isomorphic-to-the-product-of-normal-subgroup-and-quotient-group

I think this might be equivalent to why the \x[group-extension-problem] is hard. If this relation were true, then taking the direct product would be the only way to make larger groups from normal subgroups/quotients. But it's not.

= Normal subgroup
{parent=quotient-group}
{wiki}

Ultimate explanation: https://math.stackexchange.com/questions/776039/intuition-behind-normal-subgroups/3732426#3732426

Only normal subgroups can be used to form \x[quotient-group]{p}: their key definition is that they plus their cosets form a group.

Intuition:
* https://math.stackexchange.com/questions/776039/intuition-behind-normal-subgroups
* https://math.stackexchange.com/questions/1014535/is-there-any-intuitive-understanding-of-normal-subgroup/1014791

One key intuition is that "a normal subgroup is the \x[kernel-algebra]" of a \x[group-homomorphism], and the normal subgroup plus cosets are isomorphic to the image of the isomorphism, which is what the \x[fundamental-theorem-on-homomorphisms] says.

Therefore "there aren't that many \x[group-homomorphism]", and a normal subgroup it is a concrete and natural way to uniquely represent that homomorphism.

The best way to think about the, is to always think first: what is the homomorphism? And then work out everything else from there.

= Simple group
{parent=normal-subgroup}
{wiki}

Does not have any non-trivial \x[normal-subgroup].

And therefore, going back to our intuition that due to the \x[fundamental-theorem-on-homomorphisms] there is one normal group per homomorphism, a simple group is one that has no non-trivial homomorphisms.

= How to show that a group is simple
{parent=simple-group}
{wiki}

https://math.stackexchange.com/questions/203168/proving-a-group-is-simple

https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=5051&context=etd_theses proves that the \x[mathieu-group-m-24] is simple in just 200 pages. Nice.

Examples:
* \x[the-alternating-groups-of-degree-5-or-greater-are-simple]

= Ring
{disambiguate=mathematics}
{parent=group-mathematics}
{wiki}

A \x[group-mathematics] with an extra operation called multiplication which satisfies:
* \x[associative-property]
* distributes over addition (the default group operation)
* has an identity

Unlike addition, that multiplication does not need to satisfy:
* \x[commutative-property]. If this is satisfied, we can call it a \x[commutative-ring].
* existence of an inverse. If this is satisfied, we can call it a \x[division-ring].
If those are also satisfied, then we have a \x[field-mathematics].

The simplest example of a ring which is not a full fledged \x[field-mathematics] and with \x[commutative] multiplication are the \x[integer]{p}. Notably, no inverses exist except for the identity itself and -1. E.g. the inverse of 2 would be 1/2 which is not in the \x[set-mathematics].

A \x[polynomial-ring] is another example with the same properties as the \x[integer]{p}.

The simplest non-commutative ring that is not a \x[field-mathematics] is the set of all 2x2 \x[matrix]{p} of \x[real-number]{p}:
* we know that 2x2 matrix multiplication is non-commutative in general
* some 2x2 matrices have a multiplicative inverse, but others don't
Note that \x[gl-n] is not a ring because you can by addition reach the zero matrix. 

= Commutative ring
{parent=ring-mathematics}
{wiki}

= Division ring
{parent=ring-mathematics}
{wiki}

Two ways to see it:
* a \x[ring-mathematics] where \x[inverse]{p} exist
* a \x[field-mathematics] where multiplication is not necessarily \x[commutative]

= Field
{disambiguate=mathematics}
{parent=ring-mathematics}
{wiki}

A \x[ring-mathematics] where multiplication is \x[commutative] and there is always an inverse.

A field can be seen as an \x[abelian-group] that has two group operations: addition and multiplication, and they are compatible (distributive property).

Basically the nicest, least restrictive, 2-operation type of \x[algebra].

Examples:
* \x[real-number]{p}
* \x[rational-number]{p}

= Distributive property
{parent=field-mathematics}
{wiki}

One of the defining properties of \x[algebraic-structure] with two operations such as \x[ring-mathematics] and \x[field-mathematics]:
$$a(b + c) = ab + ac$$
This property shows how the two operations interact.

= Finite field
{parent=field-mathematics}
{title2=$GF(n)$}
{wiki}

A convenient notation for the elements of $GF(n)$ of prime order is to use \x[integer]{p}, e.g. for $GF(7)$ we could write:
$$
GR(7) = \{-3, -2, -1, 0, 1, 2, 3\}
$$
which makes it clear what is the additive inverse of each element, although sometimes a notation starting from 0 is also used:
$$
GR(7) = \{0, 1, 2, 3, 4, 5, 6\}
$$

For fields of \x[prime] order, regular \x[modular-arithmetic] works as the field operation.

For non-prime order, we see that \x[modular-arithmetic] does not work because the divisors have no inverse. E.g. at order 6, 2 and 3 have no inverse, e.g. for 2:
$$
0 \times 2 = 0
1 \times 2 = 2
2 \times 2 = 4
3 \times 2 = 0
4 \times 2 = 2
5 \times 2 = 4
$$
we see that things wrap around perfecly, and 1 is never reached.

For non-prime \x[prime-power] orders however, we can find a way, see \x[finite-field-of-non-prime-order].

\Video[https://www.youtube.com/watch?v=z9bTzjy4SCg]
{title=Finite fields made easy by Randell Heyman (2015)}
{description=Good introduction with examples}

= Classification of finite fields
{parent=finite-field}

There's exactly one field per \x[prime-power], So all we need to specify a field is give its order, notated e.g. as $GF(n)$.

Every element of a finite field satisfies $x^{order} = x$.

It is interesting to compare this result philosophically with the \x[classification-of-finite-groups]: fields are more constrained as they have to have two operations, and this leads to a much simpler classification!

= Finite field of non-prime order
{parent=finite-field}

As per \x[classification-of-finite-fields] those must be of \x[prime-power] order.

\x[video-finite-fields-made-easy-by-randell-heyman-2015] at https://youtu.be/z9bTzjy4SCg?t=159 shows how for order $9 = 3 \times 3$. Basically, for order $p^n$, we take:
* each element is a polynomial in $GF(p)[x]$, $GF(p)[x]$, the \x[polynomial-over-a-field][polynomial ring over the finite field $GF(p)$] with degree smaller than $n$. We've just seen how to construct $GF(p)$ for prime $p$ above, so we're good there.
* addition works element-wise modulo on $GF(p)$
* multiplication is done modulo an \x[irreducible-polynomial] of order $n$
For a worked out example, see: \x[gf-4].

= GF(2)
{c}
{parent=finite-field}
{wiki}

= GF(4)
{c}
{parent=finite-field}
{wiki}

\x[ciro-santilli] tried to https://en.wikipedia.org/w/index.php?title=Finite_field&type=revision&diff=1044934168&oldid=1044905041[add this example to Wikipedia], but revert, so here we are.

This is a good first example of a field of a \x[finite-field-of-non-prime-order], this one is a \x[prime-power] order instead.

$4 = 2^2$, so one way to represent the elements of the field will be the to use the 4 polynomials of degree 1 over \x[gf-2]:
* 0X + 0
* 0X + 1
* 1X + 0
* 1X + 1

Note that we refer in this definition to anther field, but that is fine, because we only refer to fields of \x[prime] order such as \x[gf-2], because we are dealing with \x[prime-power]{p} only. And we have already defined fields of prime order easily previously with \x[modular-arithmetic].

Over GF(2), there is only one \x[irreducible-polynomial] of degree 2:
$$X^2+X+1$$

Addition is defined element-wise with \x[modular-arithmetic] modulo 2 as defined over GF(2), e.g.:
$$(1X + 0) + (1X + 1) = (1 + 1)X + (0 + 1) = 0X + 1$$

Multiplication is done modulo $X^2+X+1$, which ensures that the result is also of degree 1.

For example first we do a regular multiplication:
$$(1X + 0) \times (1X + 1) = (1 \times 1)X^2 + (1 \times 1)X + (0 \times 1)X + (0 \times 1) = 1X^2 + 1X + 0$$

Without modulo, that would not be one of the elements of the field anymore due to the $1X^2$!

So we take the modulo, we note that:
$$1X^2 + 1X + 0 = 1(X^2+X+1) + (0X + 1)$$
and by the definition of modulo:
$$(1X^2 + 1X + 0) \mod (X^2+X+1) = (0X + 1)$$
which is the final result of the multiplication.

TODO show how taking a reducible polynomial for modulo fails. Presumably it is for a similar reason to why things fail for the prime case.

= Vector field
{parent=field-mathematics}
{wiki}

= Algebra over a field
{parent=vector-field}
{wiki}

A \x[vector-field] with a \x[bilinear-map] into itself, which we can also call a "vector product".

Note that the vector product does not have to be neither \x[associative] nor \x[commutative].

Examples: https://en.wikipedia.org/w/index.php?title=Algebra_over_a_field&oldid=1035146107#Motivating_examples
* \x[complex-number]{p}, i.e. \x[r-2] with complex number multiplication
* \x[r-3] with the \x[cross-product]
* \x[quaternion]{p}, i.e. \x[r-4] with the quaternion multiplication

= Division algebra
{parent=algebra-over-a-field}
{wiki}

An \x[algebra-over-a-field] where \x[division] exists.

= Frobenius theorem
{c}
{disambiguate=real division algebras}
{parent=division-algebra}
{wiki}

= Classification of associative real division algebras
{synonym}

There are 3: \x[real-number]{p}, \x[complex-number]{p} and \x[quaternion]{p}.

Notably, the \x[octonion]{p} are not \x[associative].

= Associative property
{parent=algebra}
{wiki}

= Associative
{synonym}

= Applied mathematics
{parent=mathematics}
{wiki}

= Applied mathematician
{synonym}

= Calculus
{parent=mathematics}
{wiki}

Well summarized as "the branch of mathematics that deals with \x[limit-mathematics]{p}.

= Mathematical analysis
{parent=calculus}
{wiki}

= Analytical
{synonym}

An fancy name for \x[calculus], with the "more advanced" connotation.

= Limit
{disambiguate=mathematics}
{parent=calculus}
{wiki}

= Limit
{synonym}

The fundamental concept of \x[calculus]!

The reason why the epsilon delta definition is so venerated is that it fits directly into well known methods of the \x[formalization-of-mathematics], making the notion completely precise.

= Convergent series
{parent=limit-mathematics}
{wiki}

= Convergence
{disambiguate=mathematics}
{synonym}

= Converges
{disambiguate=mathematics}
{synonym}

= Convergent
{disambiguate=mathematics}
{synonym}

= Continuous function
{parent=limit-mathematics}
{wiki}

= Continuity
{synonym}

= Continuous
{synonym}

= Continuous problems are simpler than discrete ones
{parent=continuous-function}

This is a general philosophy that \x[ciro-santilli], and likely others, observes over and over.

Basically, \x[continuity], or higher order conditions like \x[differentiability] seem to impose greater constraints on problems, which make them more solvable.

Some good examples of that:
* complex \x[discrete] problems:
  * \x[classification-of-finite-groups]
* simple \x[continuous] problems:
  * characterization of \x[lie-group]{p}

= Discrete
{parent=continuous-function}

Something that is very not \x[continuous].

Notably studied in \x[discrete-mathematics].

= Discretization
{parent=discrete}
{wiki}

= Discretize
{synonym}

= Infinity
{parent=limit-mathematics}
{title2=$\infty$}
{wiki}

= Infinite
{synonym}

= Finite
{synonym}

There are a few related concepts that are called infinity in \x[mathematics]:
* \x[limit]{p} that are greater than any number
* the \x[cardinality] of a \x[set] that does not have a finite number of elements
* in some number systems, there is an explicit "element at infinity" that is not a \x[limit], e.g. \x[projective-geometry]

= L'Hôpital's rule
{parent=limit-mathematics}
{title2=limit of a ratio}
{wiki}

= Derivative
{parent=calculus}
{wiki}

= Chain rule
{parent=derivative}
{wiki}

= Multivariable chain rule
{parent=chain-rule}

= Differentiable function
{parent=derivative}
{wiki}

= Differentiable
{synonym}

= Differentiability
{synonym}

= Smoothness
{parent=differentiable-function}
{wiki}

= Infinitely differentiable function
{parent=differentiable-function}

= $C^{\infty}$
{title2}
{synonym}

= Bump function
{parent=infinitely-differentiable-function}
{wiki}

= Flat top bump function
{parent=bump-function}

https://math.stackexchange.com/questions/1786964/is-it-possible-to-construct-a-smooth-flat-top-bump-function

= Maxima and minima
{parent=derivative}
{wiki}

Given a \x[function] $f$:
* from some space. For beginners the \x[real-number]{p} but more generally \x[topological-space]{p} should work in general
* to the \x[real-number]{p}
we want to find the points $x$ of the \x[domain-function] of $f$ where the value of $f$ is smaller (for minima, or larger for maxima) than all other points in some \x[neighbourhood-mathematics] of $x$.

In the case of \x[functional]{p}, this problem is treated under the theory of the \x[calculus-of-variations].

= Lifegard problem
{parent=maxima-and-minima}

https://pumphandle.consulting/2020/09/04/the-lifeguard-problem-solved/

= Derivative test
{parent=maxima-and-minima}
{wiki}

= Saddle point
{parent=maxima-and-minima}
{wiki}

= Newton dot notation
{c}
{parent=derivative}

= Partial derivative
{parent=derivative}
{wiki}

= Partial derivative notation
{parent=partial-derivative}

= Partial derivative symbol
{parent=partial-derivative-notation}
{title2=$\partial$}

Nope, it is not a \x[greek-letter], notably it is not a lowercase \x[delta]. It is just some random made up symbol that looks like a \x[letter-d]. Which is of course derived from \x[delta], which is why it is all so damn confusing.

I think the symbol is usually just read as "\x[d]" as in "d f d x" for $\pdv{F(x, y, z)}{x}$.

= Partial label partial derivative notation
{parent=partial-derivative-notation}
{title2=$\partial_x F$, $\partial_y F$}

= Partial index partial derivative notation
{parent=partial-derivative-notation}
{title2=$\partial_0 F$, $\partial_1 F$}

This notation is not so common in basic mathematics, but it is so incredibly convenient, especially with \x[einstein-notation] as shown at \x[einstein-notation-for-partial-derivatives]{full}:
$$
\partial_0 F(x, y, z) = \pdv{F(x, y, z)}{x} \\
\partial_1 F(x, y, z) = \pdv{F(x, y, z)}{y} \\
\partial_2 F(x, y, z) = \pdv{F(x, y, z)}{x} \\
$$

This notation is similar to \x[partial-label-partial-derivative-notation], but it uses indices instead of labels such as $x$, $y$, etc.

= Total derivative
{parent=derivative}
{wiki}

The total derivative of a function assigns for every point of the domain a linear map with same domain, which is the best linear approximation to the function value around this point, i.e. the tangent plane.

E.g. in 1D:
$$
Total derivative = D[f(x_0)](x) = f(x_0) + \pdv{f}{x}(x_0) \times x
$$
and in 2D:
$$
D[f(x_0, y_0)](x, y) = f(x_0, y_0) + \pdv{f}{x}(x_0, y_0) \times x + \pdv{f}{y}(x_0, y_0) \times y
$$

= Directional derivative
{c}
{parent=derivative}
{wiki}

= Integral
{parent=calculus}
{wiki}

= Area
{parent=integral}
{wiki}

= Volume
{parent=area}
{wiki}

\x[3d] \x[area].

= Riemann integral
{c}
{parent=integral}
{wiki}

The easy and less generic \x[integral]. The harder one is the \x[lebesgue-integral].

= Lebesgue integral
{c}
{parent=integral}
{wiki=Lebesgue_integration}

"More complex and general" integral. Matches the \x[riemann-integral] for "simple functions", but also \x[lebesgue-integral-vs-riemann-integral][works for some "funkier" functions that Riemann does not work for].

\x[ciro-santilli] sometimes wonders how much someone can gain from learning this besides \x[the-beauty-of-mathematics], since we can hand-wave a \x[lebesgue-integral] on almost anything that is of practical use. The beauty is good reason enough though.

= Lebesgue integral vs Riemann integral
{c}
{parent=lebesgue-integral}

Advantages over Riemann:
* \x[lebesgue-integral-of-lp-is-complete-but-riemann-isn-t].
* https://youtu.be/PGPZ0P1PJfw?t=710 you are able to switch the order of integrals and limits of function sequences on non-uniform convergence. TODO why do we care? This is linked to the \x[fourier-series] of course, but concrete example?

\Video[https://youtube.com/watch?v=PGPZ0P1PJfw]
{title=Riemann integral vs. Lebesgue integral by The Bright Side Of Mathematics (2018)}
{description=https://youtube.com/watch?v=PGPZ0P1PJfw&t=808 shows how Lebesgue can be visualized as a partition of the function range instead of domain, and then you just have to be able to measure the size of pre-images.

One advantage of that is that the range is always one dimensional.

But the main advantage is that having infinitely many discontinuities does not matter.

Infinitely many discontinuities can make the Riemann partitioning diverge.

But in Lebesgue, you are instead measuring the size of preimage, and to fit infinitely many discontinuities in a finite domain, the size of this preimage is going to be zero.

So then the question becomes more of "how to define the measure of a subset of the domain".

Which is why we then fall into \x[measure-theory]!
}

= Real world applications of the Lebesgue integral
{parent=lebesgue-integral-vs-riemann-integral}

In "practice" it is likely "useless", because the functions that it can integrate that Riemann can't are just too funky to appear in practice :-)

Its value is much more indirect and subtle, as in "it serves as a solid basis of \x[quantum-mechanics]" due to the definition of \x[hilbert-space]{p}.

Bibliography:
* https://math.stackexchange.com/questions/53121/how-do-people-apply-the-lebesgue-integration-theory
* https://www.quora.com/What-are-some-real-life-applications-of-Lebesgue-Integration

= Lebesgue measurable
{c}
{parent=lebesgue-integral}

= Lebesgue integral of $\LP$ is complete but Riemann isn't
{c}
{parent=lebesgue-integral}

$\LP$ is:
* \x[complete-metric-space][complete] under the Lebesgue integral, this result is may be called the \x[riesz-fischer-theorem]
* not complete under the \x[riemann-integral]: https://math.stackexchange.com/questions/397369/space-of-riemann-integrable-functions-not-complete

And then this is why \x[quantum-mechanics] basically lives in \x[l2]: not being complete makes no sense physically, it would mean that you can get closer and closer to states that don't exist!

TODO intuition

= Riesz-Fischer theorem
{c}
{parent=lebesgue-integral-of-lp-is-complete-but-riemann-isn-t}
{wiki=Riesz–Fischer_theorem}

A measurable function defined on a closed interval is square integrable (and therefore in \x[l2]) if and only if \x[fourier-series] converges in \x[l2] norm the function:
$$
\lim_{N \to \infty} \left \Vert S_N f - f \right \|_2 = 0
$$

= $\LP$ is complete
{parent=riesz-fischer-theorem}

TODO

= Fourier basis is complete for $\LTwo$
{c}
{parent=riesz-fischer-theorem}
{id=fourier-basis-is-complete-for-l2}

https://math.stackexchange.com/questions/316235/proving-that-the-fourier-basis-is-complete-for-cr-2-pi-c-with-l2-norm

\x[riesz-fischer-theorem] is a norm version of it, and \x[carleson-s-theorem] is stronger pointwise almost everywhere version.

Note that the \x[riesz-fischer-theorem] is weaker because the pointwise limit could not exist just according to it: \x[lp-norm-sequence-convergence-does-not-imply-pointwise-convergence].

= $L^p$ norm sequence convergence does not imply pointwise convergence
{id=lp-norm-sequence-convergence-does-not-imply-pointwise-convergence}
{parent=fourier-basis-is-complete-for-l2}

https://math.stackexchange.com/questions/138043/does-convergence-in-lp-imply-convergence-almost-everywhere

There are explicit examples of this. We can have ever thinner disturbances to convergence that keep getting less and less area, but never cease to move around.

If it does converge pointwise to something, then it must match of course.

= Carleson's theorem
{c}
{parent=fourier-basis-is-complete-for-l2}
{wiki}

The \x[fourier-series] of an \x[l2] function (i.e. the function generated from the infinite sum of weighted sines) converges to the function pointwise almost everywhere.

The theorem also seems to hold (maybe trivially given the transform result) for the \x[fourier-series] (TODO if trivially, why trivially).

Only proved in 1966, and known to be a hard result without any known simple proof.

This theorem of course implies that \x[fourier-basis-is-complete-for-l2], as it explicitly constructs a decomposition into the Fourier basis for every single function.

TODO vs \x[riesz-fischer-theorem]. Is this just a stronger pointwise result, while Riesz-Fischer is about norms only?

One of the many \x[fourier-inversion-theorem]{p}.

= Lp space
{parent=lebesgue-integral-of-lp-is-complete-but-riemann-isn-t}
{wiki}

= $\LP$
{synonym}
{title2}

Integrable functions to the power $p$, usually and in this text assumed under the \x[lebesgue-integral] because: \x[lebesgue-integral-of-lp-is-complete-but-riemann-isn-t]

= $L^1$
{parent=lp-space}
{id=l1-space}

= $\LTwo$
{parent=lp-space}
{id=l2}

\x[lp] for $p == 2$.

$\LTwo$ is by far the most important of $\LP$ because it is \x[mathematical-formulation-of-quantum-mechanics][quantum mechanics states] live, because the total probability of being in any state has to be 1!

\x[l2] has some crucially important properties that other $\LP$ don't (TODO confirm and make those more precise):
* it is the only $\LP$ that is \x[hilbert-space] because it is the only one where an inner product compatible with the metric can be defined:
  * https://math.stackexchange.com/questions/2005632/l2-is-the-only-hilbert-space-parallelogram-law-and-particular-ft-gt
  * https://www.quora.com/Why-is-L2-a-Hilbert-space-but-not-Lp-or-higher-where-p-2
* \x[fourier-basis-is-complete-for-l2], which is great for solving \x[differential-equation]

= Plancherel theorem
{c}
{parent=l2}

Some sources say that this is just the part that says that the \x[norm-mathematics] of a \x[l2] function is the same as the norm of its \x[fourier-transform].

Others say that this theorem actually says that the \x[fourier-transform] is \x[bijective].

The comment at https://math.stackexchange.com/questions/446870/bijectiveness-injectiveness-and-surjectiveness-of-fourier-transformation-define/1235725#1235725 may be of interest, it says that the \x[bijection] statement is an easy consequence from the \x[norm-mathematics] one, thus the confusion.

TODO does it require it to be in \x[l1-space] as well? \x[wikipedia] https://en.wikipedia.org/w/index.php?title=Plancherel_theorem&oldid=987110841 says yes, but https://courses.maths.ox.ac.uk/node/view_material/53981 does not mention it.

= The Fourier transform is a bijection in $L^2$
{parent=plancherel-theorem}

As mentioned at \x[plancherel-theorem]{full}, some people call this part of \x[plancherel-theorem], while others say it is just a corollary.

This is an important fact in \x[quantum-mechanics], since it is because of this that it makes sense to talk about \x[position-and-momentum-space] as two dual representations of the \x[wave-function] that contain the exact same amount of information.

= Every Riemann integrable function is Lebesgue integrable
{parent=plancherel-theorem}

But only for the proper Riemann integral: https://math.stackexchange.com/questions/2293902/functions-that-are-riemann-integrable-but-not-lebesgue-integrable

= Measure theory
{parent=calculus}
{wiki=Measure_(mathematics)}

Main motivation: \x[lebesgue-integral].

The Bright Side Of Mathematics 2019 playlist: https://www.youtube.com/watch?v=xZ69KEg7ccU&list=PLBh2i93oe2qvMVqAzsX1Kuv6-4fjazZ8j

The key idea, is that we can't define a measure for the power set of R. Rather, we must select a large measurable subset, and the Borel sigma algebra is a good choice that matches intuitions.

= Fourier series
{c}
{parent=calculus}
{wiki}

Approximates an original function by sines. If the function is "well behaved enough", the approximation is to arbitrary precision.

\x[fourier]'s original motivation, and a key application, is \x[solving-partial-differential-equations-with-the-fourier-series].

Can only be used to approximate for periodic functions (obviously from its definition!). The \x[fourier-transform] however overcomes that restriction:
* https://math.stackexchange.com/questions/1115240/can-a-non-periodic-function-have-a-fourier-series
* https://math.stackexchange.com/questions/1378633/every-function-can-be-represented-as-a-fourier-series

The Fourier series behaves really nicely in \x[l2], where it always exists and converges pointwise to the function: \x[carleson-s-theorem].

\Video[https://www.youtube.com/watch?v=r6sGWTCMz2k]
{title=But what is a Fourier series? From heat flow to circle drawings | DE4 by \x[3blue1brown] (2019)}
{description=Amazing 2D visualization of the decomposition of complex functions.}

= Applications of the Fourier series
{parent=fourier-series}

= Solving partial differential equations with the Fourier series
{parent=applications-of-the-fourier-series}

See: https://math.stackexchange.com/questions/579453/real-world-application-of-fourier-series/3729366#3729366

\x[separation-of-variables] of certain equations like the \x[heat-equation] and \x[wave-equation] are solved immediately by calculating the \x[fourier-series] of initial conditions!

Other basis besides the Fourier series show up for other equations, e.g.:
* \x[bessel-function]
* \x[hermite-polynomials]

= Discrete Fourier transform
{title2=DFT}
{parent=fourier-series}
{wiki}

= Fourier transform
{parent=fourier-series}
{c}
{wiki}

Continuous version of the \x[fourier-series].

Can be used to represent functions that are not periodic: https://math.stackexchange.com/questions/221137/what-is-the-difference-between-fourier-series-and-fourier-transformation while the \x[fourier-series] is only for periodic functions.

Of course, every function defined on a finite line segment (i.e. a \x[compact-space]).

Therefore, the \x[fourier-transform] can be seen as a generalization of the \x[fourier-series] that can also decompose functions defined on the entire \x[real-line].

As a more concrete example, just like the \x[fourier-series] is how you solve a the \x[heat-equation] on a line segment with \x[dirichlet-boundary-condition]{p} as shown at: \x[solving-partial-differential-equations-with-the-fourier-series]{full}, the \x[fourier-transform] is what you need to solve the problem when the \x[domain-function] is the entire \x[real-line].

= Multidimensional Fourier transform
{parent=fourier-transform}

Lecture notes:
* http://www.robots.ox.ac.uk/~az/lectures/ia/lect2.pdf Lecture 2: 2D Fourier transforms and applications by A. Zisserman (2014)

\Video[https://www.youtube.com/watch?v=v743U7gvLq0]
{title=How the 2D FFT works by Mike X Cohen (2017)}
{description=Animations showing how the 2D Fourier transform looks like for simple inpuf functions.}

= Fourier inversion theorem
{parent=fourier-transform}
{wiki}

A set of theorems that prove under different conditions that the \x[fourier-transform] has an inverse for a given space, examples:
* \x[carleson-s-theorem] for \x[l2]

= Laplace transform
{c}
{parent=fourier-transform}

\Video[https://www.youtube.com/watch?v=7UvtU75NXTg]
{title=The Laplace Transform: A Generalized Fourier Transform by Steve Brunton (2020)}
{description=Explains how the Laplace transform works for functions that do not go to zero on infinity, which is a requirement for the \x[fourier-transform]. No applications in that video yet unfortunately.}

= History of the Fourier series
{parent=fourier-series}

First published by Fourier in 1807 to solve the \x[heat-equation].

= Topology
{parent=calculus}
{wiki}

= Topological
{synonym}

Topology is the plumbing of \x[calculus].

The key concept of topology is a \x[neighbourhood-mathematics].

Just by havin the notion of neighbourhood, concepts such as \x[limit-mathematics] and \x[continuity] can be defined without the need to specify a precise numerical value to the distance between two points with a \x[metric-mathematics].

As an example. consider the \x[orthogonal-group], which is also naturally a \x[topological-space]. That group does not usually have a notion of distance defined for it by default. However, we can still talk about certain properties of it, e.g. that \x[the-orthogonal-group-is-compact], and that \x[the-orthogonal-group-has-two-connected-components].

= Covering space
{parent=topology}
{wiki}

Basically it is a larger space such that there exists a \x[surjection] from the large space onto the smaller space, while still being compatible with the \x[topology] of the small space.

We can characterize the cover by how injective the function is. E.g. if two elements of the large space map to each element of the small space, then we have a \x[double-cover] and so on.

= Double cover
{parent=covering-space}

= Neighbourhood
{disambiguate=mathematics}
{parent=topology}
{wiki}

The key concept of \x[topology].

= Topological space
{parent=topology}
{wiki}

= Manifold
{parent=topology}
{wiki}

We map each point and a small enough \x[neighbourhood-mathematics] of it to \x[r-n], so we can talk about the manifold points in terms of coordinates.

Does not require any further structure besides a consistent \x[topological] map. Notably, does not require \x[metric-mathematics] nor an addition operation to make a \x[vector-space].

Manifolds are \x[good][cool]. Especially \x[differentiable-manifold]{p} which we can do \x[calculus] on.

A notable example of a \x[non-euclidean-geometry] manifold is the space of \x[generalized-coordinate]{p} of a \x[lagrangian]. For example, in a problem such as the \x[double-pendulum], some of those generalized coordinates could be angles, which wrap around and thus are not \x[euclidean].

= Atlas
{disambiguate=topology}
{parent=manifold}
{wiki}

Collection of \x[coordinate-chart]{p}.

The key element in the definition of a \x[manifold].

= Coordinate chart
{parent=atlas-topology}

= Covariant derivative
{parent=manifold}
{wiki}

A generalized definition of \x[derivative] that works on \x[manifold]{p}.

TODO: how does it maintain a single value even across different \x[coordinate-chart]{p}?

= Differentiable manifold
{parent=manifold}
{wiki}

TODO find a concrete numerical example of doing \x[calculus] on a differentiable manifold and visualizing it. Likely start with a boring circle. That would be sweet...

= Tangent space
{parent=manifold}
{wiki}

TODO what's the point of it.

Bibliography:
* https://www.youtube.com/watch?v=j1PAxNKB_Zc Manifolds #6 - Tangent Space (Detail) by WHYB maths (2020). This is worth looking into.
  * https://www.youtube.com/watch?v=oxB4aH8h5j4 actually gives a more concrete example. Basically, the vectors are defined by saying "we are doing the \x[directional-derivative] of any function along this direction".

    One thing to remember is that of course, the most convenient way to define a function $f$ and to specify a direction, is by using one of the \x[coordinate-chart]{p}.

    We can then just switch between charts by change of basis.
* http://jakobschwichtenberg.com/lie-algebra-able-describe-group/ by \x[jakob-schwichtenberg]
* https://math.stackexchange.com/questions/1388144/what-exactly-is-a-tangent-vector/2714944 What exactly is a tangent vector? on \x[stack-exchange]

= Tangent vector to a manifold
{parent=tangent-space}

A member of a \x[tangent-space].

= One-form
{parent=manifold}
{wiki}

https://www.youtube.com/watch?v=tq7sb3toTww&list=PLxBAVPVHJPcrNrcEBKbqC_ykiVqfxZgNl&index=19 mentions that it is a bit like a \x[dot-product] but for a \x[tangent-vector-to-a-manifold]: it measures how much that vector \x[derivative][derives] along a given direction.

= Metric
{disambiguate=mathematics}
{parent=topology}
{title2=$d(x, y)$}
{wiki}

= Distance
{synonym}

= Metric
{synonym}

A metric is a function that give the distance, i.e. a \x[real-number], between any two elements of a space.

A metric may be induced from a \x[norm] as shown at: \x[metric-induced-by-a-norm]{full}.

Because a \x[norm-induced-by-an-inner-product][norm can be induced by an inner product], and the \x[inner-product] given by the \x[matrix-representation-of-a-positive-definite-symmetric-bilinear-form], in simple cases metrics can also be represented by a \x[matrix].

= Metric space
{parent=metric-mathematics}
{wiki}

Canonical example: \x[euclidean-space].

= Metric space vs normed vector space vs inner product space
{parent=metric-space}

TODO examples:
* \x[metric-space] that is not a \x[normed-vector-space]
* \x[norm-mathematics] vs \x[metric]: a norm gives size of one element. A \x[metric] is the distance between two elements. Given a norm in a space with subtraction, we can obtain a distance function: the \x[metric-induced-by-a-norm].

\Image[https://upload.wikimedia.org/wikipedia/commons/7/74/Mathematical_Spaces.png]
{title=Hierarchy of topological, metric, normed and inner product spaces.}

= Complete metric space
{parent=metric-space}
{wiki}

In plain English: the space has no visible holes. If you start walking less and less on each step, you always converge to something that also falls in the space.

One notable example where completeness matters: \x[lebesgue-integral-of-lp-is-complete-but-riemann-isn-t].

= Normed vector space
{parent=metric-space}
{wiki}

= Inner product space
{parent=normed-vector-space}
{wiki}

Subcase of a \x[normed-vector-space], therefore also necessarily a \x[vector-space].

= Inner product
{parent=inner-product-space}
{wiki}

Appears to be analogous to the \x[dot-product], but also defined for \x[infinite-dimensions].

= Norm
{disambiguate=mathematics}
{parent=metric-space}
{title2=$|x|$}

= Norm
{synonym}

Vs \x[metric]:
* a norm is the size of one element. A \x[metric] is the distance between two elements.
* a norm is only defined on a \x[vector-space]. A \x[metric] could be defined on something that is not a vector space. Most basic examples however are also \x[vector-space]{p}.

= Norm induced by an inner product
{parent=norm-mathematics}
{wiki}

= Norm induced by the inner product
{synonym}

An \x[inner-product] $x \cdot y$ induces a \x[norm] with:
$$
|x| = \sqrt{<x, x>}
$$

= Metric induced by a norm
{parent=norm-mathematics}

In a \x[vector-space], a \x[metric] may be induced from a norm by using \x[subtraction]:
$$
d(x, y) = |x - y|
$$

= Pseudometric space
{parent=metric-space}
{wiki}

\x[metric-space]{c} but where the distance between two distinct points can be zero.

Notable example: \x[minkowski-space]{child}.

= Compact space
{parent=topology}
{wiki}

= Compact
{synonym}

= Dense set
{parent=topology}
{wiki}

= Connected space
{parent=topology}
{wiki}

= Disconnected space
{synonym}

= Connected component
{parent=connected-space}
{wiki}

When a \x[disconnected-space] is made up of several smaller \x[connected-space]{p}, then each smaller component is called a "connected component" of the larger space.

See for example the

= Simply connected space
{parent=connected-space}
{wiki}

= Simply connected
{synonym}

= Loop
{disambiguate=topology}
{parent=simply-connected-space}

= Homotopy
{parent=topology}
{wiki}

= Homotopic
{synonym}

= Generalized Poincaré conjecture
{parent=homotopy}

There are two cases:
* (topological) manifolds
* differential manifolds

Questions: are all compact manifolds / differential manifolds homotopic / diffeomorphic to the sphere in that dimension?
* for topological manifolds: this is a generalization of the \x[poincare-conjecture].

  Original problem posed, $n = 3$ for topological manifolds.

  \x[millennium-prize-problems]{p=0}.

  Last to be proven, only the 4-differential manifold case missing as of 2013.

  Even the truth for all $n > 4$ was proven in the 60's!

  Why is low dimension harder than high dimension?? Surprise!

  AKA: classification of compact 3-manifolds. The result turned out to be even simpler than compact 2-manifolds: there is only one, and it is equal to the 3-sphere.

  For dimension two, we know there are infinitely many: \x[classification-of-closed-surfaces]
* for differential manifolds:

  Not true in general. First counter example is $n = 7$. Surprise: what is special about the number 7!?

  Counter examples are called \x[exotic-sphere]{p}.

  Totally unpredictable count table:
  | Dimension    | 1 | 2 | 3 | 4 | 5 | 6 | 7  | 8 | 9 | 10 | 11  | 12 | 13 | 14 | 15    | 16 | 17 | 18 | 19     | 20 |
  | Smooth types | 1 | 1 | 1 | ? | 1 | 1 | 28 | 2 | 8 | 6  | 992 | 1  | 3  | 2  | 16256 | 2  | 16 | 16 | 523264 | 24 |
  $n = 4$ is an open problem, there could even be infinitely many. Again, why are things more complicated in lower dimensions??

= Exotic sphere
{parent=generalized-poincare-conjecture}
{wiki}

= Poincaré conjecture
{c}
{parent=generalized-poincare-conjecture}
{wiki}

= Classification of closed surfaces
{parent=generalized-poincare-conjecture}

* https://en.wikipedia.org/wiki/Surface_(topology)#Classification_of_closed_surfaces
* http://www.proofwiki.org/wiki/Classification_of_Compact_Two-Manifolds

So simple!! You can either:
* cut two holes and glue a handle. This is easy to visualize as it can be embedded in \x[r-3]: you just get a \x[torus], then a double torus, and so on
* cut a single hole and glue  a\x[mobius-strip] in it. Keep in mind that this is possible because the \x[mobius-strip] has a single boundary just like the hole you just cut. This leads to another infinite family that starts with:
  * 1: \x[real-projective-plane]
  * 2: \x[klein-bottle]

A handle cancels out a \x[mobius-strip], so adding one of each does not lead to a new object.

You can glue a Mobius strip into a single hole in dimension larger than 3! And it gives you a Klein bottle!

Intuitively speaking, they can be sees as the smooth surfaces in N-dimensional space (called an embedding), such that deforming them is allowed. 4-dimensions is enough to embed cover all the cases: 3 is not enough because of the Klein bottle and family.

= Torus
{c}
{parent=classification-of-closed-surfaces}
{wiki}

= Möbius strip
{c}
{parent=classification-of-closed-surfaces}
{wiki}

= Klein bottle
{c}
{parent=classification-of-closed-surfaces}
{wiki}

\x[sphere] with two \x[mobius-strip]{p} stuck into it as per the \x[classification-of-closed-surfaces].

= Real coordinate space
{c}
{parent=topology}
{wiki}

= $\R^n$
{synonym}
{title2}

= Real line
{parent=real-coordinate-space}
{wiki}

= $\R^1$
{synonym}
{title2}

= 1D
{synonym}

= Real plane
{parent=real-coordinate-space}

= $\R^2$
{synonym}
{title2}

= 2D
{synonym}

= Real coordinate space of dimension three
{c}
{parent=real-coordinate-space}

= $\R^3$
{synonym}
{title2}

= 3D
{synonym}

= Real coordinate space of dimension four
{c}
{parent=real-coordinate-space}

= $\R^4$
{synonym}
{title2}

= Four-dimensional space
{synonym}

= Four-dimensional
{synonym}

= 4D
{title2}
{synonym}

Important 4D spaces:
* \x[3-sphere]

= Visualizing 4D
{parent=real-coordinate-space-of-dimension-four}

Simulate it. Just simulate it.

\Video[http://youtube.com/watch?v=0t4aKJuKP0Q]
{title=4D Toys: a box of four-dimensional toys by Miegakure (2017)}

= Dimension
{parent=real-coordinate-space}
{wiki}

= Infinite dimensional
{parent=dimension}

= Infinite dimensions
{synonym}

https://math.stackexchange.com/questions/466707/what-are-some-examples-of-infinite-dimensional-vector-spaces

= Finite dimensional
{parent=infinite-dimensional}

= Finite dimension
{synonym}

= Complex coordinate space
{parent=real-coordinate-space}
{title2=$\C^n$}
{wiki}

= Complex dot product
{parent=complex-coordinate-space}

https://math.stackexchange.com/questions/2459814/what-is-the-dot-product-of-complex-vectors/4300169#4300169

The complex dot product is defined as:
$$
\sum a_i \overline{b_i}
$$

E.g. in $\C^1$:
$$
(a + bi) \cdot (c + di) = (a + bi) (\overline{c + di}) = (a + bi) (c - di) = (ac + bd) + (bc - ad)i
$$

We can see therefore that this is a \x[form-mathematics], and a positive definite because:
$$
(a + bi) \cdot (a + bi) = (aa + bb) + (ba - ab)i = a^2 + b^2
$$

Just like the usual \x[dot-product], this will be a \x[positive-definite-symmetric-bilinear-form] by definition.

= Euclidean space
{c}
{parent=real-coordinate-space}
{wiki}

= Euclidean
{synonym}

\x[r-n] with extra structure added to make it into a \x[metric-space]{parent}.

= Euclidean metric signature matrix
{parent=euclidean-space}

The \x[identity-matrix].

= Cartesian coordinate system
{c}
{parent=euclidean-space}
{wiki}

= Cartesian coordinate
{synonym}

= Polar coordinate system
{c}
{parent=euclidean-space}
{wiki}

= Polar coordinate
{synonym}

= Spherical coordinate system
{c}
{parent=polar-coordinate-system}
{wiki}

= Spherical coordinate
{synonym}

= Pythagorean theorem
{c}
{parent=euclidean-space}
{wiki}

= Non-Euclidean geometry
{c}
{parent=euclidean-space}
{wiki}

= Non-Euclidean
{synonym}

= Elliptic geometry
{parent=non-euclidean-geometry}
{wiki}

= Model of elliptic geometry
{parent=elliptic-geometry}

= Projective elliptic geometry
{parent=model-of-elliptic-geometry}

= Projective model of elliptic geometry
{synonym}

Each elliptic space can be modelled with a \x[real-projective-space]. The best thing is to just start thinking about the \x[real-projective-plane].

= Hyperbolic gemoetry
{parent=non-euclidean-geometry}
{wiki}

= Hyperbolic functions
{parent=hyperbolic-gemoetry}
{wiki}

= Hyperbolic sine
{parent=hyperbolic-functions}

= sinh
{synonym}

= Hyperbolic cossine
{parent=hyperbolic-functions}

= cosh
{synonym}

= Distribution
{disambiguate=mathematics}
{parent=calculus}

Generalize \x[function-mathematics]{p} to allow adding some useful things which people wanted to be classical functions but which are not,

It therefore requires you to redefine and reprove all of calculus.

For this reason, most people are tempted to assume that all the hand wavy intuitive arguments \x[undergrad] teachers give are true and just move on with life. And they generally are.

One notable example where distributions pop up are the \x[eigenvector]{p} of the \x[position-operator] in \x[quantum-mechanics], which are given by \x[dirac-delta-function]{p}, which is most commonly rigorously defined in terms of \x[distribution-mathematics]{p}.

Distributions are also defined in a way that allows you to do calculus on them. Notably, you can define a \x[derivative], and the derivative of the \x[heaviside-step-function] is the \x[dirac-delta-function].

= Dirac delta function
{c}
{parent=distribution-mathematics}
{wiki}

The "0-width" pulse \x[distribution-mathematics] that integrates to a step.

There's not way to describe it as a classical \x[function-mathematics], making it the most important example of a \x[distribution-mathematics].

Applications:
* \x[position-operator] in \x[quantum-mechanics]. It's not a coincidence that the function is named after \x[paul-dirac].

= Green's function
{c}
{parent=dirac-delta-function}
{wiki}

= Heaviside step function
{c}
{parent=dirac-delta-function}
{wiki}

= Complex analysis
{parent=calculus}
{wiki}

The surprising thing is that a bunch of results are simpler in complex analysis!

= Holomorphic function
{parent=complex-analysis}
{wiki}

Being a complex holomorphic function is an extremely strong condition.

The existence of the first derivative implies the existence of all derivatives.

Another extremely strong consequence is the \x[identity-theorem].

"Holos" means "entire" in Greek, so maybe this is a reference to the fact that due to the identity theorem, knowing the function on a small open ball implies knowing the function everywhere.

= Analytic continuation
{parent=complex-analysis}
{wiki}

\x[visualizing-the-riemann-hypothesis-and-analytic-continuation-by-3blue1brown-2016] is a good quick visual non-mathematical introduction is to it.

The key question is: how can this continuation be unique since we are defining the function outside of its original domain?

The answer is: due to the \x[identity-theorem].

= Visualizing the Riemann hypothesis and analytic continuation by 3Blue1Brown (2016)
{parent=analytic-continuation}

Good ultra quick visual non-mathematical introduction to the Riemann hypothesis and analytic continuation.

\Video[http://youtube.com/watch?v=sD0NjbwqlYw]

= Identity theorem
{parent=analytic-continuation}
{wiki}

Essentially, defining an \x[holomorphic-function] on any open subset, no matter how small, also uniquely defines it everywhere.

This is basically why it makes sense to talk about \x[analytic-continuation] at all.

One way to think about this is because the \x[taylor-series] matches the exact value of an holomorphic function no matter how large the difference from the starting point.

Therefore a holomorphic function basically only contains as much information as a countable sequence of numbers.

= Riemann zeta function
{c}
{parent=identity-theorem}
{wiki}

= Riemann hypothesis
{c}
{parent=riemann-zeta-function}
{wiki}

\x[visualizing-the-riemann-hypothesis-and-analytic-continuation-by-3blue1brown-2016] is a good quick visual non-mathematical introduction is to it.

One of the \x[millennium-prize-problems]{parent} and \x[hilbert-s-problems]{parent}.

= Hilbert space
{c}
{parent=calculus}
{wiki}

Key for \x[quantum-mechanics], see: \x[mathematical-formulation-of-quantum-mechanics], the most important example by far being \x[l2].

= Complete basis
{parent=hilbert-space}

Finding a complete basis such that each vector solves a given \x[differential-equation] is the basic method of solving \x[partial-differential-equation] through \x[separation-of-variables].

The first example of this you must see is \x[solving-partial-differential-equations-with-the-fourier-series].

Notable examples:
* \x[fourier-series]{child} for the \x[heat-equation] as shown at \x[fourier-basis-is-complete-for-l2] and \x[solving-partial-differential-equations-with-the-fourier-series]
* \x[hermite-functions]{child} for the \x[quantum-harmonic-oscillator]
* \x[legendre-polynomials]{child} for \x[laplace-s-equation] in \x[spherical-coordinate]{p}
* \x[bessel-function]{child} for the \x[2d-wave-equation-on-a-circular-domain] in \x[polar-coordinate]{p}

= Differential equation
{parent=calculus}
{tag=functional-equation}
{wiki}

= Euler number
{c}
{parent=differential-equation}
{title2=$e$}
{wiki}

= Natural logarithm
{parent=euler-number}
{title2=$ln(n)$, $log_e(n)$}
{wiki}

= Euler-Mascheroni constant
{c}
{parent=natural-logarithm}
{wiki=Euler–Mascheroni constant}

\x[convergence-mathematics]{c}: https://math.stackexchange.com/questions/629630/simple-proof-euler-mascheroni-gamma-constant

= Linear differential equation
{parent=differential-equation}
{wiki}

The name is a bit obscure if you don't think in very generalized terms right out of the gate. It refers to a \x[linear-polynomial] of \x[multivariate-polynomial][multiple variables], which by definition must have the super simple form of:
$$
f(x_0, x_1, ..., x_n) = c_0x_0 + c_1x_1 + ... + c_nx_n + k
$$
and then we just put the unknown $y$ and each derivative into that simple polynomial:
$$
f(y(x), y'(x), ..., y^{(n)}(x)) = c_0y + c_1y' + ... + c_ny^{(n)} + k
$$
except that now the $c_i$ are not just constants, but they can also depend on the argument $x$ (but not on $y$ or its derivatives).

Explicit solutions exist for the very specific cases of:
* constant coefficients, any degree. These were known for a long time, and are were studied when \x[ciro-santilli-s-formal-education][Ciro was at university] in the \x[university-of-sao-paulo].
* degree 1 and any coefficient

= Holonomic function
{parent=linear-differential-equation}
{wiki}

= Order of a differential equation
{parent=differential-equation}
{wiki}

Order of the highest derivative that appears.

= Ordinary differential equation
{parent=differential-equation}
{title2=ODE}
{wiki}

= Existence and uniqueness of solutions of ordinary differential equations
{parent=ordinary-differential-equation}
{tag=existence-and-uniqueness}

= Peano existence theorem
{c}
{parent=existence-and-uniqueness-of-solutions-of-ordinary-differential-equations}
{wiki}

= Picard-Lindelöf theorem
{c}
{parent=existence-and-uniqueness-of-solutions-of-ordinary-differential-equations}
{wiki=Picard–Lindelöf theorem}

= System of ordinary differential equations
{parent=ordinary-differential-equation}

= System of linear ordinary differential equations
{parent=system-of-ordinary-differential-equations}

= Partial differential equation
{parent=differential-equation}
{wiki}

= PDE
{c}
{title2}
{synonym}

= Analytical method to solve a partial differential equation
{parent=partial-differential-equation}

* \x[how-to-use-lie-groups-to-solve-differential-equations]{child}

= Separation of variables
{parent=analytical-method-to-solve-a-partial-differential-equation}
{wiki}

Technique to solve \x[partial-differential-equation]{p}

Naturally leads to the \x[fourier-series], see: \x[solving-partial-differential-equations-with-the-fourier-series], and to other analogous expansions:

One notable application is the solution of the \x[schrodinger-equation] via the \x[time-independent-schrodinger-equation].

Bibliography:
* https://math.libretexts.org/Bookshelves/Differential_Equations/Book%3A_Differential_Equations_for_Engineers_(Lebl)/4%3A_Fourier_series_and_PDEs/4.06%3A_PDEs_separation_of_variables_and_the_heat_equation on \x[libretexts] for the \x[heat-equation]

= Numerical method to solve a partial differential equation
{parent=partial-differential-equation}
{wiki=Numerical_methods_for_partial_differential_equations}

= Numerical methods to solve partial differential equations
{synonym}

The \x[finite-element-method] is one of the most common ways to solve PDEs in practice.

= Variational formulation of a partial differential equation
{parent=numerical-method-to-solve-a-partial-differential-equation}

https://www.cis.upenn.edu/~cis515/cis515-12-sl11.pdf

Used for example in \x[freefem] and \x[fenics-project] as the input description of the PDEs, TODO why.

= Weak solution
{parent=variational-formulation-of-a-partial-differential-equation}
{wiki}

= Finite element method
{parent=numerical-method-to-solve-a-partial-differential-equation}
{wiki}

Used to solve \x[partial-differential-equation].

TODO understand, give intuition, justification of bounds and \x[javascript] demo.

= Important partial differential equation
{parent=partial-differential-equation}

The majority likely comes from \x[physics]:
* \x[heat-equation]{child}
* \x[wave-equation]{child}
* \x[maxwell-s-equations]{child}
* \x[schrodinger-equation]{child}
* \x[navier-stokes-equations]{child}

= Laplace's equation
{c}
{parent=important-partial-differential-equation}
{wiki}

Like a \x[heat-equation] but for functions without time dependence, space-only.

TODO confirm: does the solution of the heat equation always converge to the solution of the Laplace equation as time tends to infinity?

In one dimension, the Laplace equation is boring as it is just a straight line since the second derivative must be 0. That also matches our intuition of the limit solution of the heat equation.

Uniqueness: \x[uniqueness-theorem-for-poisson-s-equation].

= Legendre polynomials
{c}
{parent=laplace-s-equation}

Show up when solving the \x[laplace-s-equation] on \x[spherical-coordinate]{p} by \x[separation-of-variables], which leads to the \x[differential-equation] shown at: https://en.wikipedia.org/w/index.php?title=Legendre_polynomials&oldid=1018881414#Definition_via_differential_equation[].

= Poisson's equation
{c}
{parent=laplace-s-equation}
{wiki}

Generalization of \x[laplace-s-equation] where the value is not necessarily 0.

= Uniqueness theorem for Poisson's equation
{c}
{parent=poisson-s-equation}
{wiki}

= Harmonic function
{parent=laplace-s-equation}
{wiki}

A solution to \x[laplace-s-equation].

= Spherical harmonic
{parent=harmonic-function}
{wiki=Spherical_harmonics}

Correspond to the angular part of \x[laplace-s-equation] in spherical coordinates after using \x[separation-of-variables] as shown at: https://en.wikipedia.org/wiki/Spherical_harmonics#Laplace's_spherical_harmonics

= Heat equation
{parent=important-partial-differential-equation}
{wiki}

Besides being useful in engineering, it was very important historically from a "development of mathematics point of view", e.g. \x[history-of-the-fourier-series][it was the initial motivation for the Fourier series].

Some interesting properties:
* TODO confirm: for a fixed boundary condition that does not depend on time, the solutions always approaches one specific equilibrium function.

  This is in contrast notably with the \x[wave-equation], which can oscillate forever.
* TODO: for a given point, can the temperature go down and then up, or is it always monotonic with time?
* information propagates instantly to infinitely far. Again in contrast to the wave equation, where information propagates at wave speed.

Sample numerical solutions:
* with \x[freefem]:
  * \x[heat-dirichlet-1d-freefem]
  * \x[heat-dirichlet-2d-freefem]

= Wave equation
{parent=important-partial-differential-equation}
{wiki}

Describes perfect lossless waves on the surface of a string, or on a water surface.

\x[javascript] toy solvers:
* https://jtiscione.github.io/webassembly-wave/index.html circular domain, create waves with mouse click
* https://dionyziz.com/graphics/wave-experiment/ with useless 3D \x[webgl] visualization :-), waves with mouse click. Solving itself done on \x[cpu], not GPU.

Uniqueness: https://math.stackexchange.com/questions/1113622/uniqueness-of-solutions-to-the-wave-equation

As mentioned at: https://math.stackexchange.com/questions/579453/real-world-application-of-fourier-series/3729366#3729366[] from \x[solving-partial-differential-equations-with-the-fourier-series] citing https://courses.maths.ox.ac.uk/node/view_material/1720[], analogously to the \x[heat-equation], the wave linear equation can be be solved nicely with \x[separation-of-variables].

= The wave equation can be seen as infinitely many infinitesimal coupled oscillators
{parent=wave-equation}

TODO confirm, see also: \x[coupled-oscillators]. And then this idea can be used to define/motivate \x[quantum-field-theory] in terms of \x[quantum-harmonic-oscillator]{p} with \x[second-quantization].

* https://youtu.be/SMmFgIEGYtw?t=324 Quantum Field Theory 2a - Field Quantization I by \x[viascience] (2018)

= Wave
{parent=wave-equation}
{wiki}

= Diffraction
{parent=wave-equation}
{wiki}

= Arago spot
{c}
{parent=diffraction}
{wiki}

= Huygens-Fresnel principle
{c}
{parent=diffraction}
{wiki=Huygens–Fresnel principle}

= Kirchhoff's diffraction formula
{c}
{parent=huygens-fresnel-principle}
{wiki}

Approximation to \x[huygens-fresnel-principle].

= Fraunhofer diffraction
{c}
{parent=kirchhoff-s-diffraction-formula}
{wiki}

Far field approximation to \x[kirchhoff-s-diffraction-formula], i.e. when the plane of observation is far from the object diffracting.

= Fresnel diffraction
{c}
{parent=kirchhoff-s-diffraction-formula}
{wiki}

Near field approximation to \x[kirchhoff-s-diffraction-formula], i.e. when the plane of observation is near the object diffracting.

= Refraction
{parent=wave-equation}
{wiki}

= Resonance
{parent=wave-equation}
{wiki}

= Resonate
{synonym}

= Resonates
{synonym}

Examples:
* \x[mechanical-resonance]

= Wave interference
{parent=wave-equation}
{wiki}

= Interference pattern
{parent=wave-interference}

What you see along a line or plane in a \x[wave-interference].

Notably used for the pattern of the \x[double-slit-experiment].

= 2D wave equation on a circular domain
{parent=wave-equation}
{wiki=Vibrations_of_a_circular_membrane}

= Bessel function
{parent=2d-wave-equation-on-a-circular-domain}
{wiki}

Shows up when trying to solve \x[2d-wave-equation-on-a-circular-domain] in \x[polar-coordinate]{p} with \x[separation-of-variables], where we have to decompose the initial condition in termes of a \x[fourier-bessel-series], exactly like the \x[fourier-series] appears when solving the wave equation in linear coordinates.

For the same fundamental reasons, also appears when calculating the \x[schrodinger-equation-solution-for-the-hydrogen-atom].

= Fourier-Bessel series
{parent=bessel-function}
{wiki=Fourier–Bessel_series}

Completeness: https://math.stackexchange.com/questions/2192665/is-this-set-of-bessel-functions-a-basis-for-all-c10-a-functions TODO

This is the \x[bessel-function] analogue to \x[fourier-basis-is-complete-for-l2].

= Helmholtz equation
{c}
{parent=wave-equation}
{wiki}

\x[eigenvalue] problem of \x[laplace-s-equation].

= Existence and uniqueness of solutions of partial differential equations
{parent=partial-differential-equation}
{tag=existence-and-uniqueness}

If you have a \x[pde] that models \x[physics][physical phenomena], it is fundamental that:
* there must exist a solution for every physically valid initial condition, otherwise it means that the equation does not describe certain cases of reality
* the solution must be unique, otherwise how are we to choose between the multiple solutions?

Unlike for \x[ordinary-differential-equation]{p} which have the https://en.wikipedia.org/wiki/Picard–Lindelöf_theorem[Picard–Lindelöf theorem], the existence and uniqueness of solution is not well solved for PDEs.

For example, \x[navier-stokes-existence-and-smoothness]{child} was one of the \x[millennium-prize-problems].

= Partial differential equation solver
{parent=partial-differential-equation}

= FreeFem
{parent=partial-differential-equation-solver}
{wiki=FreeFem%2B%2B}

https://freefem.org/

https://github.com/FreeFem/FreeFem-sources

Started in 1987 and written in Pascal, by the French from \x[pierre-and-marie-curie-university], the French are really strong in \x[numerical-analysis].

Ciro wasn't expecting it to be as old. Ported to C++ in 1992.

The fact that French wrote it can be seen in the documentation, for example https://doc.freefem.org/tutorials/index.html uses file extension `mycode.edp` instead of `mycode.pde` where `dep` stands for "https://fr.wikipedia.org/wiki/Équation_aux_dérivées_partielles[Équation aux dérivées partielles]".

Besides the painful build, using FreeFem is relatively simple, as can be seen from the examples on the website.

They do use a \x[domain-specific-language] on the examples, which appears to be the main/only interface, which is a bad thing, Ciro would rather have a \x[python] \x[api] as the "main API", which is more the approach taken by the \x[fenics-project], but so be it. This \x[domain-specific-language] business means that you always stumble upon basic stuff you want to do but can't, and then you have to think about how to share data between the simulation and the plotting. The plotting notably is super complex and they can't implement all of what people want, upstream examples often offload that to gnuplot. This is potentially a big advantage of \x[fenics-project].

It nice though that they do have some graphics out of the box, as that allows to quickly debug common problems.

Uses \x[variational-formulation-of-a-partial-differential-equation], which is not immediately obvious to beginners? The introduction https://doc.freefem.org/tutorials/poisson.html gives an ultra quick example, but your are mostly on your own with that.

On Ubuntu 20.04, the `freefem` is a bit out-of-date (3.5.8, there isn't even a tag for that in the \x[github] repo, and refs/tags/release_3_10 is from 2010!) and fails to run the examples from the website. It did work with the example package though, but the output does not have color, which makes me sad :-)
``
sudo apt install freefem freefem-examples
freefem /usr/share/doc/freefem-examples/heat.pde
``

So let's just compile the latest v4.6 it from source, on Ubuntu 20.04:
``
sudo apt build-dep freefem
git clone https://github.com/FreeFem/FreeFem-sources
cd FreeFem-sources
# Post v4.6 with some fixes.
git checkout 3df0e2370d9752801ac744b11307b14e16743a44

# Won't apply automatically due to tab hell.
# https://superuser.com/questions/607410/how-to-copy-paste-tab-characters-via-the-clipboard-into-terminal-session-on-gnom
git apply <<'EOS'
diff --git a/3rdparty/ff-petsc/Makefile b/3rdparty/ff-petsc/Makefile
index dc62ab06..13cd3253 100644
--- a/3rdparty/ff-petsc/Makefile
+++ b/3rdparty/ff-petsc/Makefile
@@ -204,7 +204,7 @@ $(SRCDIR)/tag-make-real:$(SRCDIR)/tag-conf-real
 $(SRCDIR)/tag-install-real :$(SRCDIR)/tag-make-real
     cd $(SRCDIR) && $(MAKE) PETSC_DIR=$(PETSC_DIR) PETSC_ARCH=fr install
     -test -x "`type -p otool`" && make changer
-    cd $(SRCDIR) && $(MAKE) PETSC_DIR=$(PETSC_DIR) PETSC_ARCH=fr check
+    #cd $(SRCDIR) && $(MAKE) PETSC_DIR=$(PETSC_DIR) PETSC_ARCH=fr check
     test -e $(DIR_INSTALL_REAL)/include/petsc.h
     test -e $(DIR_INSTALL_REAL)/lib/petsc/conf/petscvariables
     touch $@
@@ -293,7 +293,6 @@ $(SRCDIR)/tag-tar:$(PACKAGE)
     -tar xzf $(PACKAGE)
     patch -p1 < petsc-hpddm.patch
 ifeq ($(WIN32DLLTARGET),)
-    patch -p1 < petsc-metis.patch
 endif
     touch $@
 $(PACKAGE):
EOS

autoreconf -i
./configure --enable-download --enable-optim --prefix="$(pwd)/../FreeFem-install"
./3rdparty/getall -a
cd 3rdparty/ff-petsc
make petsc-slepc
cd -
./reconfigure
make -j`nproc`
make install
cd ../FreeFem-install
PATH="${PATH}:$(pwd)/bin" ./bin/FreeFem++ ../FreeFem-sources/examples/tutorial/
``

Ciro's initial build experience was a bit painful, possibly because it was done on a relatively new Ubuntu 20.04 as of June 2020, but in the end it worked: https://github.com/FreeFem/FreeFem-sources/issues/141

The main/only dependency appears to be https://en.wikipedia.org/wiki/Portable,_Extensible_Toolkit_for_Scientific_Computation[PETSc] which is used by default, which is a good sign, as that library appears to automatically parallelize a single input to several backends (single \x[cpu], MPI, GPU) so you know things will scale up as you reach simulations.

The problem is that it compiling such a complex dependency opens up much more room for hard to solve compilation errors, and takes a lot more time.

= FreeFem examples
{parent=freefem}

= heat-dirichlet.1d.freefem
{parent=freefem-examples}

1-dimensional \x[heat-equation] example with \x[dirichlet-boundary-condition]
* \a[freefem/heat-dirichlet.1d.freefem]

= heat-dirichlet-2d-freefem
{parent=freefem-examples}

2-dimensional \x[heat-equation] example with \x[dirichlet-boundary-condition]:
* \a[freefem/heat-dirichlet.2d.freefem]

= FEniCS Project
{c}
{parent=partial-differential-equation-solver}
{wiki}

https://fenicsproject.org/

One big advantage over \x[freefem] is that it uses plain old \x[python] to describe the problems instead of a \x[domain-specific-language]. \x[matplotlib] is used for plotting by default, so we get full Python power out of the box!

Also uses \x[variational-formulation-of-a-partial-differential-equation] like \x[freefem] which is a pain.

One downside is that its documentation is a Springer published PDF https://link.springer.com/content/pdf/10.1007%2F978-3-319-52462-7.pdf which is several years out-of-date (tested with FEnics 2016.2. Newbs. This causes problems e.g.: https://stackoverflow.com/questions/53730427/fenics-did-not-show-figure-nameerror-name-interactive-is-not-defined/57390687#57390687

\x[system-of-partial-differential-equations] are mentioned at: https://link.springer.com/content/pdf/10.1007%2F978-3-319-52462-7.pdf 3.5 "A system of advection–diffusion–reaction equations". You don't need to manually iterate between the equations.

On Ubuntu 20.04 as per https://fenicsproject.org/download/
``
sudo apt-get install software-properties-common
sudo add-apt-repository ppa:fenics-packages/fenics
sudo apt-get update
sudo apt-get install --no-install-recommends fenics
sudo apt install fenics
python3 -m pip install -u matplotlib
``
Before 2020-06, it was failing with:
``
E: The repository 'http://ppa.launchpad.net/fenics-packages/fenics/ubuntu focal Release' does not have a Release file.
``
but they seem to have created the Ubuntu 20.04 package as of 2020-06, so it now worked! https://askubuntu.com/questions/866901/what-can-i-do-if-a-repository-ppa-does-not-have-a-release-file

TODO heat equation \x[hello-world].

= Hans Petter Langtangen
{c}
{parent=fenics-project}
{wiki}

\x[github] account: https://github.com/hplgit

It should be mentioned that when you start \x[googling] for \x[partial-differential-equation][PDE] stuff, you will reach Han's writings a lot under his \x[github-pages]: http://hplgit.github.io/[], and he is one of the main authors of the \x[fenics-project].

Unfortunately he died of \x[cancer] in 2016, shame, he seemed like a good educator.

He also published to \x[github] pages with his own crazy \x[markdown]-like multi-output \x[markup-language]: https://github.com/hplgit/doconce[].

Rest in peace, Hans.

= System of partial differential equations
{parent=partial-differential-equation}

In many important applications, what you have to solve is not just a single \x[partial-differential-equation], but multiple partial differential equations coupled to each other. This is the case for many key PDEs including:
* \x[maxwell-s-equations], see: \x[explicit-scalar-form-of-the-maxwell-s-equations]{full}
* \x[navier-stokes-equations]
* \x[schrodinger-equation], see: \x[why-are-complex-numbers-used-in-the-schrodinger-equation]{full}

= Classification of second order partial differential equations into elliptic, parabolic and hyperbolic
{parent=partial-differential-equation}

One major application of this classification is that different \x[boundary-condition]{p} are suitable for different types of \x[partial-differential-equation]{p} as explained at: \x[which-boundary-conditions-lead-to-existence-and-uniqueness-of-a-second-order-pde].

Bibliography:
* https://math.stackexchange.com/questions/1090299/why-are-elliptic-parabolic-hyperbolic-pdes-called-elliptic-parabolic-hyperb

= Elliptic partial differential equation
{parent=classification-of-second-order-partial-differential-equations-into-elliptic-parabolic-and-hyperbolic}
{wiki}

= Parabolic partial differential equation
{parent=classification-of-second-order-partial-differential-equations-into-elliptic-parabolic-and-hyperbolic}
{wiki}

= Hyperbolic partial differential equation
{parent=classification-of-second-order-partial-differential-equations-into-elliptic-parabolic-and-hyperbolic}
{wiki}

= Which boundary conditions lead to existence and uniqueness of a second order PDE
{parent=classification-of-second-order-partial-differential-equations-into-elliptic-parabolic-and-hyperbolic}

http://www.cns.gatech.edu/~predrag/courses/PHYS-6124-12/StGoChap6.pdf 6.1 "Classification of PDE's" clarifies which boundary conditions are needed for existence and uniqueness of each \x[classification-of-second-order-partial-differential-equations-into-elliptic-parabolic-and-hyperbolic][type of second order of PDE]:
* \x[elliptic-partial-differential-equation] and \x[parabolic-partial-differential-equation]: \x[dirichlet-boundary-condition] or \x[neumann-boundary-condition]
* \x[hyperbolic-partial-differential-equation]: \x[cauchy-boundary-condition]

= Phase space
{parent=differential-equation}
{wiki}

This idea comes up particularly in the \x[phase-space-coordinate] of \x[hamiltonian-mechanics].

= Boundary condition
{parent=differential-equation}

= Initial condition
{parent=boundary-condition}

Basically a subset of the \x[boundary-condition] for when one of the parameters is time and we are specifying values for the time 0.

= Boundary value problem
{parent=boundary-condition}
{wiki}

= Dirichlet boundary condition
{c}
{parent=boundary-condition}
{wiki}

Specifies fixed values.

Can be used for \x[elliptic-partial-differential-equation]{p} and \x[parabolic-partial-differential-equation]{p}.

Numerical examples:
* with \x[freefem]:
  * \x[heat-dirichlet-1d-freefem]
  * \x[heat-dirichlet-2d-freefem]

= Neumann boundary condition
{c}
{parent=boundary-condition}
{wiki}

Specifies the derivative in a direction normal to the boundary.

Can be used for \x[elliptic-partial-differential-equation]{p} and \x[parabolic-partial-differential-equation]{p}.

= Cauchy boundary condition
{c}
{parent=neumann-boundary-condition}
{wiki}

Sets both a \x[dirichlet-boundary-condition] and a \x[neumann-boundary-condition] for a single part of the boundary.

Can be used for \x[hyperbolic-partial-differential-equation]{p}.

We understand intuitively that this imposes stricter requirements on solutions, which makes it easier to guarantee uniqueness, but also harder to have existence. TODO intuitively why hyperbolic need this extra level of restriction.

= Robin boundary condition
{parent=neumann-boundary-condition}
{c}
{wiki}

Linear combination of a \x[dirichlet-boundary-condition] and \x[neumann-boundary-condition] at each point of the boundary.

Examples:
* \x[heat-equation] when metal plaque is immersed in a large external environment of fixed temperature.

  In this case, the normal derivative at the boundary is proportional to the difference between the temperature of the boundary and the fixed temperature of the external environment.

  The result as time tends to infinity is that the temperature of the plaque tends to that of the environment.

  Shown a solved example in the \x[freefem] tutorial: https://doc.freefem.org/tutorials/thermalConduction.html (https://github.com/FreeFem/FreeFem-doc/blob/1d5996d8b891fd553fd318321249c2c30f693fc3/source/tutorials/thermalConduction.rst)

= Open boundary condition
{parent=neumann-boundary-condition}

In the context of wave-like equations, an open-boundary condition is one that "lets the wave go through without reflection".

This condition is very useful when we want to simulate infinite domains with a numerical method. \x[ciro-santilli] wants to do this all the time when trying to come up with demos for his \x[physics] writings.

Here are some resources that cover such boundary conditions:
* https://www.asc.tuwien.ac.at/~arnold/pdf/graz/graz.pdf lots of slides
* http://hplgit.github.io/wavebc/doc/pub/._wavebc_cyborg002.html mentions them and gives a 1D formula. It mentions that things get complicated in 2D and 3D TODO why.

  The other page: http://hplgit.github.io/wavebc/doc/pub/._wavebc_cyborg003.html shows solution demos.

= Mixed boundary condition
{parent=neumann-boundary-condition}
{wiki}

Multiple \x[boundary-condition]{p} for different parts of the boundary.

= Time dependent boundary condition
{parent=boundary-condition}

Most commonly, \x[boundary-condition]{p} such as the \x[dirichlet-boundary-condition] are taken to be fixed values in time.

But it also makes sense to think about cases where those values vary in time.

Some bibliography:
* https://math.stackexchange.com/questions/261251/heat-equation-with-time-dependent-boundary-conditions
* https://secure.math.ubc.ca/~peirce/M257_316_2012_Lecture_20.pdf

= Control theory
{parent=differential-equation}
{wiki}

This basically adds one more ingredient to \x[partial-differential-equation]{p}: a \x[function] that we can select.

And then the question becomes: if this function has such and such limitation, can we make the solution of the \x[differential-equation] have such and such property?

It's quite fun from a mathematics point of view!

Control theory also takes into consideration possible \x[discretization] of the domain, which allows using \x[numerical-methods-to-solve-partial-differential-equations], as well as digital, rather than analogue control methods.

= Control engineering
{parent=control-theory}
{wiki}

= Control system
{parent=control-theory}
{wiki}

= Feedback loop
{parent=control-theory}

= Series
{disambiguate=mathematics}
{parent=calculus}
{wiki}

= Power series
{parent=series-mathematics}
{wiki}

= Radius of convergence
{parent=power-series}
{wiki}

= Taylor series
{c}
{parent=power-series}
{wiki}

= Gradient, Divergence, Curl, and Laplacian
{parent=calculus}

= Curl
{disambiguate=mathematics}
{parent=gradient-divergence-curl-and-laplacian}
{title2=$\curl{}$}
{wiki}

Points in the direction in which a wind spinner spins fastest.

= Nabla symbol
{parent=gradient-divergence-curl-and-laplacian}
{title2=$\nabla$}
{wiki}

= Nabla
{synonym}

As if \x[greek-letter]{p} weren't enough, \x[physicist]{p} and \x[mathematician]{p} also like to make up tons of symbols, \x[mathematical-symbol-that-looks-like-a-greek-letter-but-isn-t][some of which look like the could actually be Greek letters]!

Nabla is one of those: it was completely made up in modern times, and just happens to look like an inverted upper case \x[delta-letter] to make things even more confusing!

Nabla means "harp" in Greek, which looks like the symbol.

= Del
{parent=nabla-symbol}
{wiki}

Oh, and if it weren't enough, \x[mathematician]{p} have a separate name for the damned \x[nabla-symbol] : "del" instead of "nabla".

TODO why is it called "Del"? Is is because it is an inverted uppercase \x[delta-letter]?

= Divergence
{parent=gradient-divergence-curl-and-laplacian}
{title2=$\div{}$, $div()$}
{wiki}

Takes a \x[vector-mathematics] field as input and produces a scalar field.

Mnemonic: it gives out the amount of fluid that is going in or out of a given volume per unit of time.

Therefore, if you take a cubic volume:
* the input has to be the 6 flows across each face, therefore 3 derivatives
* the output is the variation of the quantity of fluid, and therefore a scalar

= Gradient
{parent=gradient-divergence-curl-and-laplacian}
{title2=$\grad{}$}
{wiki}

Takes a scalar field as input and produces a vector field.

Mnemonic: the gradient shows the direction in which the function increases fastest.

Think of a color gradient going from white to black from left to right.

Therefore, it has to:
* take a scalar field as input. Otherwise, how do you decide which vector is larger than the other?
* output a vector field that contains the direction in which the scalar increases fastest locally at each point. This has to give out vectors, since we are talking about directions

= Laplace operator
{parent=gradient-divergence-curl-and-laplacian}
{title2=$\Delta$, $\nabla^2$}
{wiki}

= Laplacian
{c}
{synonym}

Can be denoted either by:
* the upper case \x[greek-letter] \x[delta]
* \x[nabla-symbol] squared
Our default symbol is going to be:
$$
\laplacian{}
$$

= D'alembert operator
{c}
{parent=laplace-operator}
{title2=$\Box$}
{wiki}

The \x[laplace-operator] for \x[minkowski-space].

Can be nicely written with \x[einstein-notation] as shown at: \x[d-alembert-operator-in-einstein-notation]{full}.

= Infinitesimal
{parent=calculus}
{wiki}

Just use \x[limit-mathematics]{p} instead, please. The \x[french] are particularly guilty of this.

= Discrete mathematics
{parent=mathematics}
{wiki}

= Graph
{disambiguate=discrete mathematics}
{parent=discrete-mathematics}
{wiki}

= Graph
{synonym}

= Edge
{disambiguate=graph}
{parent=graph-discrete-mathematics}
{wiki}

= Vertex
{parent=graph-discrete-mathematics}
{wiki}

= Node
{disambiguate=graph}
{synonym}

= Functional equation
{parent=mathematics}
{wiki}

= Cauchy's functional equation
{parent=functional-equation}
{wiki}

Nice result on \x[lebesgue-measurable] required for unicity.

= Game theory
{parent=mathematics}
{wiki}

As mentioned at \x[human-compatible-by-stuart-j-russell-2019], \x[game-theory] can be seen as the part of \x[artificial-intelligence] that deas with scenarios where multiple intelligent agents are involved.

= Balance of power
{c}
{parent=game-theory}
{wiki}

= Nash equilibrium
{c}
{parent=game-theory}
{wiki}

The best example to look at first is the \x[penalty-kick-left-right-nash-equilibrium].

Then, a much more interesting example is choosing a deck of a TCG competition: \x[magic-the-gathering-meta-based-deck-choice-is-a-bimatrix-game], which is the exact same, but each player has N choices rather than 2.

The next case that should be analyzed is the \x[prisoner-s-dilemma].

The key idea is that:
* imagine that the game will be played many times between two players
* if one player always chooses one deck, the other player will adapt by choosing the anti-deck
* therefore, the best strategy for both players, is to pick decks randomly, each with a certain probability. This type of probabilistic approach is called a \x[mixed-strategy]
* if any player deviates from the equilibrium probability, then the other player can add more of the anti-deck to the deck that the other player deviated, and gain an edge

  Therefore, using equilibrium probabilities is the optimal way to play

= Penalty kick left right Nash equilibrium
{parent=nash-equilibrium}

When taking a penalty kick in \x[soccer], the kicker must chose left or right.

And before he kicks, the goalkeeper must also decide left or right, because there is no time to see where the ball is going.

Because the kicker is right footed however, he kicker kicks better to one side than the other. So we have four probabilities:
* goal kick left keeper jumps left
* goal kick right keeper jumps right
* goal kick left keeper jumps right. Note that it is possible that this won't be a goal, even though the keeper is nowhere near the ball, as the ball might just miss the goal by a bit.
* kick right and keeper jumps left. Analogous to above

= Mixed strategy
{parent=nash-equilibrium}
{wiki}

= Prisoner's dilemma
{parent=nash-equilibrium}
{wiki}

https://en.wikipedia.org/wiki/Prisoner%27s_dilemma

= Bimatrix game
{parent=nash-equilibrium}

\x[magic-the-gathering-meta-based-deck-choice-is-a-bimatrix-game].

= Tit for tat
{parent=game-theory}
{wiki}

Related ideas:
* \x[fight-fire-with-fire]

= Geometry
{parent=mathematics}
{wiki}

= Fractal
{parent=geometry}
{wiki}

= Point
{disambiguate=geometry}
{parent=geometry}
{wiki}

= Point
{synonym}

= Line
{disambiguate=geometry}
{parent=point-geometry}
{wiki}

= Line
{synonym}

= Hyperplane
{parent=point-geometry}
{wiki}

Generalization of a \x[plane] for any number of dimensions.

Kind of the opposite of a line: the line has dimension 1, and the plane has dimension D-1.

In $D=2$, both happen to coincide, a boring example of an \x[exceptional-isomorphism].

= Plane
{disambiguate=geometry}
{parent=hyperplane}
{wiki}

= Plane
{synonym}

= n-sphere
{parent=geometry}
{title2=$S^n$}
{wiki}

= Antipodal point
{parent=n-sphere}
{wiki}

= Diameter
{parent=n-sphere}
{wiki}

= Radius
{parent=diameter}
{wiki}

= Circle
{parent=n-sphere}
{title2=$S^1$}
{wiki}

= 1-sphere
{synonym}
{title2}

= Sphere
{parent=n-sphere}
{title2=$S^2$}
{wiki}

= 2-sphere
{synonym}
{title2}

= Great circle
{parent=sphere}
{wiki}

= 3-sphere
{parent=n-sphere}
{title2=$S^3$}
{wiki}

Diffeomorphic to \x[su-2].

= Projective geometry
{parent=geometry}
{wiki}

= Projective space
{parent=projective-geometry}
{title2=$\projectiveSpace(V)$}
{wiki}

A \x[unique] projective space can be defined for any \x[vector-space].

The projective space associated with a given \x[vector-space] $V$ is denoted $\projectiveSpace(V)$.

The definition is to take the vector space, remove the zero element, and identify all elements that lie on the same line, i.e. $\vec{v} = \lambda \vec{w}$

The most important initial example to study is the \x[real-projective-plane].

= Projective plane
{parent=projective-space}
{wiki}

= Real projective space
{parent=projective-geometry}
{title2=$RP^n$, $\projectiveSpace(\R^{n+1})$}

In those cases at least, it is possible to add a \x[metric-mathematics] to the spaces, leading to \x[elliptic-geometry].

= Real projective line
{parent=real-projective-space}
{title2=$RP^1$, $\projectiveSpace(\R^2)$}
{wiki}

Just a \x[circle].

Take $\R^2$ with a line at $x = 0$. Identify all the points that an observer 

= Real projective plane
{parent=real-projective-space}
{title2=$RP^2$, $\projectiveSpace(\R^3)$}
{wiki}

For some reason, \x[ciro-santilli] is mildly obsessed with understanding and visualizing the real projective plane.

To see why this is called a plane, move he center of the sphere to $z=1$, and project each line passing on the center of the sphere on the x-y plane. This works for all points of the sphere, except those at the equator $z=1$. Those are the \x[points-at-infinity]. Note that there is one such point at infinity for each direction in the x-y plane.

= Synthetic geometry of the real projective plane
{parent=real-projective-plane}

It good to think about how \x[euclid-s-postulates] look like in the real projective plane:
* two parallel lines on the plane meet at a point on the sphere!

  Since there is one point of infinity for each direction, there is one such point for every direction the two parallel lines might be at. The \x[parallel-postulate] does not hold, and is replaced with a simpler more elegant version: every two lines meet at exactly one point.

  One thing to note however is that ther \x[real-projective-plane] does not have \x[angle]{p} defined on it by definition. Those can be defined, forming \x[elliptic-geometry] through the \x[projective-model-of-elliptic-geometry], but we can interpret the "parallel lines" as "two lines that meet at a point at infinity"
* points in the real projective plane are lines in \x[r-3]
* lines in the real projective plane are planes in \x[r-3].

  For every two projective points there is a single projective line that passes through them.

  Since it is a plane in \x[r-3], it always intersects the real plane at a line.

  Note however that not all lines in the real plane correspond to a projective line: only lines tangent to a circle at zero do.

Unlike the \x[real-projective-line] which is \x[homotopic] to the \x[circle], the \x[real-projective-plane] is not \x[homotopic] to the \x[sphere].

The \x[topological] difference bewteen the \x[sphere] and the \x[real-projective-space] is that for the \x[sphere] all those points in the x-y circle are identified to a single point.

One more generalized argument of this is the \x[classification-of-closed-surfaces], in which the \x[real-projective-plane] is a \x[sphere] with a hole cut and one \x[mobius-strip] glued in.

= Model of the real projective plane
{parent=real-projective-plane}

= Lines through origin model of the real projective plane
{parent=model-of-the-real-projective-plane}

This is the standard model.

= Spherical cap model of the real projective plane
{parent=model-of-the-real-projective-plane}

\x[ciro-santilli]'s preferred visualization of the real projective plane is a small variant of the standard "lines through origin in \x[r-3]".

Take a open half \x[sphere] e.g. a sphere but only the points with $z > 0$.

Each point in the half sphere identifies a unique line through the origin.

Then, the only lines missing are the lines in the x-y plane itself.

For those sphere points in the \x[circle] on the x-y plane, you should think of them as magic poins that are identified with the corresponding \x[antipodal-point], also on the x-y, but on the other side of the origin. So basically you you can teleport from one of those to the other side, and you are still in the same point.

Ciro likes this model because then all the magic is confined just to the $z=0$ part of the model, and everything else looks exactly like the sphere.

It is useful to contrast this with the sphere itself. In the sphere, all points in the circle $z = 0$ are the same point. But this is not the case for the \x[projective-plane]. You cannot instantly go to any other point on the $z=0$ by just moving a little bit, you have to walk around that circle.

\Image[https://raw.githubusercontent.com/cirosantilli/media/master/spherical-cap-model-of-the-real-projective-plane.svg]
{title=Spherical cap model of the real projective plane}
{description=On the x-y plane, you can magically travel immediately between \x[antipodal-point]{p} such as A/A', B/B' and C/C'. Or equivalently, those pairs are the same point. Every other point outside the x-y plane is just a regular point like a normal \x[sphere].}

= The real projective plane is not simply connected
{parent=real-projective-plane}

To see that the \x[real-projective-plane] is not \x[simply-connected-space], considering the \x[lines-through-origin-model-of-the-real-projective-plane], take a \x[loop-topology] that starts at $(1, 0, 0)$ and moves along the $y=0$ \x[great-circle] ends at $(-1, 0, 0)$.

Note that both of those points are the same, so we have a loop.

Now try to shrink it to a point.

There's just no way!

= Point at infinity
{parent=real-projective-plane}
{wiki}

= Points at infinity
{synonym}

= Homogenous coordinates
{parent=real-projective-plane}
{wiki}

= Polytope
{parent=geometry}
{wiki}

A \x[polygon] is a 2-dimensional \x[polytope], \x[polyhedra] is a 3-dimensional \x[polytope]. 

= Convex polytope
{parent=polytope}
{wiki}

= Convex
{synonym}

= Regular polytope
{parent=polytope}
{wiki}

TODO understand and explain definition.

= Classification of regular polytopes
{parent=regular-polytope}
{wiki=Regular_polytope#Classification_and_description}

The 3D regular convex polyhedrons are super famous, have the name: \x[platonic-solid], and have been known since antiquity. In particular, there are only 5 of them.

The counts per dimension are:
\Table[
|| Dimension
|| Count

| 2
| Infinite

| 3
| 5

| 4
| 6

| >4
| 3
]
{title=Number of regular polytopes per dimension.}

The cool thing is that the 3 that exist in 5+ dimensions are all of one of the three families:
* \x[simplex]
* \x[hypercube]
* \x[cross-polytope]
Then, the 2 3D missing ones have 4D analogues and the sixth one in 4D does not have a 3D analogue: https://en.wikipedia.org/wiki/24-cell[the 24-cell]. Yes, this is the kind of irregular stuff \x[ciro-santilli] lives \x[the-beauty-of-mathematics][for].

= Simplex
{parent=classification-of-regular-polytopes}
{wiki}

\x[triangle]{c}, \x[tetrahedron].

The name does not imply regular by default. For regular ones, you should say "regular polytope".

Non-regular description: take convex hull take D + 1 vertices that are not on a single D-plan.

= Hypercube
{parent=classification-of-regular-polytopes}
{wiki}

\x[square], cube. 4D case known as \x[tesseract].

Convex hull of all $\{-1, 1\}^D$ (\x[cartesian-product] power) D-tuples, e.g. in \x[3d]:
``
( 1,  1,  1)
( 1,  1, -1)
( 1, -1,  1)
( 1, -1, -1)
(-1,  1,  1)
(-1,  1, -1)
(-1, -1,  1)
(-1, -1, -1)
``

From this we see that there are $2^D$ \x[vertex]{p}.

Two \x[vertex]{p} are linked iff they differ by a single number. So each vertex has D neighbors.

= Hyperrectangle
{parent=hypercube}
{wiki}

The \x[regular-polytope][non-regular] version of the \x[hypercube].

= Cross polytope
{parent=classification-of-regular-polytopes}
{wiki}

Examples: \x[square], \x[octahedron].

Take $(0, 0, 0, \dots, 0)$ and flip one of 0's to $\pm 1$. Therefore has $2 \times D$ \x[vertex]{p}.

Each edge E is linked to every other edge, except it's opposite -E.

= Polygon
{parent=polytope}
{wiki}

= Quadrilateral
{parent=polygon}
{wiki}

= Rectangle
{parent=quadrilateral}
{wiki}

= Parallelogram
{parent=polygon}
{wiki}

= Parallelepiped
{parent=parallelogram}
{wiki}

\x[3d] \x[parallelogram].

= Volume of the parallelepiped
{parent=parallelepiped}

= Volume of a parallelepiped
{synonym}

= Regular polygon
{parent=polygon}
{wiki}

= Regular convex polygon
{parent=regular-polygon}

= Triangle
{parent=regular-convex-polygon}
{wiki}

= Square
{parent=regular-convex-polygon}
{tag=rectangle}
{wiki}

= Pentagon
{parent=regular-convex-polygon}
{wiki}

= Hexagon
{parent=regular-convex-polygon}
{wiki}

= Octagon
{parent=regular-convex-polygon}
{wiki}

= Polyhedron
{parent=polytope}
{wiki}

= Polyhedra
{synonym}

= Tetrahedron
{parent=polyhedron}
{wiki}

= Octahedron
{parent=polyhedron}
{wiki}

= Regular polyhedron
{parent=polytope}
{wiki}

= Platonic solid
{c}
{parent=regular-polyhedron}
{wiki}

A \x[convex] \x[regular-polyhedron].

Their \x[the-beauty-of-mathematics][beauty is a classification type result] as stated at \x[classification-of-regular-polytopes].

https://en.wikipedia.org/wiki/Platonic_solid#Topological_proof

= 4-polytope
{parent=polytope}
{wiki}

= Regular 4-polytope
{parent=4-polytope}
{wiki}

= Tesseract
{parent=regular-4-polytope}
{wiki}

= Differential geometry
{parent=geometry}

Bibliography:
* https://maths-people.anu.edu.au/~andrews/DG/ Lectures on Differential Geometry by Ben Andrews

= Lie group
{c}
{parent=differential-geometry}
{wiki}

The key and central motivation for studying Lie groups and their \x[lie-algebra]{p} appears to be to characterize \x[symmetry] in \x[lagrangian-mechanics] through \x[noether-s-theorem], just start from there.

Notably \x[local-symmetry]{p} appear to map to forces, and local means "around the identity", notably: \x[local-symmetries-of-the-lagrangian-imply-conserved-currents].

More precisely: \x[local-symmetries-of-the-lagrangian-imply-conserved-currents].

TODO \x[ciro-santilli] really wants to understand what all the fuss is about:
* https://math.stackexchange.com/questions/1322206/lie-groups-lie-algebra-applications
* https://mathoverflow.net/questions/58696/why-study-lie-algebras
* https://math.stackexchange.com/questions/405406/definition-of-lie-algebra

Oh, there is a low dimensional classification! Ciro is \x[high-flying-bird-vs-gophers][a sucker for classification theorems]! https://en.wikipedia.org/wiki/Classification_of_low-dimensional_real_Lie_algebras

The fact that there are elements arbitrarily close to the identity, which is only possible due to the group being continuous, is the key factor that simplifies the treatment of Lie groups, and follows the philosophy of \x[continuous-problems-are-simpler-than-discrete-ones].

Bibliography:
* https://youtu.be/kpeP3ioiHcw?t=2655 "Particle Physics Topic 6: Lie Groups and Lie Algebras" by Alex Flournoy (2016). Good \x[special-orthogonal-group][SO(3)] explicit exponential expansion example. Then next lecture shows why SU(2) is the representation of SO(3). Next ones appear to eventually get to the physical usefulness of the thing, but I lost patience. Not too far out though.
* https://www.youtube.com/playlist?list=PLRlVmXqzHjURZO0fviJuyikvKlGS6rXrb "Lie Groups and Lie Algebras" playlist by XylyXylyX (2018). Tutorial with infinitely many hours
* http://www.staff.science.uu.nl/~hooft101/lectures/lieg07.pdf
* http://www.physics.drexel.edu/~bob/LieGroups.html

= Applications of Lie groups to differential equations
{parent=lie-group}

= How to use Lie Groups to solve differential equations
{title2}
{synonym}

Solving \x[differential-equation]{p} was apparently Lie's original motivation for developing \x[lie-group]{p}. It is therefore likely one of the most understandable ways to approach it.

It appears that Lie's goal was to understand when can a differential equation have an explicitly written solution, much like \x[galois-theory] had done for \x[algebraic-equation]{p}. Both approaches use \x[symmetry] as the key tool.

* https://www.researchgate.net/profile/Michael_Frewer/publication/269465435_Lie-Groups_as_a_Tool_for_Solving_Differential_Equations/links/548cbf250cf214269f20e267/Lie-Groups-as-a-Tool-for-Solving-Differential-Equations.pdf Lie-Groups as a Tool for Solving Differential Equations by Michael Frewer. Slides with good examples.

= Lie algebra
{c}
{parent=lie-group}
{wiki}

Like everything else in \x[lie-group]{p}, first start with the \x[matrix] as discussed at \x[lie-algebra-of-a-matrix-lie-group]{full}.

Intuitively, a \x[lie-algebra] is a simpler object than a \x[lie-group]. Without any extra structure, groups can be very complicated non-linear objects. But a \x[lie-algebra] is just an \x[algebra-over-a-field], and one with a restricted \x[bilinear-map] called the \x[lie-bracket], that has to also be \x[alternating-multilinear-map][alternating] and satisfy the \x[jacobi-identity].

Another important way to think about Lie algebras, is as \x[infinitesimal-generator]{p}.

Because of the \x[lie-group-lie-algebra-correspondence], we know that there is almost a \x[bijection] between each \x[lie-group] and the corresponding \x[lie-algebra]. So it makes sense to try and study the algebra instead of the group itself whenever possible, to try and get insight and proofs in that simpler framework. This is the key reason why people study Lie algebras. One is philosophically reminded of how \x[normal-subgroup]{p} are a simpler representation of \x[group-homomorphism]{p}.

To make things even simpler, because \x[all-vector-spaces-of-the-same-dimension-on-a-given-field-are-isomorphic], the only things we need to specify a \x[lie-group] through a \x[lie-algebra] are:
* the dimension
* the \x[lie-bracket]
Note that the \x[lie-bracket] can look different under different basis of the \x[lie-algebra] however. This is shown for example at \x[physics-from-symmetry-by-jakob-schwichtenberg-2015] page 71 for the \x[lorentz-group].

As mentioned at \x[lie-groups-physics-and-geometry-by-robert-gilmore-2008] Chapter 4 "Lie Algebras", taking the \x[lie-algebra] around the identity is mostly a convention, we could treat any other point, and things are more or less equivalent.

Bibliography:
* https://physicstravelguide.com/advanced_tools/group_theory/lie_algebras#tab__concrete on \x[physics-travel-guide]
* http://jakobschwichtenberg.com/lie-algebra-able-describe-group/ by \x[jakob-schwichtenberg]

= Infinitesimal generator
{parent=lie-algebra}

Elements of a \x[lie-algebra] can (should!) be seen a continuous analogue to the \x[generating-set-of-a-group] in finite groups.

For continuous groups however, we can't have a finite generating set in the strict sense, as a finite set won't ever cover every possible point.

But the \x[generator-of-a-lie-algebra] can be finite.

And just like in finite groups, where you can specify the full group by specifying only the relationships between generating elements, in the Lie algebra you can almost specify the full group by specifying the relationships between the elements of a \x[generator-of-the-lie-algebra].

This "specification of a relation" is done by defining the \x[lie-bracket].

The reason why the algebra works out well for continuous stuff is that by definition an \x[algebra-over-a-field] is a \x[vector-space] with some extra structure, and we know very well how to make infinitesimal elements in a vector space: just multiply its vectors by a constant $c$ that cana be arbitrarily small.

= Lie group-Lie algebra correspondence
{c}
{parent=lie-algebra}
{wiki=Lie_group–Lie_algebra_correspondence}

Every \x[lie-algebra] corresponds to a single \x[simply-connected] \x[lie-group].

The \x[baker-campbell-hausdorff-formula] basically defines how to map an algebra to the group.

Bibliography:
* \x[lie-groups-physics-and-geometry-by-robert-gilmore-2008] Chapter 7 "EXPonentiation"

= Lie algebra exponential covering problem
{c}
{parent=lie-group-lie-algebra-correspondence}

\x[lie-groups-physics-and-geometry-by-robert-gilmore-2008] 7.2 "The covering problem" gives some amazing intuition on the subject as usual.

= A single exponential map is not enough to recover a simple Lie group from its algebra
{parent=lie-algebra-exponential-covering-problem}

Example at: \x[lie-groups-physics-and-geometry-by-robert-gilmore-2008] Chapter 7 "EXPonentiation".

= The product of a exponential of the compact algebra with that of the non-compact algebra recovers a simple Lie from its algebra
{parent=lie-algebra-exponential-covering-problem}

Example at: \x[lie-groups-physics-and-geometry-by-robert-gilmore-2008] Chapter 7 "EXPonentiation".

Furthermore, the non-\x[compact] part is always \x[isomorphic] to \x[r-n], only the non-compact part can have more interesting structure.

= Two different Lie groups can have the same Lie algebra
{parent=lie-group-lie-algebra-correspondence}

The most important example is perhaps \x[so-3] and \x[su-2], both of which have the same \x[lie-algebra], but are not isomorphic.

= Every Lie algebra has a unique single corresponding simply connected Lie group
{parent=two-different-lie-groups-can-have-the-same-lie-algebra}

This \x[simply-connected] is called the \x[universal-covering-group].

E.g. in the case of \x[so-3] and \x[su-2], \x[su-2] is \x[simply-connected], but \x[so-3] is not.

= Universal covering group
{parent=every-lie-algebra-has-a-unique-single-corresponding-simply-connected-lie-group}

The \x[unique] group referred to at: \x[every-lie-algebra-has-a-unique-single-corresponding-simply-connected-lie-group].

= Every Lie group that has a given Lie algebra is the image of an homomorphism from the universal cover group
{parent=two-different-lie-groups-can-have-the-same-lie-algebra}

= Lie bracket
{c}
{parent=lie-algebra}

= Exponential map
{parent=lie-algebra}
{wiki}

Most commonly refers to: \x[exponential-map-lie-theory].

= Exponential map
{disambiguate=Lie theory}
{parent=exponential-map}
{wiki}

Like everything else in \x[lie-group] theory, you should first look at the \x[matrix] version of this operation: the \x[matrix-exponential].

The \x[exponential-map] links small transformations around the origin (infinitely small) back to larger finite transformations, and small transformations around the origin are something we can deal with a \x[lie-algebra], so this map links the two worlds.

The idea is that we can decompose a finite transformation into infinitely arbitrarily small around the origin, and proceed just like the \x[product-definition-of-the-exponential-function].

The definition of the exponential map is simply the same as that of the regular exponential function as given at \x[taylor-expansion-definition-of-the-exponential-function], except that the argument $x$ can now be an operator instead of just a number.

Examples:
* \x[the-derivative-is-the-generator-of-the-translation-group]

= Baker-Campbell-Hausdorff formula
{c}
{parent=lie-algebra}
{title2=BCH formula}
{wiki=Baker–Campbell–Hausdorff formula}

Solution $Z$ for given $X$ and $Y$ of:
$$
e^Z = e^X e^Y
$$
where $e$ is the \x[exponential-map].

If we consider just \x[real-number], $Z = X + Y$, but when X and Y are \x[non-commutative], things are not so simple.

Furthermore, TODO confirm it is possible that a solution does not exist at all if $X$ and $Y$ aren't sufficiently small.

This formula is likely the basis for the \x[lie-group-lie-algebra-correspondence]. With it, we express the actual \x[group-operation] in terms of the Lie algebra operations.

Notably, remember that a \x[algebra-over-a-field] is just a \x[vector-space] with one extra product operation defined.

Vector spaces are simple because \x[all-vector-spaces-of-the-same-dimension-on-a-given-field-are-isomorphic], so besides the dimension, once we define a \x[lie-bracket], we also define the corresponding \x[lie-group].

Since a group is basically defined by what the group operation does to two arbitrary elements, once we have that defined via the \x[baker-campbell-hausdorff-formula], we are basically done defining the group in terms of the algebra.

= Generator of a Lie algebra
{parent=lie-algebra}

= Generators of a Lie algebra
{parent=lie-algebra}

= Generator of the Lie algebra
{synonym}

Cardinality $\leq$ dimension of the vector space.

= Continuous symmetry
{parent=lie-group}
{wiki}

Basically a synonym for \x[lie-group] which is the way of modelling them.

= Local symmetry
{parent=continuous-symmetry}
{wiki}

Local symmetries appear to be a synonym to \x[internal-symmetry], see description at: \x[internal-and-spacetime-symmetries]{full}.

As mentioned at \x[quote-axelmaas-local-symmetry], local symmetries map to forces in the \x[standard-model].

Appears to be a synonym for: \x[gauge-symmetry].

A local symmetry is a transformation that you apply a different transformation for each point, instead of a single transformation for every point.

TODO what's the point of a local symmetry?

Bibliography:
* \x[quantum-field-theory-lecture-by-tobias-osborne-2017/lecture-3]
* https://physics.stackexchange.com/questions/48188/local-and-global-symmetries
* https://www.physics.rutgers.edu/grad/618/lects/localsym.pdf by Joel Shapiro gives one nice high level intuitive idea:
  \Q[In relativistic physics, global objects are awkward because the finite velocity with which effects can propagate is expressed naturally in terms of local objects. For this reason high energy physics is expressed in terms of a field theory.]
* \x[quora]:
  * https://www.quora.com/What-does-a-local-symmetry-mean-in-physics
  * https://www.quora.com/What-is-the-difference-between-local-and-global-symmetries-in-physics
  * https://www.quora.com/What-is-the-difference-between-global-and-local-gauge-symmetry

= Local symmetries of the Lagrangian imply conserved currents
{parent=local-symmetry}

TODO. I think this is the key point. Notably, \x[u-1] symmetry implies \x[charge-conservation].

More precisely, each \x[generator-of-a-lie-algebra][generator of the corresponding Lie algebra] leads to one separate conserved current, such that a single symmetry can lead to multiple conserved currents.

This is basically the \x[local-symmetry] version of \x[noether-s-theorem].

Then to maintain charge conservation, we have to maintain \x[local-symmetry], which in turn means we have to add a \x[gauge-field] as shown at \x[video-deriving-the-qed-lagrangian-by-dietterich-labs-2018].

Forces can then be seen as kind of a side effect of this.

Bibliography:
* https://photonics101.com/relativistic-electrodynamics/gauge-invariance-action-charge-conservation#show-solution has a good explanation of the Gauge transformation. TODO how does that relate to \x[u-1] symmetry?
* https://physics.stackexchange.com/questions/57901/noether-theorem-gauge-symmetry-and-conservation-of-charge

= Important Lie group
{parent=lie-group}

= Important Lie groups
{synonym}

= Matrix Lie group
{parent=important-lie-group}

This important and common simple case has easy properties.

= Every closed subgroup of $GL(n, \C)$ is a Lie group
{parent=matrix-lie-group}

\x[an-introduction-to-tensors-and-group-theory-for-physicists-by-nadir-jeevanjee-2011]{p} page 146.

= Lie algebra of a matrix Lie group
{c}
{parent=matrix-lie-group}

For this sub-case, we can define the \x[lie-algebra] of a Lie group $G$ as the set of all matrices $M \in G$ such that for all $t \in \R$:
$$
e^{tM} \in G
$$
If we fix a given $M$ and vary $t$, we obtain a \x[subgroup] of $G$. This type of subgroup is known as a \x[one-parameter-subgroup].

The immediate question is then if every element of $G$ can be reached in a unique way (i.e. is the exponential map a \x[bijection]). By looking at the \x[matrix-logarithm] however we conclude that this is not the case for \x[real] matrices, but it is for \x[complex] matrices.

Examples:
* \x[lie-algebra-of-gl-n]{child}
* \x[lie-algebra-of-sl-2]{child}
* \x[lie-algebra-of-so-3]{child}
* \x[lie-algebra-of-su-2]{child}

TODO example it can be seen that the Lie algebra is not closed \x[matrix-multiplication], even though the corresponding group is by definition. But it is closed under the \x[lie-bracket] operation.

= Lie bracket of a matrix Lie group
{c}
{parent=lie-algebra-of-a-matrix-lie-group}

$$[X, Y] = XY - YX$$

This makes it clear how the \x[lie-bracket] can be seen as a "measure of non-\x[commutativity]"

Because the \x[lie-bracket] has to be a bilinear map, all we need to do to specify it uniquely is to specify how it acts on every pair of some basis of the \x[lie-algebra].

Then, together with the \x[baker-campbell-hausdorff-formula] and the \x[lie-group-lie-algebra-correspondence], this forms an exceptionally compact description of a \x[lie-group].

= One parameter subgroup
{parent=lie-algebra-of-a-matrix-lie-group}

The one parameter subgroup of a \x[lie-group] for a given element $M$ of its \x[lie-algebra] is a \x[subgroup] of $G$ given by:
$$
{ e^{tM} \in G | t \in \R }
$$

Intuitively, $M$ is a direction, and $t$ is how far we move along a given direction. This intuition is especially vivid in for example in the case of the \x[lie-algebra-of-so-3], the \x[rotation-group].

One parameter subgroups can be seen as the continuous analogue to the \x[cycle-of-an-element-of-a-group].

= Classical group
{parent=important-lie-group}
{wiki}

= Symplectic group
{parent=classical-group}
{title2=$Sp(n, F)$}

Intuition, please? Example? https://mathoverflow.net/questions/278641/intuition-for-symplectic-groups The key motivation seems to be related to \x[hamiltonian-mechanics]. The two arguments of the \x[bilinear-form] correspond to each set of variables in Hamiltonian mechanics: the generalized positions and generalized momentums, which appear in the same number each.

Seems to be set of matrices that preserve a \x[skew-symmetric-bilinear-form], which is comparable to the \x[orthogonal-group], which preserves a \x[symmetric-bilinear-form]. More precisely, the orthogonal group has:
$$
O^T I O = I
$$
and its generalization the \x[indefinite-orthogonal-group] has:
$$
O^T S O = I
$$
where S is symmetric. So for the symplectic group we have matrices Y such as:
$$
Y^T A Y = I
$$
where A is antisymmetric. This is explained at: https://www.ucl.ac.uk/~ucahad0/7302_handout_13.pdf They also explain there that unlike as in the analogous \x[orthogonal-group], that definition ends up excluding determinant -1 automatically.

Therefore, just like the \x[special-orthogonal-group], the symplectic group is also a \x[subgroup] of the \x[special-linear-group].

= Symplectic matrix
{parent=symplectic-group}
{tag=named-matrix}

= Unitary symplectic group
{parent=symplectic-group}
{title2=$Sp(n)$}

= General linear group
{parent=important-lie-group}
{wiki}

= $GL(n)$
{synonym}
{title2}

= $GL(n, F)$
{synonym}
{title2}

Invertible matrices. Or if you think a bit more generally, an invertible \x[linear-map].

When the \x[field-mathematics] $F$ is not given, it defaults to the \x[real-number]{p}.

Non-invertible are excluded "because" otherwise it would not form a \x[group-mathematics] (every element must have an inverse). This is therefore the largest possible group under \x[matrix-multiplication], other matrix multiplication groups being subgroups of it.

= Finite general linear group
{parent=general-linear-group}
{title2=$GL(n, F_m)$}

= $GL(n, m)$
{synonym}
{title2}

\x[general-linear-group] over a \x[finite-field] of order $m$. Remember that due to the \x[classification-of-finite-fields], there is one single field for each \x[prime-power] $m$.

Exactly as over the \x[real-number]{p}, you just put the finite field elements into a $n \times n$ matrix, and then take the invertible ones.

= Lie algebra of $GL(n)$
{c}
{parent=important-lie-group}

Is the \x[set-of-all-n-by-y-square-matrices].

Because \x[gl-n] is a \x[lie-group] we can use \x[lie-algebra-of-a-matrix-lie-group]{full}.

For every matrix $x$ in the \x[set-of-all-n-by-y-square-matrices] $M_n$, $e^x$ has inverse $e^-x$.

Note that this works even if $x$ is not \x[invertible], and therefore not in \x[gl-n]!

Therefore, the Lie algebra of \x[gl-n] is the entire \x[m-n].

= Special linear group
{parent=important-lie-group}
{title2=$SL(n)$}
{wiki}

Specials sub case of the \x[general-linear-group] when the determinant equals exactly 1.

= Special linear group of dimension 2
{parent=special-linear-group}
{title2=$SL(2)$}
{wiki=SL2(R)}

= Lie algebra of $SL(n)$
{c}
{parent=special-linear-group}

= Lie algebra of the special linear group
{c}
{synonym}

= Lie algebra of $SL(2)$
{c}
{parent=lie-algebra-of-sl-n}

= Lie algebra of the special linear group of degree 2
{c}
{synonym}

This is a good first concrete example of a Lie algebra. Shown at \x[lie-groups-physics-and-geometry-by-robert-gilmore-2008] Chapter 4.2 "How to linearize a Lie Group" has an example.

We can use use the following parametrization of the \x[special-linear-group] on variables $x$, $y$ and $z$:
$$
M =
\begin{bmatrix}
1 + x & y \\
z & (1 + yz)/(1 + x) \\
\end{bmatrix}
$$

Every element with this parametrization has \x[determinant] 1:
$$
det(M) = (1 + x)(1 + yz)/(1 + x) - yz = 1
$$
Furthermore, any element can be reached, because by independently settting $x$, $y$ and $z$, $M_{00}$, $M_{01}$ and $M_{10}$ can have any value, and once those three are set, $M_{11}$ is fixed by the determinant.

To find the elements of the \x[lie-algebra], we evaluate the derivative on each parameter at 0:
$$
\begin{aligned}
M_x
&=
\evalat{\dv{M}{x}}{(x,y,z) = (0,0,0)}
&=
\evalat{
\begin{bmatrix}
1 & 0 \\
0 & -(1 + yz)/(1 + x)^2 \\
\end{bmatrix}
}{(x,y,z) = (0,0,0)}
&=
\begin{bmatrix}
1 & 0 \\
0 & -1 \\
\end{bmatrix}
\\

M_y
&=
\evalat{\dv{M}{y}}{(x,y,z) = (0,0,0)}
&=
\evalat{
\begin{bmatrix}
0 & 1 \\
0 & z/(1 + x) \\
\end{bmatrix}
}{(x,y,z) = (0,0,0)}
&=
\begin{bmatrix}
0 & 1 \\
0 & 0 \\
\end{bmatrix}
\\

M_z
&=
\evalat{\dv{M}{z}}{(x,y,z) = (0,0,0)}
&=
\evalat{
\begin{bmatrix}
0 & 0 \\
1 & y/(1 + x) \\
\end{bmatrix}
}{(x,y,z) = (0,0,0)}
&=
\begin{bmatrix}
0 & 0 \\
1 & 0 \\
\end{bmatrix}
\\

\end{aligned}
$$

Remembering that the \x[lie-bracket-of-a-matrix-lie-group] is really simple, we can then observe the following \x[lie-bracket] relations between them:
$$
\begin{aligned}
[M_x, M_y] &= M_xM_y - M_yM_x &= \begin{bmatrix}0 & 1 \\  0 & 0 \\\end{bmatrix} &- \begin{bmatrix}0 & -1 \\ 0 & 0 \\\end{bmatrix} &= \begin{bmatrix}0 & 2 \\  0 &  0 \\\end{bmatrix} &=  2M_y\\
[M_x, M_z] &= M_xM_z - M_zM_x &= \begin{bmatrix}0 & 0 \\ -1 & 0 \\\end{bmatrix} &- \begin{bmatrix}0 &  0 \\ 1 & 0 \\\end{bmatrix} &= \begin{bmatrix}0 & 0 \\ -2 &  0 \\\end{bmatrix} &= -2M_z\\
[M_y, M_z] &= M_yM_z - M_zM_y &= \begin{bmatrix}1 & 0 \\  0 & 0 \\\end{bmatrix} &- \begin{bmatrix}0 &  0 \\ 0 & 1 \\\end{bmatrix} &= \begin{bmatrix}1 & 0 \\  0 & -1 \\\end{bmatrix} &=   M_x\\
\end{aligned}
$$

One key thing to note is that the specific matrices $M_x$, $M_y$ and $M_z$ are not really fundamental: we could easily have had different matrices if we had chosen any other parametrization of the group.

TODO confirm: however, no matter which parametrization we choose, the \x[lie-bracket] relations between the three elements would always be the same, since it is the number of elements, and the definition of the \x[lie-bracket], that is truly fundamental.

\x[lie-groups-physics-and-geometry-by-robert-gilmore-2008] Chapter 4.2 "How to linearize a Lie Group" then calculates the \x[exponential-map] of the vector $xM_x + yM_y + zM_z$ as:
$$
I cosh(\theta) + M_x sinh(\theta)/\theta
$$
with:
$$
\theta^2 = x^2 + bc
$$

TODO now the natural question is: can we cover the entire Lie group with this exponential? \x[lie-groups-physics-and-geometry-by-robert-gilmore-2008] Chapter 7 "EXPonentiation" explains why not.

= Finite special general linear group
{parent=special-linear-group}

= $SL(n, m)$
{synonym}
{title2}

Just like for the \x[finite-general-linear-group], the definition of special also works for finite fields, where 1 is the multiplicative identity!

Note that the definition of \x[orthogonal-group] may not have such a clear finite analogue on the other hand.

= Isometry group
{parent=important-lie-group}
{wiki}

The \x[group-mathematics] of all transformations that preserve some \x[bilinear-form], notable examples:
* \x[orthogonal-group]{child} preserves the \x[inner-product]
* \x[unitary-group]{child} preserves a \x[hermitian-form]
* \x[lorentz-group]{child} preserves the \x[minkowski-inner-product]

= Lie algebra of a isometry group
{c}
{parent=isometry-group}
{wiki}

We can almost reach the \x[lie-algebra] of any \x[isometry-group] in a single go. For every $X$ in the \x[lie-algebra] we must have:
$$
\forall v, w \in V, t \in \R (e^{tX}v|e^{tX}w) = (v|w)
$$
because $e^{tX}$ has to be in the isometry group by definition as shown at \x[lie-algebra-of-a-matrix-lie-group]{full}.

Then:
$$
\evalat{\dv{(e^{tX}v|e^{tX}w)}{t}}{0} = 0
\implies
\evalat{(Xe^{tX}v|e^{tX}w) + (e^{tX}v|Xe^{tX}w)}{0} = 0
\implies
(Xv|w) + (v|Xw) = 0
$$
so we reach:
$$
\forall v, w \in V (Xv|w) = -(v|Xw)
$$
With this relation, we can easily determine the \x[lie-algebra] of common isometries:
* \x[lie-algebra-of-o-n]

Bibliography:
* \x[an-introduction-to-tensors-and-group-theory-for-physicists-by-nadir-jeevanjee-2011] page 151

= Orthogonal group
{parent=important-lie-group}
{wiki}

= $O(n)$
{synonym}
{title2}

= Definition of the orthogonal group
{parent=orthogonal-group}

Intuitive definition: real group of rotations + reflections.

Mathematical definition that most directly represents this: \x[the-orthogonal-group-is-the-group-of-all-matrices-that-preserve-the-dot-product].

= The orthogonal group is the group of all matrices that preserve the dot product
{parent=definition-of-the-orthogonal-group}

When viewed as matrices, it is the group of all matrices that preserve the \x[dot-product], i.e.:
$$
O(n) = { O \in M(n) | \forall x, y, x^Ty = (Ox)^T (Oy) }
$$
This implies that it also preserves important geometric notions such as \x[norm-mathematics] (intuitively: distance between two points) and \x[angle]{p}.

This is perhaps the best "default definition".

= What happens to the definition of the orthogonal group if we choose other types of symmetric bilinear forms
{parent=the-orthogonal-group-is-the-group-of-all-matrices-that-preserve-the-dot-product}

We looking at the definition \x[the-orthogonal-group-is-the-group-of-all-matrices-that-preserve-the-dot-product], we notice that the \x[dot-product] is one example of \x[positive-definite-symmetric-bilinear-form], which in turn can also be represented by a matrix as shown at: \x[matrix-representation-of-a-symmetric-bilinear-form]{full}.

By looking at this more general point of view, we could ask ourselves what happens to the group if instead of the \x[dot-product] we took a more general \x[bilinear-form], e.g.:
* $I_2$: another \x[positive-definite-symmetric-bilinear-form] such as $(x_1, x_2)^T(y_1, y_2) = 2 x_1 y_1 + x_2 y_2$?
* $I_-$ what if we drop the \x[positive-definite] requirement, e.g. $(x_1, x_2)^T(y_1, y_2) = - x_1 y_1 + x_2 y_2$?
The answers to those questions are given by the \x[sylvester-s-law-of-inertia] at \x[all-indefinite-orthogonal-groups-of-matrices-of-equal-metric-signature-are-isomorphic]{full}.

= The orthogonal group is the group of all invertible matrices where the inverse is equal to the transpose
{parent=definition-of-the-orthogonal-group}

Let's show that this definition is equivalent to \x[the-orthogonal-group-is-the-group-of-all-matrices-that-preserve-the-dot-product].

Note that:
$$
x^Ty = (Ox)^T (Oy) = x^T O^T O y
$$
and for that to be true for all possible $x$ and $y$ then we must have:
$$
O^T O = I
$$
i.e. the \x[matrix-inverse] is equal to the \x[transpose].

Conversely, if:
$$
O^T O = I
$$
is true, then
$$
(Ox)^T (Oy) = x^T (O^T O) y = x^Ty
$$

These matricese are called the \x[orthogonal-matrix]{p}.

TODO is there any more intuitive way to think about this?

= Elements of the orthogonal group have determinant plus or minus one
{parent=the-orthogonal-group-is-the-group-of-all-invertible-matrices-where-the-inverse-is-equal-to-the-transpose}

\x[the-orthogonal-group-is-the-group-of-all-invertible-matrices-where-the-inverse-is-equal-to-the-transpose]

= The orthogonal group is the group of all matrices with orthonormal rows and orthonormal columns
{parent=definition-of-the-orthogonal-group}

Or equivalently, the set of rows is \x[orthonormal], and so is the set of columns. TODO proof that it is equivalent to \x[the-orthogonal-group-is-the-group-of-all-matrices-that-preserve-the-dot-product].

= Topology of the orthogonal group
{parent=orthogonal-group}

= The orthogonal group is compact
{parent=topology-of-the-orthogonal-group}

= Connected components of the orthogonal group
{parent=topology-of-the-orthogonal-group}

= The orthogonal group has two connected components
{synonym}
{title2}

The \x[orthogonal-group] has 2 \x[connected-component]{p}:
* one with determinant +1, which is is itself a \x[subgroup] known as the \x[special-orthogonal-group]. These are pure \x[rotation]{p} without a reflection.
* the other with determinant -1. This is not a \x[subgroup] as which is not contain the origin. It represents \x[rotation]{p} with a reflection.

It is instructive to visualize how the $\pm1$ looks like in \x[so-3]:
* you take the first basis vector and move it to any other. You have therefore two angular parameters.
* you take the second one, and move it to be orthogonal to the first new vector. (you can choose a circle around the first new vector, and so you have another angular parameter.
* at last, for the last one, there are only two choices that are orthogonal to both previous ones, one in each direction. It is this directio, relative to the others, that determines the "has a reflection or not" thing

As a result it is \x[group-isomorphism][isomorphic] to the \x[direct-product-of-groups][direct product] of the special orthogonal group by the \x[cyclic-group] of \x[order-algebra] 2:
$$
O(n) \cong SO(n) \times C_2
$$

A low dimensional example:
$$
O(1) \cong SO(2) \times C_2
$$
because you can only do two things: to flip or not to flip the line around zero.

Note that having the determinant plus or minus 1 is not a definition: there are non-orthogonal groups with determinant plus or minus 1. This is just a property. E.g.:
$$
M = \begin{bmatrix} 2 & 3 \\ 1 & 2 \\ \end{bmatrix}
$$
has determinant 1, but:
$$
M^TM = \begin{bmatrix} 5 & 8 \\ 8 & 11 \\ \end{bmatrix}
$$
so $M$ is not orthogonal.

= Lie algebra of $O(n)$
{c}
{parent=orthogonal-group}

From \x[lie-algebra-of-a-isometry-group]{full} we reach:

= Special orthogonal group
{parent=orthogonal-group}

= $SO(n)$
{synonym}
{title2}

= Rotation group
{synonym}
{title2}

= Rotation
{synonym}

Group of rotations of a rigid body.

Like \x[orthogonal-group] but without reflections. So it is a "special case" of the orthogonal group.

This is a subgroup of both the \x[orthogonal-group] and the \x[special-linear-group].

= Lie algebra of $SO(3)$
{c}
{parent=special-orthogonal-group}

We can reach it by taking the rotations in three directions, e.g. a rotation around the z axis:
$$
R_z(\theta)
=
\begin{bmatrix}
cos(\theta) & -sin(\theta) & 0 \\
sin(\theta) & cos(\theta) & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
$$
then we derive and evaluate at 0:
$$
L_z
=
\evalat{\dv{R_z(\theta)}{\theta}}{0}
=
\evalat{\begin{bmatrix}
-sin(\theta) & -cos(\theta) & 0 \\
cos(\theta) & -sin(\theta) & 0 \\
0 & 0 & 1 \\
\end{bmatrix}}{0}
=
\begin{bmatrix}
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 0 \\
\end{bmatrix}
$$
$L_z$ therefore represents the infinitesimal rotation.

Note that the \x[exponential-map] reverses this and gives a finite rotation around the Z axis back from the \x[infinitesimal-generator] $L_z$:
$$
e^{\theta L_z} = R_z(\theta)
$$

Repeating the same process for the other directions gives:
$$
L_x = TODO
L_y = TODO
$$
We have now found 3 \x[linearly-independent] elements of the Lie algebra, and since $SO(3)$ has dimension 3, we are done.

= Lie bracket of the rotation group
{c}
{parent=lie-algebra-of-so-3}

Based on the $L_x$,$L_y$ and $L_z$ derived at \x[lie-algebra-of-so-3] we can calculate the \x[lie-bracket] as:
$$
TODO
$$

= 3D rotation group
{parent=special-orthogonal-group}
{wiki}

= Special orthogonal group of degree 3
{synonym}

= $SO(3)$
{synonym}
{title2}

Has \x[su-2] as a \x[double-cover].

= Unitary group
{parent=orthogonal-group}
{wiki}

= $U(n)$
{synonym}
{title2}

\x[group-mathematics]{c} of the \x[unitary-matrix]{p}.

\x[complex-number][Complex] analogue of the \x[orthogonal-group].

One notable difference from the orthogonal group however is that the unitary group is connected "because" its determinant is not fixed to two disconnected values 1/-1, but rather goes around in a continuous \x[unit-circle]. $U(1)$ \i[is] the unit circle.

= Unitary group of degree 1
{parent=unitary-group}

= $U(1)$
{synonym}
{title2}

= Unitary group of degree 2
{parent=unitary-group}

= $U(2)$
{synonym}
{title2}

Diffeomorphic to the 3 sphere.

= Unit circle
{parent=unitary-group}
{wiki}

The $U(1)$ \x[unitary-group] is one very over-generalized way of looking at it :-)

= Special unitary group
{parent=unitary-group}
{wiki}

= $SU(n)$
{synonym}
{title2}

The complex analogue of the \x[special-orthogonal-group], i.e. the subgroup of the \x[unitary-group] with determinant equals exactly 1 instead of an arbitrary complex number with absolute value equal 1 as is the case for the unitary group.

= Special unitary of degree 2
{parent=special-unitary-group}
{wiki}

= $SU(2)$
{synonym}
{title2}

https://en.wikipedia.org/wiki/Representation_theory_of_SU(2)

\x[double-cover]{c} of \x[so-3].

\x[isomorphic]{c} to the \x[quaternion]{p}.

= Representations of $SU(2)$
{parent=special-unitary-of-degree-2}

= Lie algebra of $SU(2)$
{c}
{parent=representations-of-su-2}

Bibliography:
* \x[physics-from-symmetry-by-jakob-schwichtenberg-2015] page 54.

= 2D representation of $SU(2)$
{parent=representations-of-su-2}

\x[pauli-matrix].

= Projective linear group
{parent=important-lie-group}
{wiki}

TODO motivation. Motivation. Motivation. Motivation. The definitin with \x[quotient-group] is easy to understand.

= Finite projective linear group
{parent=projective-linear-group}
{wiki=Projective_linear_group#Finite_fields}

= $PGL(q, p)$
{title2}
{synonym}

= Projective special linear group
{parent=projective-linear-group}

= Finite projective special linear group
{parent=projective-special-linear-group}

= $PSL(p, q)$
{synonym}
{title2}

= $PSL(2, p)$
{parent=finite-projective-special-linear-group}

= PSL(2,7)
{parent=psl-2-p}
{wiki}

The second smallest non-\x[abelian] finite \x[simple-group] after the \x[alternating-group-of-degree-5].

= Poincaré group
{c}
{parent=important-lie-group}
{wiki}

= Poincaré transformation
{c}
{synonym}

Full set of all possible \x[special-relativity] symmetries:
* translations in space and time
* rotations in space
* \x[lorentz-boost]{p}

In simple and concrete terms. Suppose you observe N particles following different trajectories in \x[spacetime].

There are two observers traveling at constant speed relative to each other, and so they see different trajectories for those particles:
* space and time shifts, because their space origin and time origin (time they consider 0, i.e. when they started their timers) are not synchronized. This can be modelled with a 4-vector addition.
* their space axes are rotated relative to one another. This can be modelled with a 4x4 matrix multiplication.
* and they are moving relative to each other, which leads to the usual spacetime interactions of \x[special-relativity]. Also modelled with a 4x4 matrix multiplication.
Note that the first two types of transformation are exactly the non-relativistic \x[galilean-transformation]{p}.

The Poincare group is the set of all matrices such that such a relationship like this exists between two frames of reference.

= Galilean transformation
{c}
{parent=poincare-group}
{wiki}

= Translation
{c}
{disambiguate=geometry}
{parent=galilean-transformation}
{wiki}

Subset of \x[galilean-transformation] with speed equals 0.

= Translation group
{parent=translation-geometry}

This is a good and simple first example of \x[lie-algebra] to look into.

= The derivative is the generator of the translation group
{parent=translation-group}

Take the group of all \x[translation-geometry]{p} in \x[r-1].

Let's see how the \x[generator-of-a-lie-algebra][generator] of this group is the \x[derivative] \x[operator]:
$$
\pdv{}{x}
$$

The way to think about this is:
* the translation group operates on the argument of a function $f(x)$
* the generator is an \x[operator] that operates on $f$ itself

So let's take the \x[exponential-map-lie-theory]:
$$
e^{x_0\pdv{}{x}}f(x) = \left( 1 + x_0 \pdv{}{x} + x_0^2 \pdv{^2}{x^2} + \ldots\right)f(x)
$$
and we notice that this is exactly the \x[taylor-series] of $f(x)$ around the identity element of the translation group, which is 0! Therefore, if $f(x)$ behaves nicely enough, within some \x[radius-of-convergence] around the origin we have for finite $x_0$:
$$
e^{x_0\pdv{}{x}}f(x) = f(x + x_0)
$$

This example shows clearly how the \x[exponential-map-lie-theory] applied to a (differential) \x[operator] can generate finite (non-infinitesimal) \x[translation-geometry]{p}!

= Galilean invariance
{c}
{parent=galilean-transformation}
{wiki}

= Galilean invariant
{c}
{synonym}

A \x[law-of-physics] is Galilean invariant if the same formula works both when you are standing still on land, or when you are on a boat moving at constant velocity.

For example, if we were describing the movement of a \x[point-particle], the exact same formulas that predict the evolution of $x_{land}(t)$ must also predict $x_{boat}(t)$, even though of course both of those $x(t)$ will have different values. 

It would be extremely unsatisfactory if the formulas of the \x[laws-of-physics] did not obey \x[galilean-invariance]. Especially if you remember that \x[earth] is travelling extremelly fast relative to the \x[sun]. If there was no such invariance, that would mean for example that the \x[laws-of-physics] would be different in other \x[planet]{p} that are moving at different speeds. That would be a strong sign that our laws of physics are not complete.

The consequence/cause of that is that you cannot know if you are moving at a constant speed or not.

\x[lorentz-invariance] generalizes \x[galilean-invariance] to also account for \x[special-relativity], in which a more complicated invariant that also takes into account different times observed in different \x[inertial-frames-of-reference] is also taken into account. But the fundamental desire for the \x[lorentz-invariance] of the \x[laws-of-physics] remains the same.

= Covariance
{parent=galilean-invariance}
{wiki}

= Covariant
{synonym}

Generally means that he form of the equation $f(x)$ does not change if we transform $x$.

This is generally what we want from the laws of physics.

E.g. a \x[galilean-transformation] generally changes the exact values of coordinates, but not the form of the laws of physics themselves.

\x[lorentz-covariance] is the main context under which the word "covariant" appears, because we really don't want the form of the equations to change under \x[lorentz-transform]{p}, and "covariance" is often used as a synonym of "Lorentz covariance".

TODO some sources distinguish "invariant" from "covariant": \x[invariant-vs-covariant].

= Invariant vs covariant
{parent=covariance}

Some sources distinguish "invariant" from "covariant" such that under some transformation (typically \x[lie-group]):
* invariant: the value of $f(x)$ does not change if we transform $x$
* covariant: the form of the equation $f(x)$ does not change if we transform $x$.
TODO examples.

Bibliography:
* https://physics.stackexchange.com/questions/7700/definitions-and-usage-of-covariant-form-invariant-invariant
* https://physics.stackexchange.com/questions/270296/what-is-the-difference-between-lorentz-invariant-and-lorentz-covariant

= Lorentz group
{c}
{parent=poincare-group}
{wiki}

= $SO(1,3)$
{synonym}
{title2}

\x[subgroup]{c} of the \x[poincare-group] without translations. Therefore, in those, the spacetime origin is always fixed.

Or in other words, it is as if two observers had their space and time origins at the exact same place. However, their space axes may be rotated, and one may be at a relative speed to the other to create a \x[lorentz-boost]. Note however that if they are at relative speeds to one another, then their axes will immediately stop being at the same location in the next moment of time, so things are only valid infinitesimally in that case.

This group is made up of matrix multiplication alone, no need to add the offset vector: space rotations and \x[lorentz-boost] only spin around and bend things around the origin.

One definition: set of all 4x4 matrices that keep the \x[minkowski-inner-product], mentioned at \x[physics-from-symmetry-by-jakob-schwichtenberg-2015] page 63. This then implies:
$$
\Lambda ^ T \eta \Lambda = \eta
$$

= Representation theory of the Lorentz group
{c}
{parent=lorentz-group}
{wiki}

\x[physics-from-symmetry-by-jakob-schwichtenberg-2015] page 66 shows one in terms of 4x4 complex matrices.

More importantly though, are the representations of the \x[lie-algebra-of-the-lorentz-group], which are generally also just also called "Representation of the Lorentz group" since you can reach the representation from the algebra via the \x[exponential-map].

Bibliography:
* \x[physics-from-symmetry-by-jakob-schwichtenberg-2015] chapter 3.7 "The Lorentz Group O (1, 3)"

= Representation of the Lorentz group
{c}
{parent=representation-theory-of-the-lorentz-group}

= Representations of the Lorentz group
{synonym}

One of the representations of the \x[lorentz-group] that show up in the \x[representation-theory-of-the-lorentz-group].

= Lie algebra of the Lorentz group
{c}
{parent=representation-of-the-lorentz-group}

= Spinor
{parent=representation-of-the-lorentz-group}
{wiki}

TODO understand a bit more intuitively.

* \x[physics-from-symmetry-by-jakob-schwichtenberg-2015] page 72
* https://physics.stackexchange.com/questions/172385/what-is-a-spinor
* https://physics.stackexchange.com/questions/41211/what-is-the-difference-between-a-spinor-and-a-vector-or-a-tensor
* https://physics.stackexchange.com/questions/74682/introduction-to-spinors-in-physics-and-their-relation-to-representations
* http://www.weylmann.com/spinor.pdf

= Lorentz boost
{c}
{parent=lorentz-group}

Two observers travel at fixed speed relative to each other. They synchronize origins at x=0 and t=0, and their spacial axes are perfectly aligned. This is a subset of the \x[lorentz-group]. TODO confirm it does not form a subgroup however.

= Indefinite orthogonal group
{parent=lorentz-group}
{wiki}

= $O(m,n)$
{synonym}
{title2}

Generalization of \x[orthogonal-group] to preserve different \x[bilinear-form]{p}. Important because the \x[lorentz-group] is \x[so-1-3].

= Definition of the indefinite orthogonal group
{parent=indefinite-orthogonal-group}

Given a \x[matrix] $A$ with \x[metric-signature] containing $m$ positive and $n$ negative entries, the \x[indefinite-orthogonal-group] is the set of all matrices that preserve the \x[matrix-representation-of-a-bilinear-form][associated bilinear form], i.e.:
$$
O(m, n) = {O \in M(m + n) | \forall x, y x^T A y = (Ox)^T A (Oy)}
$$
Note that if $A = I$, we just have the standard \x[dot-product], and that subcase corresponds to the following definition of the \x[orthogonal-group]: \x[the-orthogonal-group-is-the-group-of-all-matrices-that-preserve-the-dot-product]{full}.

As shown at \x[all-indefinite-orthogonal-groups-of-matrices-of-equal-metric-signature-are-isomorphic], due to the \x[sylvester-s-law-of-inertia], only the metric signature of $A$ matters. E.g., if we take two different matrices with the same metric signature such as:
$$
\begin{bmatrix}
1 0
0 -1
\end{bmatrix}
$$
and:
$$
\begin{bmatrix}
2 0
0 -3
\end{bmatrix}
$$
both produce \x[isomorphic] spaces. So it is customary to just always pick the matrix with only +1 and -1 as entries.

= All indefinite orthogonal groups of matrices of equal metric signature are isomorphic
{parent=definition-of-the-indefinite-orthogonal-group}

Following the \x[definition-of-the-indefinite-orthogonal-group], we want to show that only the \x[metric-signature] matters.

First we can observe that the exact matrices are different. For example, taking the standard matrix of $O(2)$:
$$
\begin{bmatrix}
1 0
0 1
\end{bmatrix}
$$
and:
$$
\begin{bmatrix}
2 0
0 1
\end{bmatrix}
$$
both have the same \x[metric-signature]. However, we notice that a rotation of 90 degrees, which preserves the first form, does not preserve the second one! E.g. consider the vector $x = (1, 0)$, then $x \cdot x = 1$. But after a rotation of 90 degrees, it becomes $x_2 = (0, 1)$, and now $x_2 \cdot x_2 = 2$! Therefore, we have to search for an \x[isomorphism] between the two sets of matrices.

For example, consider the \x[orthogonal-group], which can be defined as shown at \x[the-orthogonal-group-is-the-group-of-all-matrices-that-preserve-the-dot-product] can be defined as:

= Indefinite special orthogonal group
{parent=indefinite-orthogonal-group}

= $SO(m,n)$
{synonym}
{title2}

Like the \x[special-orthogonal-group] is to the \x[orthogonal-group], \x[so-m-n] is the subset of \x[o-m-n] with \x[determinant] equal to exactly 1.

= Representation theory
{parent=lie-group}
{wiki}

= Representation
{disambiguate=group theory}
{synonym}

Basically, a "representation" means associating each group element as an invertible \x[matrix]{p}, i.e. a matrix in (possibly some subset of) \x[gl-n], that has the same properties as the group.

Or in other words, associating to the more abstract notion of a \x[group-mathematics] more concrete objects with which we are familiar (e.g. a matrix). 

Each such matrix then represents one specific element of the group.

This is basically what everyone does (or should do!) when starting to study \x[lie-group]{p}: we start looking at \x[matrix-lie-group]{p}, which are very concrete.

Or more precisely, mapping each group element to a \x[linear-map] over some \x[vector-field] $V$ (which can be represented by a matrix infinite dimension), in a way that respects the group operations:
$$R(g) : G \to GL(V)$$

As shown at \x[physics-from-symmetry-by-jakob-schwichtenberg-2015]
* page 51, a representation is not unique, we can even use matrices of different dimensions to represent the same group
* 3.6 classifies the \x[representations-of-su-2]. There is only one possibility per dimension!
* 3.7 "The Lorentz Group O(1,3)" mentions that even for a "simple" group such as the \x[lorentz-group], not all representations can be described in terms of matrices, and that we can construct such representations with the help of \x[lie-group] theory, and that they have fundamental physical application

Motivation:
* https://math.stackexchange.com/questions/1628464/what-is-representation-theory

Bibliography:
* https://www.youtube.com/watch?v=9rDzaKASMTM "RT1: Representation Theory Basics" by \x[mathdoctorbob] (2011). Too much theory, give me the motivation!

= Irreducible representation
{parent=representation-theory}
{wiki}

= Casimir element
{c}
{parent=irreducible-representation}
{wiki}

= Schur's lemma
{c}
{parent=representation-theory}
{wiki}

= Simple Lie group
{parent=lie-group}
{wiki}

= Classification of simple Lie groups
{parent=simple-lie-group}

https://en.wikipedia.org/wiki/Simple_Lie_group#List

A bit like the \x[classification-of-simple-finite-groups], they also have a few \x[sporadic-group]{p}! Not as spectacular since as usual \x[continuous-problems-are-simpler-than-discrete-ones], but still, not bad.

= Lie group bibliography
{parent=lie-group}

Recommended from \x[physics-from-symmetry-by-jakob-schwichtenberg-2015] page 92:
* \x[an-introduction-to-tensors-and-group-theory-for-physicists-by-nadir-jeevanjee-2011]
* \x[naive-lie-theory-by-john-stillwell-2008]
* \x[lie-algebras-in-particle-physics-by-howard-georgi-1999]

= An Introduction to Tensors and Group Theory for Physicists by Nadir Jeevanjee (2011)
{c}
{parent=lie-group-bibliography}

This does not seem to go deep into the \x[standard-model] as \x[physics-from-symmetry-by-jakob-schwichtenberg-2015], appears to focus more on more basic applications.

But because it is more basic, it does explain some things quite well.

= Lie Groups, Physics, and Geometry by Robert Gilmore (2008)
{c}
{parent=lie-group-bibliography}

The author seems to have uploaded the entire book by chapters at: https://www.physics.drexel.edu/~bob/LieGroups.html

And the author is the cutest: https://www.physics.drexel.edu/~bob/Personal.html[].

Overview:
* Chapter 3: gives a bunch of examples of important \x[matrix-lie-group]{p}. These are done by imposing certain types of constraints on the \x[general-linear-group], to obtain \x[subgroup]{p} of the general linear group. Feels like the start of a \x[classification-mathematics]
* Chapter 4: defines \x[lie-algebra]. Does some basic examples with them, but not much of deep interest, that is mostl left for Chapter 7
* Chapter 5: calculates the \x[lie-algebra] for all examples from chapter 3
* Chapter 6: don't know
* Chapter 7: describes how the \x[exponential-map] links \x[lie-algebra]{p} to \x[lie-group]{p}

= Naive Lie theory by John Stillwell (2008)
{c}
{parent=lie-group-bibliography}

= Lie Algebras In Particle Physics by Howard Georgi (1999)
{c}
{parent=lie-group-bibliography}

= Mathematician
{parent=mathematics}
{wiki}

Poet, scientists and warriors all in one? https://en.wikipedia.org/wiki/180_Degrees_South:_Conquerors_of_the_Useless[Conquerors of the useless].

A wise teacher from \x[university-of-sao-paulo] once told the class \x[ciro-santilli] attended an anecdote about his life:
\Q[
I used to want to learn Mathematics.

But it was very hard.

So in the end, I became an engineer, and found an engineering solution to the problem, and married a Mathematician instead.
]
It turned out that, about 10 years later, Ciro \x[ciro-santilli-s-wife][ended up following this advice], unwittingly.

\Image[https://web.archive.org/web/20190925220347if_/https://imgs.xkcd.com/comics/purity.png]
{disambiguate=mathematician}
{title=\x[xkcd] 435: Fields arranged by purity.}
{source=https://xkcd.com/435/}

= High flying bird vs gophers
{parent=mathematician}
{tag=essays-by-ciro-santilli}

= Birds and frogs by Freeman Dyson (2009)
{c}
{synonym}
{title2}

Ciro once read that there are two types of mathematicians/scientists (he thinks it was comparing Einstein to some Jack of all trades polymath who didn't do any new discoveries):
* high flying birds, who know a bit of everything, feel the beauty of each field, but never dig deep in any of them
* gophers, who dig all the way down, on a single subject, until they either get the \x[nobel-prize], or work on the wrong problem and waste their lives

TODO long after Ciro forgot where he had read this from originally, someone later pointed him to: https://www.ams.org/notices/200902/rtx090200212p.pdf Birds and Frogs by \x[freeman-dyson] (2009), which is analogous but about Birds and Frogs. So did \x[ciro-santilli-s-bad-old-event-memory][Ciro's memory play a trick on him], or is there also a variant; of this metaphor with a gopher?

\x[ciro-santilli-s-psychology-and-physiology][Ciro is without a doubt the bird type]. Perhaps the ultimate scientist is the one who can combine both aspects in the right amount?

Ciro gets bored of things very quickly.

Once he understands the general principles, if the thing is not \x[the-next-big-thing], Ciro considers himself satisfied without all the nitty gritty detail, and moves on to the next attempt.

In the field of \x[mathematics] for example, Ciro is generally content with understanding cool theorem statements. More generally, one of Ciro's desires is for example to understand the significance of each \x[physics] \x[nobel-prize].

This is also very clear for example after Ciro achieved \x[linux-kernel-module-cheat]: he now had the perfect setup to learn all the Linux kernel shady details but at the same time after all those years he finally felt that "he could do it, so that was enough", and soon moved to other projects.

If Ciro had become a scientist, he would write \x[the-side-effects-of-ambitious-goals-are-often-the-most-valuable-thing-achieved][the best review papers ever], just like in the current reality he \x[ciro-santilli-s-stack-overflow-contributions][writes amazing programming tutorials on Stack Overflow].

Ciro has in his mind an overly large list of subjects that "he feels he should know the basics of", and whenever he finds something in one of those topics that he does not know enough about, he uncontrollably learns it, even if it is not the most urgent thing to be done. Maybe everyone is like that. But Ciro feels that he feels this urge particularly strongly. Correspondingly, if a subject is not in that list, Ciro ignores it without thinking twice.

Ciro believes that high flying birds are the type of people better suited for \x[venture-capital] investment management: you know a bit of what is hot on several fields to enough depth to decide where to place your bets and how to guide them. But you don't have the patience to actually go deeply into any one of them and deal with each individual \x[shit] that comes up.

\x[cosmos-a-personal-voyage-1980] episode 1 mentions as quoted by the https://en.wikipedia.org/w/index.php?title=Eratosthenes&oldid=1018676910[Wikipedia page] for \x[eratosthenes]:
\Q[According to an entry in the Suda (a 10th-century encyclopedia), his critics scorned him, calling him \x[beta] (the second letter of the \x[greek-alphabet]) because he always came in second in all his endeavours.]
That's Ciro.

Some related ideas:
* \x[the-fox-and-the-cat-fable]
* \x[the-hedgehog-and-the-fox-by-isaiah-berlin-1953]
* \x[jack-of-all-trades-master-of-none]

= Mathematics Genealogy Project
{c}
{parent=mathematician}
{wiki}

= List of mathematicians
{parent=mathematician}

= Alexander Grothendieck
{c}
{parent=list-of-mathematicians}
{tag=god}
{wiki}

This dude looks like a \x[god]. \x[ciro-santilli] does not understand his stuff, but just based on the names of his theories, e.g. "Yoga of anabelian algebraic geometry", and on his eccentric lifestyle, it is obvious that he was in fact a God.

= Blaise Pascal
{c}
{parent=list-of-mathematicians}
{title2=1623-1662}
{wiki}

Good film about him: \x[blaise-pascal-1972].

Good quote from his https://en.wikipedia.org/wiki/Lettres_provinciales[Les Provinciales] (1656-57) Letter XII, p. 227:
\Q[
The war in which violence endeavours to crush truth is a strange and a long one.

All the efforts of violence cannot weaken truth, but only serve to exalt it the more.

The light of truth can do nothing to arrest violence; nay, it serves to provoke it still more.

When force opposes force, the more powerful destroys the less; when words are opposed to words, those which are true and convincing destroy and scatter those which are vain and false; but violence and truth can do nothing against each other.

Yet, let no one imagine that things are equal between them; for there is this final difference, that the course of violence is limited by the ordinance of God, who directs its workings to the glory of the truth, which it attacks; whereas truth subsists eternally, and triumphs finally over its enemies, because it is eternal, and powerful, like God Himself.
]
French version reproduced at: https://www.dicocitations.com/citation/auteurajout35106.php[].

= Euclid
{c}
{parent=list-of-mathematicians}
{wiki}

= Euclid's Elements
{c}
{parent=euclid}
{wiki}

= Synthetic geometry
{parent=euclid-s-elements}
{wiki}

A way to defined geometry without talking about coordinates, i.e. like \x[euclid-s-elements], notably \x[euclid-s-postulates], as opposed to \x[descartes]'s \x[real-coordinate-space].

= Euclid's postulates
{c}
{parent=euclid-s-elements}

Postulates are what we now call \x[axiom]{p}.

There are 5: https://en.wikipedia.org/w/index.php?title=Euclidean_geometry&oldid=1036511366#Axioms[], the \x[parallel-postulate] being the most controversial/interesting.

= Parallel postulate
{parent=euclid-s-postulates}
{wiki}

= Évariste Galois
{c}
{parent=list-of-mathematicians}
{wiki}

= Galois
{c}
{synonym}

= G. H. Hardy
{c}
{parent=list-of-mathematicians}
{wiki}

= A Mathematician's Apology
{c}
{parent=g-h-hardy}
{title2=1940}
{wiki}

With major mathematicians holding ideas such as:
\Q[Exposition, criticism, appreciation, is work for second-rate minds. \[...\] It is a melancholy experience for a professional mathematician to find himself writing about mathematics. The function of a mathematician is to do something, to prove new theorems, to add to mathematics, and not to talk about what he or other mathematicians have done.]
it is not surprise that the state of \x[stem] education is so shit as of 2020, especially at the \x[the-missing-link-between-basic-and-advanced]! This also implies that the number of people that can appreciate any advanced mathematics research is tiny, and consequently so is the funding.

= Henri Poincaré
{c}
{parent=list-of-mathematicians}
{tag=ecole-polytechnique-alumnus}
{tag=physicist}
{wiki}

= Poincaré
{synonym}

= James Harris Simons
{c}
{parent=list-of-mathematicians}
{wiki}

\x[ciro-santilli-s-wife], who was frustrated with \x[academia] at some point, admires the fact that Simons managed to make infinite money, and then invested back in actual science, e.g. through the \x[simons-foundation].

= Joseph Fourier
{c}
{parent=list-of-mathematicians}
{wiki}

= Fourier
{c}
{synonym}

= Paul Erdos
{c}
{parent=list-of-mathematicians}
{wiki}

= Erdos number
{c}
{parent=paul-erdos}
{wiki}

= René Descartes
{c}
{parent=list-of-mathematicians}
{wiki}

= Descartes
{c}
{synonym}

= Mathematical notation
{parent=mathematics}
{wiki}

It is hard to decide what makes \x[ciro-santilli] more sad: the usage of \x[greek-letter]{p}, the invention of new symbols, or the fifty million alternative font styles used.

Only \x[chinese-character]{p} could be worse than that!

= Mathematical symbol that looks like a Greek letter but isn't
{parent=mathematical-notation}

These are not in the \x[greek-alphabet]:
* \x[nabla]{child}
* \x[partial-derivative-symbol]{child}

= Number theory
{parent=mathematics}
{wiki}

= Arithmetic
{parent=number-theory}
{wiki}

Definition: "easy" number theory learnt in \x[primary-school], notably the operations of \x[addition], \x[subtraction], \x[multiplication] and \x[division].

= Modular arithmetic
{parent=number-theory}
{wiki}

= Modulo operation
{parent=modular-arithmetic}
{wiki}

= Modulo
{synonym}

= Modular multiplication
{parent=modular-arithmetic}

= Modular multiplicative inverse
{parent=modular-multiplication}

= Modular exponentiation
{parent=modular-arithmetic}
{wiki}

Can be calculated efficiently with the \x[extended-euclidean-algorithm].

= Lowest common denominator
{parent=number-theory}
{title2=LCD}
{wiki}

= Prime number
{parent=number-theory}
{wiki}

= Prime
{synonym}

= Goldbach's conjecture
{c}
{parent=prime-number}
{wiki}

= Prime number theorem
{parent=prime-number}
{wiki}

= Prime power
{parent=prime-number}
{wiki}

They come up a lot in many contexts, e.g.:
* \x[classification-of-finite-fields]

= Primality test
{parent=prime-number}
{wiki}

= Greatest common divisor
{parent=prime-number}
{title2=GCD}
{wiki}

= Coprime
{parent=greatest-common-divisor}
{wiki}

Two numbers such that the \x[greatest-common-divisor] is 1.

= Euclidean algorithm
{c}
{parent=greatest-common-divisor}
{wiki}

= Extended Euclidean algorithm
{c}
{parent=euclidean-algorithm}
{wiki}

= Least common multiple
{c}
{parent=prime-number}
{title2=lcm}
{wiki}

= Numerical analysis
{parent=mathematics}
{wiki}

Techniques to get numerical approximations to numeric \x[mathematical] problems.

The entire field comes down to estimating the true values with a known error bound, and creating algorithms that make those error bounds asymptotically smaller.

Not the most beautiful field of pure \x[mathematics], but fundamentally useful since we can't solve almost any \x[computational-physics][useful equation] without computers!

The solution visualizations can also provide valuable intuition however.

Important numerical analysis problems include solving:
* \x[partial-differential-equation]{p}

= Floating-point arithmetic
{parent=numerical-analysis}
{wiki}

= Floating-point number
{parent=floating-point-arithmetic}
{wiki}

= IEEE 754
{c}
{parent=floating-point-arithmetic}
{wiki}

Selected answers by \x[ciro-santilli] on the subject:
* https://stackoverflow.com/questions/18118408/what-is-difference-between-quiet-nan-and-signaling-nan/55648118#55648118[What is difference between quiet NaN and signaling NaN?]
* https://stackoverflow.com/questions/2618059/in-java-what-does-nan-mean/55673220#55673220[In Java, what does NaN mean?]
* https://stackoverflow.com/questions/8341395/what-is-a-subnormal-floating-point-number/53203428#53203428[What is a subnormal floating point number?]

= Numerical computing language
{parent=numerical-analysis}

All those dedicated \x[applied-mathematician]{p} languages are a waste of society's time, \x[ciro-santilli] sure \x[applied-mathematician]{p} are capable of writing a few extra braces in exchange for a sane general purpose language, we should instead just invest in good libraries with fast \x[c-programming-language] bindings for those languages like \x[numpy] where needed, and powerful mainlined \x[integrated-development-environment]{p}.

And when \x[ciro-santilli] see the closed source ones like \x[matlab] being used, it makes him lose all hope on humanity. Why. As of 2020. Why? In the 1980s, maybe. But in the 2020s?

= Scilab
{c}
{parent=numerical-computing-language}
{wiki}

= MATLAB
{c}
{parent=numerical-computing-language}
{wiki}

= Perturbation theory
{parent=numerical-analysis}
{wiki}

Used a lot in \x[quantum-mechanics], where the equations are really hard to solve. There's even a dedicated wiki page for it: https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)[]. Notably, \x[feynman-diagram]{p} are a way to represent perturbation calculations in \x[quantum-field-theory].

Let's gather some of the best results we come across here:
* \x[dirac-equation-solution-for-the-hydrogen-atom]

= Polynomial
{parent=mathematics}
{wiki}

= Algebraic equation
{parent=polynomial}
{tag=functional-equation}
{wiki}

= Polynomial equation
{synonym}
{title2}

= Named algebraic equation
{parent=algebraic-equation}

= Quadratic equation
{parent=named-algebraic-equation}
{wiki}

= Quadratic formula
{parent=quadratic-equation}
{wiki}

= Cubic equation
{parent=named-algebraic-equation}
{wiki}

= Quartic equation
{parent=named-algebraic-equation}
{wiki}

= Quintic equation
{parent=named-algebraic-equation}
{wiki}

= Abel-Ruffini theorem
{c}
{parent=quintic-equation}
{wiki}

= Algebraic number
{parent=algebraic-equation}
{wiki}

= Transcendental number
{parent=algebraic-number}
{wiki}

Sometimes \x[mathematician]{p} go a little overboard with their naming.

= Diophantine equation
{c}
{parent=polynomial}
{wiki}

Generalization of systems of \x[polynomial] equations.

= Undecidable Diophantine equation problems
{parent=diophantine-equation}

https://mathoverflow.net/questions/11540/what-are-the-most-attractive-turing-undecidable-problems-in-mathematics/11557#11557 contains a good overview of the decidability status of variants.

https://mathoverflow.net/questions/11540/what-are-the-most-attractive-turing-undecidable-problems-in-mathematics/103415#103415 provides a specific undecidable equation over the \x[integer]{p} with only one missing constant

= Named small order polynomial
{parent=polynomial}

= Linear polynomial
{parent=named-small-order-polynomial}

A \x[polynomial] of degree 1, i.e. of form $ax + b$.

= Galois theory
{c}
{parent=polynomial}
{wiki}

= Irreducible polynomial
{parent=polynomial}
{wiki}

= Multivariate polynomial
{parent=polynomial}

A \x[polynomial] with multiple input arguments, e.g. with two inputs $x$ and $y$:
$$
f(x, y) = x^2 + 2x + y^3 + 1
$$
as opposed to a \x[polynomial] with a single argument e.g. one with just $x$:
$$
f(x) = x^2 + 2x + 1
$$

= Polynomial over a field
{parent=polynomial}
{title2=$Field[X]$}
{wiki}

By default, we think of polynomials over the \x[real-number]{p} or \x[complex-number]{p}.

However, a polynomial can be defined over any other field just as well, the most notable example being that of a polynomial over a \x[finite-field].

For example, given the finite field of \x[order-algebra] 9, $GP(3)$ and with elements $\{0, 1, 2\}$, we can denote polynomials over that ring as
$$
GP(3)[x]
$$
where $x$ is the variable name.

For example, one such polynomial could be:
$$
P(x) = 2x^4 + x^2 + 2
$$
and another one:
$$
Q(X) = x^3 + 2x^2 + 2
$$
Note how all the coefficients are members of the finite field we chose.

Given this, we could evaluate the polynomial for any element of the field, e.g.:
$$
P(0) = 2 (0 \times 0 \times 0 \times 0) + (0 \times 0) + 2 = 2
P(1) = 2 (1 \times 1 \times 1 \times 1) + (1 \times 1) + 2 = 2 (1) + 1 + 2 = 2
P(2) = 2 (2 \times 2 \times 2 \times 2) + (2 \times 2) + 2 = 2 (16 % 3) + (4 % 3) + 2 = 2 + 1 + 2 = 2
$$
and so on.

We can also add polynomials as usual over the field:
$$
P(x) + Q(x) = 2x^4 + x^3 + (1+2)x^2 + (2 + 2) = 2x^4 + x^3 + (0)x^2 + 1 = 2x^4 + x^3 + 1
$$
and multiplication works analogously.

= Polynomial over a ring
{parent=polynomial-over-a-field}
{wiki}

The usual definition of a \x[polynomial] is over a \x[field-mathematics] as shown at \x[polynomial-over-a-field].

However, there is nothing in the immediate definition that prevents us from having a \x[ring-mathematics] instead, i.e. a \x[field-mathematics] but without the \x[commutative-property] and \x[inverse-element]{p}.

The only thing is that then we would need to differentiate between different orderings of the terms of \x[multivariate-polynomial], e.g. the following would all be potentially different terms:
$$
2xxy + 2xyx + 2yxx +
x2xy + x2yx + y2xx +
xx2y + xy2x + yx2x +
xxy2 + xyx2 + yxx2
$$
while for a field they would all go into a single term:
$$
12x^2y
$$
so when considering a polynomial over a \x[ring-mathematics] we end up with a lot more more possible terms.

= Polynomial ring
{parent=polynomial}
{wiki}

The polynomials together with polynomial addition and multiplication form a \x[commutative] \x[ring-mathematics].

= Probability
{parent=mathematics}
{wiki}

= Expectation value
{parent=probability}
{wiki}

= Standard deviation
{parent=probability}
{wiki}

= Statistics
{parent=probability}
{wiki}

= Mathematics bibliography
{parent=mathematics}

https://github.com/vEnhance/napkin Evan Chen's (陳誼廷) Infinite Napkin. 800+ page PDF with source on \x[github] claiming to try and teach the beauty of modern maths for high schoolers. Fantastic project!!!

= Mathematics website
{parent=mathematics-bibliography}

= Theorem of the Day
{parent=mathematics-website}

\x[website]{c}: https://www.theoremoftheday.org/

= Mathematics YouTube channel
{parent=mathematics-bibliography}

= 3Blue1Brown
{c}
{parent=mathematics-youtube-channel}
{wiki}

https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw

Amazing graphs and formulas.

Python graphics engine open sourced at: https://github.com/3b1b/manim "Animation engine for explanatory math videos". But for some reason there is a community fork: https://github.com/ManimCommunity/manim/ "This repository is maintained by the Manim Community, and is not associated with Grant Sanderson or 3Blue1Brown in any way (though we are definitely indebted to him for providing his work to the world). If you want to study how Grant makes his videos, head over to his repository (3b1b/manim). This is a more frequently updated repository than that one, and is recommended if you want to use Manim for your own projects." what a mess.

= MathDoctorBob
{c}
{parent=mathematics-youtube-channel}
{title2=Robert Donley}

https://www.youtube.com/user/MathDoctorBob/videos

He got so old from 2012 to 2021 :-)

This dude did well. If only he had \x[ourbigbook-com][written a hyperlinked wiki rather than making videos]! It would allow people to jump in at any point and just click back. It would be Godlike.

https://mathdoctorbob.org/About.html says:
\Q[Robert Donley received his doctorate in Mathematics from Stony Brook University and has over two decades of teaching experience at the high school, undergraduate, and graduate levels.]

