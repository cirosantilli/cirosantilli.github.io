= Artificial intelligence
{wiki}

= AI
{c}
{title2}
{synonym}

= Artificial general intelligence
{parent=artificial-intelligence}
{wiki}

= AGI
{c}
{synonym}
{title2}

Given enough computational power per dollar, AGI is inevitable, but it is not sure certain ever happen given the end of \x[moore-s-law][end of Moore's Law].

Alternatively, it could also be achieved genetically modified biological brains + https://en.wikipedia.org/wiki/Brain_in_a_vat[brain in a vat].

Imagine a brain the size of a building, perfectly engineered to solve certain engineering problems, and giving hints to human operators + taking feedback from cameras and audio attached to the operators.

This likely implies \x[transhumanism], and \x[mind-uploading].

\x[ciro-santilli] joined the silicon industry at one point to help increase our computational capacity and reach AGI.

Ciro believes that the easiest route to full AI, if any, could involve \x[ciro-s-2d-reinforcement-learning-games].

= AI alignment
{c}
{parent=artificial-intelligence}
{wiki}

As highlighted e.g. at \x[human-compatible-by-stuart-j-russell-2019], this AI alignment intrisically linked to the idea of \x[utility] in \x[economy].

= Artificial intelligence bibliography
{c}
{parent=artificial-intelligence}
{wiki}

= Human compatible by Stuart J. Russell (2019)
{c}
{parent=artificial-intelligence-bibliography}
{wiki=Human_compatible}

The key takeaway is that setting an explicit \x[value-function] to an \x[agi] entity is a good way to destroy the world due to poor \x[ai-alignment]. We are more likely to not destroy by creating an AI whose goals is to "do what humans what it to do", but in a way that it does not know before hand what it is that humans want, and it has to learn from them.

Some other cool ideas:
* a big thing that is missing for AGI in the 2010's is some kind of more hierarchical representation of the continuous input data of the world, e.g.:
  * when we behave, we do things in subroutines. E.g. life goal: save hunger. Subgoal: apply for some grant. Subsubgoal: eat, sleep, take shower. Subsub goal: move muscles to get me to table and open a can.
  * we can group continuous things into higher objects, e.g. all these pixels I'm seeing in front of me are a computer. So I treat all of them as a single object in my mind.
* \x[game-theory] can be seen as part of \x[artificial-intelligence] that deals with scenarios where multiple intelligent agents are involved
* \x[economy], and notably the study of the \x[utility], is intrinsically linked to \x[ai-alignment]

= Superintelligence by Nick Bostrom (2014)
{c}
{parent=artificial-intelligence-bibliography}
{wiki=Superintelligence:_Paths,_Dangers,_Strategies}

Good points:
* \x[post-mortem-connectome-extraction-with-microtome]{c}

= Natural language processing
{parent=machine-learning}
{wiki}

An impossible \x[artificial-general-intelligence][AGI-complete] dream.

It is impossible to understand speech, and take meaningful actions from it, if you don't understand what is being talked about.

And without doubt, "understanding what is being talked about" comes down to understanding (efficiently representing) the geometry of the 3D world with a time component.

Not from hearing sounds alone.

= AI training games
{c}
{tag=serious-game}
{parent=artificial-intelligence}

\x[ciro-santilli] took a stab at: \x[ciro-s-2d-reinforcement-learning-games], but he didn't sink too much/enough into that project.

= OpenAI
{c}
{parent=ai-training-games}
{wiki}

= OpenAI Gym
{c}
{parent=openai}

https://github.com/openai/gym

\Include[video-game]{parent=game}
