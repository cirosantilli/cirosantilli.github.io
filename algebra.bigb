= Algebra
{wiki}

= Algebraic
{synonym}

Not to be confused with \x[algebra-over-a-field], which is a particular \x[algebraic-structure] studied within algebra.

= Abstract algebra
{parent=algebra}
{wiki}

We just use "Abstract algebra" as a synonym for \x[algebra].

= Algebraic structure
{parent=algebra}
{wiki}

A \x[set-mathematics] $S$ plus any number of functions $f_i : S \times S \to S$, such that each $f_i$ satisfies some properties of choice.

Key examples:
* \x[group]: one function
* \x[field-mathematics]: two functions
* \x[ring-mathematics]: also two functions, but with less restrictive properties

= Commutator
{parent=algebraic-structure}
{wiki}

= Identity element
{parent=algebraic-structure}
{wiki}

= Inverse element
{parent=identity-element}
{wiki}

= Inverse
{synonym}

Some specific examples:
* \x[invertible-matrix]

= Invertible
{parent=inverse-element}

= Order
{disambiguate=algebra}
{parent=algebraic-structure}
{wiki}

The order of a \x[algebraic-structure] is just its \x[cardinality].

Sometimes, especially in the case of structures with an \x[infinite] number of elements, it is often more convenient to talk in terms of some parameter that characterizes the structure, and that parameter is usually called the \x[degree-algebra].

= Degree
{disambiguate=algebra}
{parent=order-algebra}
{wiki}

The degree of some \x[algebraic-structure] is some parameter that describes the structure. There is no universal definition valid for all structures, it is a per structure type thing.

This is particularly useful when talking about structures with an \x[infinite] number of elements, but it is sometimes also used for finite structures.

Examples:
* the \x[dihedral-group] of degree n acts on n elements, and has order 2n
* the parameter $n$ that characterizes the size of the \x[general-linear-group] $GL(n)$ is called the degree of that group, i.e. the dimension of the underlying matrices

= Finite algebraic structure
{parent=order-algebra}

Examples:
* \x[finite-group]{child}
* \x[finite-field]{child}

= Linear algebra
{parent=algebra}
{wiki}

= Linear function
{parent=linear-algebra}
{wiki}

= Linear
{synonym}

= Linearly
{synonym}

The term is not very clear, as it could either mean:
* a \x[real-number] function whose graph is a line, i.e.:
  $$f(x) = ax + b$$
  or for higher dimensions, a \x[hyperplane]:
  $$f(x_1, x_2, \ldots, x_n) = c_1 x_1 + c_2 x_2 + \ldots + c_n x_n + b$$
* a \x[linear-map]. Note that the above linear functions are not linear maps unless $b = 0$ (known as the homogeneous case), because e.g.:
  $$f(x + y) = ax + ay + b$$
  but
  $$f(x) + f(y) = ax + b + ay + b$$
  For this reason, it is better never to refer to linear maps as linear functions.

= Linear map
{parent=linear-algebra}
{title2=linear operator}
{wiki}

A linear map is a function $f : V_1(F) \to V_2(F)$ where $V_1(F)$ and $V_2(F)$ are two vector spaces over \x[underlying-field-of-a-vector-space][underlying fields] $F$ such that:
$$
\forall v_{1}, v_{2} \in V_1, c_{1}, c_{2} \in F \\
f(c_{1} v_{1} + c_{2} v_{2}) = c_{1} f(v_{1}) + c_{2} f(v_{2})
$$

A common case is $F = \R$, $V_1 = \R_m$ and $V_2 = \R_n$.

One thing that makes such functions particularly simple is that they can be fully specified by specifyin how they act on all possible combinations of input basis vectors: they are therefore specified by only a finite number of elements of $F$.

Every linear map in \x[finite-dimension] can be represented by a \x[matrix], the points of the \x[domain-function] being represented as \x[vector]{p}.

As such, when we say "linear map", we can think of a generalization of \x[matrix-multiplication] that makes sense in \x[infinite-dimensional] spaces like \x[hilbert-space]{p}, since calling such infinite dimensional maps "matrices" is stretching it a bit, since we would need to specify infinitely many rows and columns.

The prototypical building block of \x[infinite-dimensional] linear map is the \x[derivative]. In that case, the vectors being operated upon are \x[function]{p}, which cannot therefore be specified by a finite number of parameters, e.g. 

For example, the left side of the \x[time-independent-schrodinger-equation] is a linear map. And the \x[time-independent-schrodinger-equation] can be seen as a \x[eigenvalue] problem.

= Form
{disambiguate=mathematics}
{parent=linear-map}

A form is a \x[function] from a \x[vector-space] to elements of the \x[underlying-field-of-the-vector-space].

Examples:
* \x[linear-form]
* \x[bilinear-form]
* \x[multilinear-form]

= Linear form
{parent=linear-map}
{wiki}

A \x[linear-map]{c} where the \x[image-mathematics] is the \x[underlying-field-of-the-vector-space], e.g. $\R^n \to \R$.

The set of all \x[linear-form]{p} over a \x[vector-space] forms another vector space called the \x[dual-space].

= Matrix representation of a linear form
{parent=linear-form}

For the typical case of a linear form over \x[r-n], the form can be seen just as a row vector with n elements, the full form being specified by the value of each of the \x[basis-vector]{p}.

= Dual space
{parent=linear-form}
{title2=$V^*$}
{wiki}

The dual space of a \x[vector-space] $V$, sometimes denoted $V^*$, is the vector space of all \x[linear-form]{p} over $V$ with the obvious addition and scalar multiplication operations defined.

Since a linear form is completely determined by how it acts on a \x[basis], and since for each basis element it is specified by a scalar, at least in finite dimension, the dimension of the dual space is the same as the $V$, and so they are isomorphic because \x[all-vector-spaces-of-the-same-dimension-on-a-given-field-are-isomorphic], and so the dual is quite a boring concept in the context of finite dimension.

Infinite dimension seems more interesting however, see: https://en.wikipedia.org/w/index.php?title=Dual_space&oldid=1046421278#Infinite-dimensional_case

One place where duals are different from the non-duals however is when dealing with \x[tensor]{p}, because they transform differently than vectors from the base space $V$.

= Dual vector
{parent=dual-space}
{title2=$e^i$}

Dual vectors are the members of a \x[dual-space].

In the context of \x[tensor]{p} , we use raised indices to refer to members of the dual basis vs the underlying basis:
$$
\begin{aligned}
e_1 & \in V \\
e_2 & \in V \\
e_3 & \in V \\
e^1 & \in V^* \\
e^2 & \in V^* \\
e^3 & \in V^* \\
\end{aligned}
$$
The dual basis vectors $e^i$ are defined to "pick the corresponding coordinate" out of elements of V. E.g.:
$$
\begin{aligned}
e^1 (4, -3, 6) & =  4 \\
e^2 (4, -3, 6) & = -3 \\
e^3 (4, -3, 6) & =  6 \\
\end{aligned}
$$
By expanding into the basis, we can put this more succinctly with the \x[kronecker-delta] as:
$$
e^i(e_j) = \delta_{ij}
$$

Note that in \x[einstein-notation], the components of a dual vector have lower indices. This works well with the upper case indices of the dual vectors, allowing us to write a dual vector $f$ as:
$$
f = f_i e^i
$$

In the context of \x[quantum-mechanics], the \x[bra-ket][bra] notation is also used for dual vectors.

= Linear operator
{parent=linear-map}
{wiki}

= Operator
{synonym}

We define it as a \x[linear-map] where the \x[domain-function] is the same as the \x[image-mathematics], i.e. an \x[endofunction].

Examples:
* a 2x2 matrix can represent a \x[linear-map] from \x[r-2] to \x[r-2], so which is a linear operator
* the \x[derivative] is a \x[linear-map] from \x[c-infty] to \x[c-infty], so which is also a linear operator

= Adjoint operator
{parent=linear-operator}
{title2=$A^\dagger$}

Given a \x[linear-operator] $A$ over a space $S$ that has a \x[inner-product] defined, we define the adjoint operator $A^\dagger$ (the $\dagger$ symbol is called "dagger") as the unique operator that satisfies:
$$
\forall v, w \in S, <Av, w> = <v, A^{\dagger} w>
$$

= Self-adjoint operator
{parent=linear-map}
{wiki}

= Self-adjoint
{synonym}

= Multilinear map
{parent=linear-map}
{wiki}

= Bilinear map
{parent=multilinear-map}
{wiki}

= Bilinear product
{synonym}

\x[linear-map]{c} of two variables.

More formally, given 3 \x[vector-space]{p} X, Y, Z over a single \x[field-mathematics], a bilinear map is a function from:
$$f : X \times Y \to Z$$
that is linear on the first two arguments from X and Y, i.e.:
$$f(a_1\vec{x_1} + a_2\vec{x_2}, \vec{y}) = a_1f(\vec{x_1}, \vec{y}) + a_2f(\vec{x_2}, \vec{y})$$
Note that the definition only makes sense if all three vector spaces are over the same field, because linearity can mix up each of them.

The most important example by far is the \x[dot-product] from $\R^n \times \R^n \to \R$, which is more specifically also a \x[symmetric-bilinear-form].

= Bilinear form
{parent=multilinear-map}
{title2=$B(x, y)$}
{wiki}

Analogous to a \x[linear-form], a bilinear form is a \x[bilinear-map]{c} where the \x[image-mathematics] is the \x[underlying-field-of-the-vector-space], e.g. $\R^n \times \R^m \to \R$.

Some definitions require both of the input spaces to be the same, e.g. $\R^n \times \R^n \to \R$, but it doesn't make much different in general.

The most important example of a bilinear form is the \x[dot-product]. It is only defined if both the input spaces are the same.

= Matrix representation of a bilinear form
{parent=bilinear-form}

As usual, it is useful to think about how a \x[bilinear-form] looks like in terms of \x[vector]{p} and \x[matrix]{p}.

Unlike a \x[linear-form], which \x[matrix-representation-of-a-linear-form][was a vector], because it has two inputs, the bilinear form is represented by a matrix $M$ which encodes the value for each possible pair of \x[basis-vector]{p}.

In terms of that \x[matrix], the form $B(x,y)$ is then given by:
$$
B(x,y) = x^T M y
$$

= Effect of a change of basis on the matrix of a bilinear form
{parent=matrix-representation-of-a-bilinear-form}
{title2=$B_2 = C^T B C$}

If $C$ is the \x[change-of-basis-matrix], then the \x[matrix-representation-of-a-bilinear-form] $M$ that looked like:
$$
B(x,y) = x^T M y
$$
then the matrix in the new basis is:
$$
C^T M C
$$
\x[sylvester-s-law-of-inertia] then tells us that the number of positive, negative and 0 eigenvalues of both of those matrices is the same.

Proof: the value of a given bilinear form cannot change due to a \x[change-of-basis], since the bilinear form is just a \x[function-mathematics], and does not depend on the choice of basis. The only thing that change is the matrix representation of the form. Therefore, we must have:
$$
x^T M y = x_{new}^T M_{new} y_{new}
$$
and in the new basis:
$$
x = C x_{new} \\
y = C y_{new} \\
x_{new}^T M_{new} y_{new} = x^T M y =  (Cx_{new})^T M (Cy_{new}) = x_{new}^T (C^T M C) y_{new} \\
$$
and so since:
$$
\forall x_{new}, y_{new} x_{new}^T M_{new} y_{new} = x_{new}^T (C^T M C) y_{new} \implies M_{new} = C^T M C \\
$$

Related:
* https://proofwiki.org/wiki/Matrix_of_Bilinear_Form_Under_Change_of_Basis

= Multilinear form
{parent=multilinear-map}
{wiki}

See \x[form-mathematics].

Analogous to a \x[linear-form], a multilinear form is a \x[multilinear-map]{c} where the \x[image-mathematics] is the \x[underlying-field-of-the-vector-space], e.g. $\R^{n_1} \times \R^{n_2} \times \R^{n_2} \to \R$.

= Symmetric bilinear map
{parent=multilinear-map}
{wiki}

Subcase of \x[symmetric-multilinear-map]:
$$f(x, y) = f(y, x)$$

Requires the two inputs $x$ and $y$ to be in the same \x[vector-space] of course.

The most important example is the \x[dot-product], which is also a \x[positive-definite-symmetric-bilinear-form].

= Symmetric bilinear form
{parent=symmetric-bilinear-map}
{wiki}

\x[symmetric-bilinear-map]{p} that is also a \x[bilinear-form].

= Matrix representation of a symmetric bilinear form
{parent=symmetric-bilinear-form}

= Matrix representation of the symmetric bilinear form
{synonym}

Like the \x[matrix-representation-of-a-bilinear-form], it is a \x[matrix], but now the matrix has to be a \x[symmetric-matrix].

We can then immediately see that the matrix is symmetric, then so is the form. We have:
$$
B(x,y) = x^T M y
$$
But because $B(x,y)$ is a \x[scalar], we have:
$$
B(x,y) = B(x,y)^T
$$
and:
$$
B(x,y) = B(x,y)^T = (x^T M y)^T = y^T M^T x = y^T M^T x = y^T M x = B(y,x)
$$

= Hermitian form
{c}
{parent=symmetric-bilinear-map}
{wiki}

The \x[complex-number] analogue of a \x[symmetric-bilinear-form].

The prototypical example of it is the \x[complex-dot-product].

Note that this form is neither strictly \x[symmetric-bilinear-map][symmetric], it satisfies:
$$
<x, y> = \overline{<y, x>}
$$
where the over bar indicates the \x[complex-conjugate], nor is it linear for complex scalar multiplication on the second argument.

Bibliography:
* https://mathworld.wolfram.com/HermitianForm.html

= Matrix representation of a Hermitian form
{parent=hermitian-form};

A \x[hermitian-matrix].

= Quadratic form
{parent=symmetric-bilinear-map}
{wiki}

\x[multivariate-polynomial]{c} where each term has degree 2, e.g.:
$$
f(x,y) = 2y^2 + 10yx + x^2
$$
is a quadratic form because each term has degree 2:
* $y^2$
* $xy$
* $x^2$
but e.g.:
$$
f(x,y) = 2y^2 + 10yx + x^3
$$
is not because the term $x^3$ has degree 3.

There is a \x[1-to-1] relationship between \x[quadratic-form]{p} and \x[symmetric-bilinear-form]{p}. In matrix representation, this can be written as:
$$
\vec{x}^T B \vec{x}
$$
where $\vec{x}$ contains each of the variabes of the form, e.g. for 2 variables:
$$
\vec{x} = [x, y]
$$

Strictly speaking, the associated \x[bilinear-form] would not need to be a \x[symmetric-bilinear-form], at least for the \x[real-number]{p} or \x[complex-number]{p} which are \x[commutative]. E.g.:
$$
\begin{bmatrix}x y\end{bmatrix}
\begin{bmatrix}0 & 1 \\ 2 & 0 \\ \end{bmatrix}
\begin{bmatrix}x \\ y \\ \end{bmatrix}
=
\begin{bmatrix}x y\end{bmatrix}
\begin{bmatrix}y \\ 2x \\\end{bmatrix}
= xy + 2yx
= 3xy
$$
But that same matrix could also be written in symmetric form as:
$$\begin{bmatrix}0 & 1.5 \\ 1.5 & 0 \\ \end{bmatrix}$$
so why not I guess, its simpler/more restricted.

= Positive definite symmetric bilinear form
{parent=symmetric-bilinear-map}
{wiki}

\x[symmetric-bilinear-form]{c} that is also positive definite, i.e.:
$$
\forall x, B(x, x) > 0
$$

= Matrix representation of a positive definite symmetric bilinear form
{parent=positive-definite-symmetric-bilinear-form}

A \x[positive-definite-matrix] that is also a \x[symmetric-matrix].

= Skew-symmetric bilinear map
{parent=symmetric-bilinear-map}

= Antisymmetric bilinear map
{synonym}

Subcase of \x[antisymmetric-multilinear-map]:
$$f(x, y) = -f(y, x)$$

= Skew-symmetric bilinear form
{parent=symmetric-bilinear-map}
{wiki}

\x[skew-symmetric-bilinear-map]{c} that is also a \x[bilinear-form].

= Symmetric multilinear map
{parent=multilinear-map}

Same value if you swap any input arguments.

= Antisymmetric multilinear map
{parent=symmetric-multilinear-map}

Change sign if you swap two input values.

= Alternating multilinear map
{parent=multilinear-map}

Implies \x[antisymmetric-multilinear-map].

= Dot product
{parent=linear-algebra}
{wiki}

The definition of the "dot product" of a general space varies quite a lot with different contexts.

Most definitions tend to be \x[bilinear-form]{p}.

We use the unqualified generally refers to the dot product of \x[real-coordinate-space]{p}, which is a \x[positive-definite-symmetric-bilinear-form]. Other important examples include:
* the \x[complex-dot-product], which is not strictly \x[symmetric-bilinear-map][symmetric] nor \x[linear], but it is \x[positive-definite]
* \x[minkowski-inner-product], sometimes called" "Minkowski dot product is not \x[positive-definite]
The rest of this section is about the \x[r-n] case.

The \x[positive-definite] part of the definition likely comes in because we are so familiar with \x[metric-space]{p}, which requires a positive \x[norm] in the \x[norm-induced-by-an-inner-product].

The default \x[euclidean-space] definition, we use the \x[matrix-representation-of-a-symmetric-bilinear-form] as the identity matrix, e.g. in \x[r-3]:
$$
M =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
$$
so that:
$$
\vec{x} \cdot \vec{y}
=
\begin{bmatrix}
x_1 & x_2 & x_3 \\
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
\end{bmatrix}
=
x_1y_1 + x_2y_2 + x_3y_3
$$

= Orthogonality
{parent=dot-product}
{wiki}

= Orthogonal
{synonym}

= Orthonormality
{parent=orthogonality}
{wiki}

= Orthonormal
{synonym}

= Angle
{parent=dot-product}
{wiki}

= Cross product
{parent=linear-algebra}
{title2=$\va{x} \times \va{y}$}
{wiki}

= Jacobi identity
{c}
{parent=cross-product}
{wiki}

= Index picking function
{parent=linear-algebra}

= Kronecker delta
{c}
{parent=index-picking-function}
{title2=$\delta_{ij}$}
{wiki}

= Levi-Civita symbol
{c}
{parent=index-picking-function}
{title2=$\varepsilon$}
{wiki}

Denoted by the \x[greek-letter-epsilon] with `\varepsilon` encoding in \x[latex].

Definition:
* \x[odd-permutation]: -1
* \x[even-permutation]: 1
* not a \x[permutation]: 0. This happens iff two more more indices are repeated

= Levi-Civita symbol as a tensor
{parent=levi-civita-symbol}

\x[an-introduction-to-tensors-and-group-theory-for-physicists-by-nadir-jeevanjee-2011]{c} shows that this is a \x[tensor] that represents the \x[volume-of-a-parallelepiped].

It takes as input three vectors, and outputs one real number, the volume. And it is linear on each vector. This perfectly satisfied the definition of a tensor of \x[order-of-a-tensor][order] (3,0).

Given a basis $(e_i, e_j, e_k)$ and a function that return the volume of a parallelepiped given by three vectors $V(v_1, v_2, v_3)$, $\varepsilon_{ikj} = V(e_i, e_j, e_k)$.

= Projection
{disambiguate=mathematics}
{parent=linear-algebra}

= Matrix
{parent=linear-algebra}
{wiki}

= Matrix operation
{parent=matrix}
{wiki}

= Determinant
{parent=matrix-operation}
{title2=$det$}
{wiki}

Name origin: likely because it "determines" if a matrix is \x[invertible-matrix][invertible] or not, as a matrix is invertible iff determinant is not zero.

= Matrix inverse
{parent=matrix-operation}
{title2=$M^{-1}$}

When it exists, which is not for all matrices, only \x[invertible-matrix], the inverse is denoted:
$$
M^{-1}
$$

= Invertible matrix
{parent=matrix-inverse}
{tag=named-matrix}
{wiki}

The set of all \x[invertible-matrix]{p} forms a \x[group]: the \x[general-linear-group] with \x[matrix-multiplication]. Non-invertible matrices don't form a group due to the lack of inverse.

= Transpose
{parent=matrix-operation}
{title2=$M^T$}
{wiki}

= Transpose of a matrix multiplication
{parent=transpose}
{wiki}

When it distributes it inverts the order of the \x[matrix-multiplication]:
$$(MN)^T = N^T M^T$$

= Inverse of the transpose
{parent=transpose}
{wiki}

The \x[transpose] and \x[matrix-inverse] commute:
$$
(M^T)-1 = (M^{-1})^T
$$

= Matrix multiplication
{parent=matrix}
{wiki}

= Matrix product
{synonym}

Since a \x[matrix] $M$ can be seen as a \x[linear-map] $f_M(\vec{x})$, the product of two matrices $MN$ can be seen as the composition of two \x[linear-map]{p}:
$$f_M(f_N(\vec{x}))$$
One cool thing about linear functions is that we can easily pre-calculate this product only once to obtain a new matrix, and so we don't have to do both multiplications separately each time.

= Matrix multiplication algorithm
{parent=matrix-multiplication}
{tag=computational-problem}

https://math.stackexchange.com/questions/30330/fast-algorithm-for-solving-system-of-linear-equations/259372#259372

= Matrix decomposition
{parent=matrix-multiplication}
{wiki}

= Eigenvalues and eigenvectors
{parent=matrix}
{wiki}

= Applications of eigenvalues and eigenvectors
{parent=eigenvalues-and-eigenvectors}

* https://math.stackexchange.com/questions/23312/what-is-the-importance-of-eigenvalues-eigenvectors/3503875#3503875
* https://math.stackexchange.com/questions/1520832/real-life-examples-for-eigenvalues-eigenvectors
* https://matheducators.stackexchange.com/questions/520/what-is-a-good-motivation-showcase-for-a-student-for-the-study-of-eigenvalues

= Characteristic polynomial
{parent=eigenvalues-and-eigenvectors}
{wiki}

= Eigenvalue
{parent=eigenvalues-and-eigenvectors}

See: \x[eigenvalues-and-eigenvectors].

= Spectrum
{disambiguate=functional analysis}
{parent=eigenvalue}

Set of \x[eigenvalue]{p} of a \x[linear-operator].

= Continuous spectrum
{disambiguate=functional analysis}
{parent=spectrum-functional-analysis}

Unlike the simple case of a \x[matrix], in \x[infinite-dimensional] vector spaces, the spectrum may be continuous.

The quintessential example of that is the spectrum of the \x[position-operator] in \x[quantum-mechanics], in which any \x[real-number] is a possible \x[eigenvalue], since the particle may be found in any position. The associated \x[eigenvector]{p} are the corresponding \x[dirac-delta-function]{p}.

= Eigendecomposition of a matrix
{parent=eigenvalues-and-eigenvectors}
{wiki}

= Eigendecomposition
{synonym}

Every \x[invertible-matrix] $M$ can be written as:
$$
M = QDQ^{-1}
$$
where:
* $D$ is a \x[diagonal-matrix] containing the \x[eigenvalue]{p} of $M$
* columns of $Q$ are \x[eigenvector]{p} of $M$
Note therefore that this decomposition is unique up to swapping the order of eigenvectors. We could fix a canonical form by sorting eigenvectors from smallest to largest in the case of a \x[real-number].

Intuitively, Note that this is just the \x[change-of-basis] formula, and so:
* $Q^{-1}$ changes basis to align to the eigenvectors
* $D$ multiplies eigenvectors simply by eigenvalues
* $Q$ changes back to the original basis

= Eigendecomposition of a real symmetric matrix
{parent=eigendecomposition-of-a-matrix}

= The eigendecomposition of a real symmetric matrix is done with orthogonal matrices
{synonym}

The general result from \x[eigendecomposition-of-a-matrix]:
$$
M = QDQ^{-1}
$$
becomes:
$$
M = ODO^T
$$
where $O$ is an \x[orthogonal-matrix], and therefore has $O^{-1} = O^T$.

= Sylvester's law of inertia
{c}
{parent=eigendecomposition-of-a-matrix}
{wiki}

The main interest of this theorem is in \x[classifying-mathematics] the \x[indefinite-orthogonal-group]{p}, which in turn is fundamental because the \x[lorentz-group] is an \x[indefinite-orthogonal-group]{p}, see: \x[all-indefinite-orthogonal-groups-of-matrices-of-equal-metric-signature-are-isomorphic].

It also tells us that a \x[change-of-basis] does not the alter the \x[metric-signature] of a \x[bilinear-form], see \x[matrix-congruence-can-be-seen-as-the-change-of-basis-of-a-bilinear-form].

The theorem states that the number of 0, 1 and -1 in the \x[metric-signature] is the same for two \x[symmetric-matrix]{p} that are \x[congruent-matrix]{p}.

For example, consider:
$$
A = \begin{bmatrix}2 & \sqrt{2} \\ \sqrt{2} & 3 \\\end{bmatrix}
$$

The \x[eigenvalue]{p} of $A$ are $1$ and $4$, and the associated eigenvectors are:
$$
v_1 = [-\sqrt{2}, 1]^T
v_4 = [\sqrt{2}/2, 1]^T
$$
\x[sympy] code:
``
A = Matrix([[2, sqrt(2)], [sqrt(2), 3]])
A.eigenvects()
``
and from the \x[eigendecomposition-of-a-real-symmetric-matrix] we know that:
$$
A = PDP^T =
\begin{bmatrix}-\sqrt{2} & \sqrt{2}/2 \\ 1 & 1\\\end{bmatrix}
\begin{bmatrix}1 & 0 \\ 0 & 4\\\end{bmatrix}
\begin{bmatrix}-\sqrt{2} & 1 \\ \sqrt{2}/2 & 1\\\end{bmatrix}
$$

Now, instead of $P$, we could use $PE$, where $E$ is an arbitrary \x[diagonal-matrix] of type:
$$
\begin{bmatrix}e_1 & 0 \\ 0 & e_2\\\end{bmatrix}
$$
With this, would reach a new matrix $B$:
$$
B = (PE)D(PE)^T = P(EDE^T)P^T = P(EED)P^T
$$
Therefore, with this congruence, we are able to multiply the eigenvalues of $A$ by any positive number $e_1^2$ and $e_2^2$. Since we are multiplying by two arbitrary positive numbers, we cannot change the signs of the original eigenvalues, and so the \x[metric-signature] is maintained, but respecting that any value can be reached.

Note that the \x[matrix-congruence] relation looks a bit like the \x[eigendecomposition-of-a-matrix]:
$$
D = SMS^T
$$
but note that $D$ does not have to contain \x[eigenvalue]{p}, unlike the \x[eigendecomposition-of-a-matrix]. This is because here $S$ is not fixed to having \x[eigenvector]{p} in its columns.

But because the matrix is symmetric however, we could always choose $S$ to actually diagonalize as mentioned at \x[eigendecomposition-of-a-real-symmetric-matrix]. Therefore, the \x[metric-signature] can be seen directly from \x[eigenvalue]{p}.

Also, because $D$ is a \x[diagonal-matrix], and thus symmetric, it must be that:
$$
S^T = S^{-1}
$$

What this does represent, is a general \x[change-of-basis] that maintains the matrix a \x[symmetric-matrix].

Related:
* https://math.stackexchange.com/questions/1817906/sylvesters-law-of-inertia
* https://math.stackexchange.com/questions/1284601/what-is-the-lie-group-that-leaves-this-matrix-invariant
* https://physics.stackexchange.com/questions/24495/metric-signature-explanation

= Congruent matrix
{parent=sylvester-s-law-of-inertia}

= Matrix congruence
{synonym}

Two \x[symmetric-matrix]{p} $A$ and $B$ are defined to be congruent if there exists an $S$ in \x[gl-n] such that:
$$
A = S B S^T
$$

= Matrix congruence can be seen as the change of basis of a bilinear form
{parent=congruent-matrix}

From \x[effect-of-a-change-of-basis-on-the-matrix-of-a-bilinear-form], remember that a change of basis $C$ modifies the \x[matrix-representation-of-a-bilinear-form] as:
$$
C^T M C
$$

So, by taking $S = C^T$, we understand that two matrices being congruent means that they can both correspond to the same \x[bilinear-form] in different bases.

= Matrix similarity
{parent=sylvester-s-law-of-inertia}
{wiki}

= Similar matrix
{synonym}

= Metric signature
{parent=sylvester-s-law-of-inertia}
{wiki}

= Metric signature matrix
{parent=metric-signature}

= Eigenvector
{parent=eigenvalues-and-eigenvectors}

See: \x[eigenvalues-and-eigenvectors].

= Eigenvectors and eigenvalues of the identity matrix
{parent=eigenvalues-and-eigenvectors}

https://math.stackexchange.com/questions/1507290/linear-algebra-identity-matrix-and-its-relation-to-eigenvalues-and-eigenvectors/3934023#3934023

= Spectral theorem
{parent=eigenvalues-and-eigenvectors}
{wiki}

= Hermitian matrix
{c}
{parent=spectral-theorem}
{wiki}

= Hermitian operator
{c}
{parent=hermitian-matrix}

This is the possibly infinite dimensional version of a \x[hermitian-matrix], since \x[linear-operator]{p} are the possibly infinite dimensional version of \x[matrix]{p}.

There's a catch though: now we don't have explicit matrix indices here however in general, the generalized definition is shown at: https://en.wikipedia.org/w/index.php?title=Hermitian_adjoint&oldid=1032475701#Definition_for_bounded_operators_between_Hilbert_spaces

= Riesz representation theorem
{c}
{parent=hermitian-operator}

= Kronecker product
{c}
{parent=matrix}
{wiki}

= Named matrix
{parent=matrix}
{wiki=List_of_named_matrices}

= Diagonal matrix
{parent=named-matrix}
{wiki}

Forms a \x[normal-subgroup] of the \x[general-linear-group].

= Scalar matrix
{parent=diagonal-matrix}
{title2=$Z(V)$}
{wiki=Diagonal_matrix#Scalar_matrix}

Forms a \x[normal-subgroup] of the \x[general-linear-group].

= Identity matrix
{parent=scalar-matrix}
{title2=$I_n$}
{wiki}

= Square matrix
{parent=named-matrix}
{wiki}

= Matrix ring
{parent=square-matrix}
{wiki}

= Matrix ring of degree n
{title2}
{synonym}

= $M_n$
{title2}
{synonym}

= Set of all n-by-y square matrices
{title2}
{synonym}

The matrix ring of degree n $M_n$ is the set of all n-by-n square matrices together with the usual \x[vector-space] and \x[matrix-multiplication] operations.

This set forms a \x[ring-mathematics].

Related terminology:
* https://math.stackexchange.com/questions/412200/what-is-the-notation-for-the-set-of-all-m-times-n-matrices

= Orthogonal matrix
{parent=named-matrix}
{wiki}

Members of the \x[orthogonal-group].

= Unitary matrix
{parent=orthogonal-matrix}
{wiki}

\x[complex-number][Complex]{c} analogue of \x[orthogonal-matrix].

Applications:
* in \x[quantum-computer]{p} programming basically comes down to creating one big unitary matrix as explained at: \x[quantum-computing-is-just-matrix-multiplication]

= Triangular matrix
{parent=named-matrix}
{wiki}

= Symmetric matrix
{parent=named-matrix}
{wiki}

A \x[matrix] that equals its \x[transpose]:
$$
M = M^T
$$

Can represent a \x[symmetric-bilinear-form] as shown at \x[matrix-representation-of-a-symmetric-bilinear-form], or a \x[quadratic-form].

= Definite matrix
{parent=symmetric-matrix}
{wiki}

The definition implies that this is also a \x[symmetric-matrix].

= Positive definite matrix
{parent=definite-matrix}

= Positive definite
{synonym}

The \x[dot-product] is a \x[positive-definite-matrix], and so we see that those will have an important link to familiar \x[geometry].

= Skew-symmetric matrix
{parent=symmetric-matrix}
{wiki}

= Antisymmetric matrix
{title2}
{synonym}

WTF is a skew? "Antisymmetric" is just such a better name! And it also appears in other definitions such as \x[antisymmetric-multilinear-map].

= Skew-symmetric form
{parent=skew-symmetric-matrix}

= Vector space
{parent=linear-algebra}
{wiki}

= Basis
{disambiguate=linear algebra}
{parent=vector-space}
{wiki}

= Basis
{synonym}

= Basis vector
{synonym}

= Change of basis
{parent=basis-linear-algebra}
{wiki}

$$N = BMB^{-1}$$
where:
* $M$: matrix in the old basis
* $N$: matrix in the new basis
* $B$: change of basis matrix

= Change of basis matrix
{parent=change-of-basis}

The change of basis matrix $C$ is the matrix that allows us to express the new basis in an old basis:
$$x_{old} = Cx_{new}$$

Mnemonic is as follows: consider we have an initial basis $(x_{old}, y_{old})$. Now, we define the new basis in terms of the old basis, e.g.:
$$
\begin{aligned}
x_{new} &= 1x_{old} + 2y_{old} \\
y_{new} &= 3x_{old} + 4y_{old} \\
\end{aligned}
$$
which can be written in matrix form as:
$$
\begin{bmatrix}x_{new} \\ y_{new} \\\end{bmatrix} =
\begin{bmatrix}1 && 2 \\ 3 && 4 \\\end{bmatrix}
\begin{bmatrix}x_{old} \\ y_{old} \\\end{bmatrix}
$$
and so if we set:
$$
M = \begin{bmatrix}1 && 2 \\ 3 && 4 \\\end{bmatrix}
$$
we have:
$$
\vec{x_{new}} = M\vec{x_{old}}
$$

The usual question then is: given a vector in the new basis, how do we represent it in the old basis?

The answer is that we simply have to calculate the \x[matrix-inverse] of $M$:
$$
\vec{x_{old}} =  M^{-1}\vec{x_{new}}
$$

That $M^{-1}$ is the matrix inverse.

= Change of basis between symmetric matrices
{parent=change-of-basis}

When we have a \x[symmetric-matrix], a \x[change-of-basis] keeps symmetry iff it is done by an \x[orthogonal-matrix], in which case:
$$N = BMB^{-1} = OMO^T$$

= Linear independence
{parent=basis-linear-algebra}
{wiki}

= Linearly independent
{synonym}

= Classification of vector spaces
{parent=vector-space}

= All vector spaces of the same dimension on a given field are isomorphic
{synonym}

https://en.wikipedia.org/wiki/Dimension_(vector_space)#Facts

= Underlying field of a vector space
{parent=vector-space}

= Underlying field of the vector space
{synonym}

Every vector space is defined over a \x[field-mathematics].

E.g. in $\R^3$, the underlying \x[field-mathematics] is $\R$, the \x[real-number]{p}. And in $\C^2$ the underlying field is $\C$, the \x[complex-number]{p}.

Any field can be used, including \x[finite-field]. But the underlying thing has to be a field, because the definitions of a vector need all field properties to hold to make sense.

Elements of the underlying field of a vector space are known as \x[scalar-mathematics].

= Vector
{disambiguate=mathematics}
{parent=vector-space}
{wiki=Vector (mathematics and physics)}

= Vector
{synonym}

= Vectorized
{synonym}

= Scalar
{disambiguate=mathematics}
{parent=vector-mathematics}
{wiki}

= Scalar
{synonym}

A member of the \x[underlying-field-of-a-vector-space]. E.g. in $\R^3$, the underlying field is $\R$, and a scalar is a member of $\R$, i.e. a \x[real-number].

= Tensor
{parent=linear-algebra}
{wiki}

A \x[multilinear-form] with a \x[domain-function] that looks like:
$$
V^m \times {V*}^n \to \R
$$
where $V*$ is the \x[dual-space].

Because a tensor is a \x[multilinear-form], it can be fully specified by how it act on all combinations of basis sets, which can be done in terms of components. We refer to each component as:
$$
T_{i_1 \ldots i_m}^{j_1 \ldots j_n} = T(e_{i_1}, \ldots, e_{i_m}, e^{j_1}, \ldots, e^{j_m})
$$
where we remember that the raised indices refer \x[dual-vector].

Some examples:
* \x[levi-civita-symbol-as-a-tensor]{child}
* \x[a-linear-map-is-a-1-1-tensor]

= A linear map is a (1,1) tensor
{parent=tensor}

A linear map $A$ can be seen as a (1,1) \x[tensor] because:
$$
T(w, v*) = v* A w
$$
is a number, $v*$. is a \x[dual-vector], and \x[w] is a \x[vector]. Furthermoe, $T$ is linear in both $v*$ and $w$. All of this makes $T$ fullfill the definition of a (1,1) tensor.

= Tensor space
{parent=tensor}
{title2=$T^{(m, n)}$}

Bibliography:
* https://mathworld.wolfram.com/TensorSpace.html

= Order of a tensor
{parent=tensor-space}

$T^{(m, n)}$ has order $(m, n)$

= Einstein notation
{c}
{parent=tensor}
{wiki}

= Einstein summation convention
{c}
{synonym}
{title2}

The https://en.wikipedia.org/w/index.php?title=Einstein_notation&oldid=1021244532[Wikipedia page] of this article is basically a masterclass why \x[ourbigbook-com/wikipedia][Wikipedia is useless for learning technical subjects]. They are not even able to teach such a simple subject properly there!

Bibliography:
* https://www.maths.cam.ac.uk/postgrad/part-iii/files/misc/index-notation.pdf gives a definition that does not consider upper and lower indexes, it only counts how many times the indices appear

  Their definition of the \x[laplacian] is a bit wrong as only one $i$ appears in it, they likely meant to have written $\pdv{}{x_i}\pdv{F}{x_i}$ instead of $\pdv{^2 F}{x_i^2}$, related: 

= Raised and lowered indices
{parent=einstein-notation}

TODO what is the point of them? Why not just sum over every index that appears twice, regardless of where it is, as mentioned at: https://www.maths.cam.ac.uk/postgrad/part-iii/files/misc/index-notation.pdf[].

Vectors with the index on top such as $x^i$ are the "regular vectors", they are called \x[covariant-vector]{p}.

Those in indices on bottom are called \x[contravariant-vector]{p}.

It is possible to change between them by \x[raising-and-lowering-indices].

The values are different only when the \x[metric-signature-matrix] is different from the \x[identity-matrix].

= Raised index
{parent=raised-and-lowered-indices}

= Lowered index
{parent=raised-and-lowered-indices}

= Raising and lowering indices
{c}
{parent=raised-and-lowered-indices}
{wiki}

= Implicit metric signature in Einstein notation
{parent=einstein-notation}

Then a specific \x[metric] is involved, sometimes we want to automatically add it to products.

E.g., in a context considering the common \x[minkowski-inner-product-matrix] where the $\eta$ 4x4 matrix and $\mu$ is a vector in \x[r-4]
$$
x^{\mu} x_{\mu} = x^{\mu} \eta_{\mu \nu} x^{\nu} = -x_0^2 + x_1^2 + x_2^2 + x_3^2;
$$
which leads to the change of sign of some terms.

= Einstein notation for partial derivatives
{parent=einstein-notation}

The \x[einstein-summation-convention] works will with \x[partial-derivative], and this case is widely used in \x[particle-physics].

\x[partial-index-partial-derivative-notation]{c} is the \x[partial-derivative-notation] commonly used in this context, as we want to do operations by index rather than by labels such as $x$, $y$, $z$.

This notation also allows us to have \x[raised-and-lowered-indices] on the \x[partial-derivative-symbol] TODO how are they different?

= Divergence in Einstein notation
{parent=einstein-notation-for-partial-derivatives}

Given a vector function of three variables:
$$
F(x_0, x_1, x_2) = (F^0(x_0, x_1, x_2), F^1(x_0, x_1, x_2), F^2(x_0, x_1, x_2)) : \R^3 \to \R^3
$$
so note that we are denoting each component of $F$ as $F^i$ with a \x[raised-index].

Then, the \x[divergence] can be written in \x[einstein-notation] as:
$$
\div{F} = \partial_i F^i(x_0, x_1, x_2) = \pdv{F^i(x_0, x_1, x_2)}{x^i} = \pdv{F^0(x_0, x_1, x_2)}{x_0} + \pdv{F^1(x_0, x_1, x_2)}{x_1} + \pdv{F^2(x_0, x_1, x_2)}{x_2}
$$

It is common to just omit the variables of the function, so we tend to just say:
$$
\div{F} = \partial_i F^i
$$
or equivalently when referring just to the operation:
$$
\div{} = \partial_i
$$

= Laplacian in Einstein notation
{c}
{parent=einstein-notation-for-partial-derivatives}
{title2=$\partial_i \partial^i$}

Given a real function of three variables:
$$
F(x_0, x_1, x_2) = : \R^3 \to \R
$$
its \x[laplacian] can be written as:
$$
\laplacian{F(x_0, x_1, x_2)} = \partial_i \partial^i F(x_0, x_1, x_2) = \\
\partial_0 \partial^0 F(x_0, x_1, x_2) + \partial_1 \partial^1 F(x_0, x_1, x_2) + \partial_2 \partial^2 F(x_0, x_1, x_2) \\
\partial_0^2 F(x_0, x_1, x_2) + \partial_1^2 F(x_0, x_1, x_2) + \partial_2^2 F(x_0, x_1, x_2)
$$

It is common to just omit the variables of the function, so we tend to just say:
$$
\laplacian{F} = \partial_i \partial^i F
$$
or equivalently when referring just to the operation:
$$
\laplacian{} = \partial_i \partial^i
$$

= D'alembert operator in Einstein notation
{c}
{parent=laplacian-in-einstein-notation}
{title2=$\partial_i \partial^i$}

Given the function $\psi$:
$$
\psi : \R^4 \to \C
$$
the operator can be written in \x[planck-units] as:
$$
\partial_i \partial^i \psi(x_0, x_1, x_2, x_3) - m^2 \psi(x_0, x_1, x_2, x_3) = 0
$$
often written without function arguments as:
$$
\partial_i \partial^i \psi
$$
Note how this looks just like the \x[laplacian-in-einstein-notation],  since the \x[d-alembert-operator] is just a generalization of the \x[laplace-operator] to \x[minkowski-space].

= Klein-Gordon equation in Einstein notation
{parent=d-alembert-operator-in-einstein-notation}

The \x[klein-gordon-equation] can be written in terms of the \x[d-alembert-operator] as:
$$
\Box \psi + m^2 \psi = 0
$$
so we can expand the \x[d-alembert-operator-in-einstein-notation] to:
$$
\partial_i \partial^i \psi - m^2 \psi = 0
$$

= Covariance and contravariance of vectors
{parent=einstein-notation}
{wiki}

= Covariant vector
{parent=covariance-and-contravariance-of-vectors}

= Contravariant vector
{parent=covariance-and-contravariance-of-vectors}

= Linear algebra bibliography
{parent=linear-algebra}

https://textbooks.math.gatech.edu/ila/index.html Interactive Linear Algebra. Source: https://github.com/QBobWatson/ila[]. Written in \x[mathbook-xml].

= Group
{disambiguate=mathematics}
{parent=algebra}
{wiki}

= Group
{synonym}

= Center
{disambiguate=group theory}
{parent=group-mathematics}
{wiki}

= Center
{disambiguate=group}
{synonym}

= Commutative property
{parent=group-mathematics}
{wiki}

= Commutative
{synonym}

= Commutativity
{synonym}

= Abelian group
{c}
{parent=commutative-property}
{wiki}

= Abelian
{c}
{synonym}

Easily classified as the \x[direct-product-of-groups][direct product] of \x[cyclic-group]{p} of \x[prime-number][prime] order.

= Non-commutative
{parent=abelian-group}

= Symmetry
{parent=group-mathematics}
{wiki}

Directly modelled by \x[group-mathematics]{p}.

For \x[continuous-symmetry]{p}, see: \x[lie-group].

https://en.wikipedia.org/wiki/Generating_set_of_a_group

= Important mathematical group
{parent=group-mathematics}

= Important discrete mathematical group
{parent=important-mathematical-group}

= Cyclic group
{parent=important-discrete-mathematical-group}
{wiki}

= $C_n$
{synonym}
{title2}

= The direct product of two cyclic groups of coprime order is another cyclic group
{parent=important-discrete-mathematical-group}
{wiki}

You just map the value (1, 1) $C_m \times C_n$ to the value 1 of $C_{mn}$, and it works out. E.g. for $C_2 \times C_3$, the \x[generating-set-of-a-group][group generated by] of (1, 1) is:
``
0 = (0, 0)
1 = (1, 1)
2 = (0, 2)
3 = (1, 0)
4 = (0, 1)
5 = (1, 2)
6 = (0, 0) = 0
``

= Permutation
{parent=important-discrete-mathematical-group}
{wiki}

= Cycle notation
{parent=permutation}
{wiki}

A concise to describe a specific \x[permutation].

A permutation group can then be described in terms of the \x[generating-set-of-a-group] of specific elements given in cycle notation.

E.g. https://en.wikipedia.org/w/index.php?title=Mathieu_group&oldid=1034060469#Permutation_groups mentions that the \x[mathieu-group-m-11] is generated by three elements:
* (0123456789a)
* (0b)(1a)(25)(37)(48)(69)
* (26a7)(3945)
which feels quite compact for a \x[simple-group] with 95040 elements, doesn't it!

= Parity of a permutation
{parent=permutation}
{wiki}

= Odd permutation
{parent=parity-of-a-permutation}

= Even permutation
{parent=parity-of-a-permutation}

= Permutation group
{parent=permutation}
{wiki}

= Stabilizer
{disambiguate=group}
{parent=permutation-group}

Suppose we have a given \x[permutation-group] that acts on a set of n elements.

If we pick k elements of the set, the stabilizer subgroup of those k elements is a subgroup of the given permutation group that keeps those elements unchanged.

Note that an analogous definition can be given for non-finite groups. Also note that the case for all finite groups is covered by the permutation definition since \x[all-groups-are-isomorphic-to-a-subgroup-of-the-symmetric-group]

TODO existence and uniqueness. Existence is obvious for the identity permutation, but proper subgroup likely does not exist in general.

Bibliography:
* https://mathworld.wolfram.com/Stabilizer.html
* https://ncatlab.org/nlab/show/stabilizer+group from \x[nlab]

= Symmetric group
{parent=permutation-group}
{wiki}

\x[group]{c} of all \x[permutation]{p}.

= All groups are isomorphic to a subgroup of the symmetric group
{parent=symmetric-group}

Or in other words: \x[symmetric-group]{p} are boring, because they are basically everything already!

= Alternating group
{parent=permutation-group}
{wiki}

= $A_n$
{synonym}
{title2}

Group of \x[even-permutation]{p}.

Note that \x[odd-permutation]{p} don't form a \x[subgroup] of the \x[symmetric-group] like the even permutations do, because the composition of two odd permutations is an even permutation.

= Alternating group of degree 5
{parent=alternating-group}

= The alternating groups of degree 5 or greater are simple
{parent=alternating-group-of-degree-5}

https://www.youtube.com/watch?v=U_618kB6P1Q GT18.2. A_n is Simple (n ge 5) by \x[mathdoctorbob] (2012)

= Dihedral group
{parent=important-discrete-mathematical-group}
{title2=$D_n$}
{wiki}

Our notation: $D_n$, called "dihedral group of degree n", means the dihedral group of the \x[regular-polygon] with $n$ sides, and therefore has order $2n$ (all rotations + flips), called the "dihedral group of \x[order-algebra] 2n".

= Wallpaper group
{parent=important-discrete-mathematical-group}
{wiki}

17 of them.

= Space group
{parent=important-discrete-mathematical-group}
{tag=crystallography}
{wiki}

All possible repetitive crystal structures!

219 of them.

= Klein four-group
{c}
{parent=important-discrete-mathematical-group}
{wiki}

$C_2 \times C_2$

= Finite group
{parent=group-mathematics}

= Classification of finite groups
{parent=finite-group}

As shown in \x[video-simple-groups-abstract-algebra-by-socratica-2018], this can be split up into two steps:
* \x[classification-of-finite-simple-groups]: done
* \x[group-extension-problem]
This split is sometimes called the "Jordan-Hölder program" in reference to the authors of the \x[jordan-holder-theorem].

Good lists to start playing with:

History: https://math.stackexchange.com/questions/1587387/historical-notes-on-the-jordan-h%C3%B6lder-program

= List of finite groups
{parent=classification-of-finite-groups}

* https://en.wikipedia.org/wiki/List_of_small_groups

= GroupNames
{c}
{parent=list-of-finite-groups}
{wiki}

https://people.maths.bris.ac.uk/~matyd/GroupNames/index.html

This dude has done well.

= Classification of finite simple groups
{parent=classification-of-finite-groups}
{wiki}

= Classification of simple finite groups
{synonym}

\x[ciro-santilli] is very fond of this result: \x[the-beauty-of-mathematics].

How can so much complexity come out from so few rules?

How can the proof be so long (thousands of papers)?? Surprise!!

And to top if all off, the awesomely named \x[monster-group] could have a relationship with \x[string-theory] via the \x[monstrous-moonshine]?

\x[all-science-is-either-physics-or-stamp-collecting] comes to mind.

The classification contains:
* \x[cyclic-group]{p}: infinitely many, one for each \x[prime] order. Non-prime orders are not simple. These are the only \x[abelian] ones.
* \x[alternating-group]{p} of order 4 or greater: infinitely many
* \x[groups-of-lie-type]: a contains several infinite families
* \x[sporadic-group]{p}: 26 or 27 of them depending on definitions

\Video[https://www.youtube.com/watch?v=jhVMBXl5jTA]
{title=Simple Groups - Abstract Algebra by Socratica (2018)}
{description=Good quick overview.}

= Group of Lie type
{parent=classification-of-finite-simple-groups}
{wiki}

= Groups of Lie type
{synonym}

In the \x[classification-of-finite-simple-groups], groups of Lie type are a set of infinite families of simple lie groups. These are the other infinite families besides te \x[cyclic-group]{p} and \x[alternating-group]{p}.

A decent list at: https://en.wikipedia.org/wiki/List_of_finite_simple_groups[], https://en.wikipedia.org/wiki/Group_of_Lie_type[] is just too unclear. The groups of Lie type can be subdivided into:
* \x[chevalley-group]{child}{p}
* TODO the rest

The first in this family discovered were a subset of the \x[chevalley-groups-a-n-q] by \x[galois]: \x[psl-2-p], so it might be a good first one to try and understand what it looks like.

TODO understand intuitively why they are called of Lie type. Their names $A_n$, $B_n$ seem to correspond to the members of the \x[classification-of-simple-lie-groups] which are also named like that.

But they are of course related to \x[lie-group]{p}, and as suggested at \x[video-yang-mills-1-by-david-metzler-2011] part 2, the continuity actually simplifies things.

= Chevalley group
{c}
{parent=group-of-lie-type}
{wiki}

= Chevalley groups $A_n(q)$
{c}
{parent=chevalley-group}

They are the \x[finite-projective-special-linear-group]{p}.

This was the first infinite family of \x[simple-group]{p} discovered after the simple \x[cyclic-group]{p} and \x[alternating-group]{p}. The first case discovered was \x[psl-2-p] by \x[galois]. You should understand that one first.

= Sporadic group
{parent=classification-of-finite-simple-groups}
{wiki}

Examples of \x[exceptional-object]{p}{parent}.

= Mathieu group
{c}
{parent=sporadic-group}
{wiki}

Contains the first \x[sporadic-group]{p} discovered by far: 11 and 12 in 1861, and 22, 23 and 24 in 1973. And therefore presumably the simplest! The next sporadic ones discovered were the \x[janko-group]{p}, only in 1965!

Each \x[m-n] is a \x[permutation-group] on $n$ elements. There isn't an obvious algorithmic relationship between $n$ and the actual group.

TODO initial motivation? Why did Mathieu care about \x[k-transitive-group]{p}?

Their; \x[k-transitive-group] properties seem to be the main characterization, according to Wikipedia:
* 22 is 3-transitive but not 4-transitive.
* four of them (11, 12, 23 and 24) are the only \x[sporadic-group][sporadic] \x[k-transitive-group][4-transitive] groups as per the \x[classification-of-4-transitive-groups] (no known simpler proof as of 2021), which sounds like a reasonable characterization. Note that 12 and 25 are also 5 transitive.
Looking at the \x[classification-of-k-transitive-groups] we see that the Mathieu groups are the only families of 4 and 5 transitive groups other than \x[symmetric-group]{p} and \x[alternating-group]{p}. 3-transitive is not as nice, so let's just say it is the \x[stabilizer-group] of $M_23$ and be done with it.

= k-transitive group
{parent=mathieu-group}

TODO why do we care about this?

Note that if a group is k-transitive, then it is also k-1-transitive.

= Classification of k-transitive groups
{parent=k-transitive-group}

TODO this would give a better motivation for the \x[mathieu-group]

Higher transitivity: https://mathoverflow.net/questions/5993/highly-transitive-groups-without-assuming-the-classification-of-finite-simple-g

= 2-transitive group
{parent=classification-of-k-transitive-groups}
{wiki}

= Classification of 2-transitive groups
{parent=2-transitive-group}

= Classification of 3-transitive groups
{parent=classification-of-k-transitive-groups}

Might be a bit complex: https://math.stackexchange.com/questions/698327/classification-of-triply-transitive-finite-groups

= Classification of 4-transitive groups
{parent=classification-of-k-transitive-groups}

https://en.wikipedia.org/w/index.php?title=Mathieu_group&oldid=1034060469#Multiply_transitive_groups is a nice characterization of 4 of the \x[mathieu-group]{p}.

= Classification of 5-transitive groups
{parent=classification-of-k-transitive-groups}

Apparently only \x[mathieu-group-m-12] and \x[mathieu-group-m-24].

http://www.maths.qmul.ac.uk/~pjc/pps/pps9.pdf mentions:
\Q[
The automorphism group of the extended Golay code is the 54-transitive Mathieu group $M_{24}$. This is one of only two finite 5-transitive groups other than symmetric and alternating groups
]
Hmm, is that 54, or more likely 5 and 4?

https://scite.ai/reports/4-homogeneous-groups-EAKY21 quotes https://link.springer.com/article/10.1007%2FBF01111290 which suggests that is is also another one of the Mathieu groups, https://math.stackexchange.com/questions/698327/classification-of-triply-transitive-finite-groups#comment7650505_3721840 and https://en.wikipedia.org/wiki/Mathieu_group_M12 mentions M_12.

= Classification of 6-transitive groups
{parent=classification-of-k-transitive-groups}

https://math.stackexchange.com/questions/700235/is-there-an-easy-proof-for-the-classification-of-6-transitive-finite-groups says there aren't any non-boring ones.

= Mathieu group $M_11$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M11}

= Mathieu group $M_12$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M12}

= Mathieu group $M_22$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M22}

= Mathieu group $M_23$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M23}

= Mathieu group $M_24$
{c}
{parent=mathieu-group}
{wiki=Mathieu_group_M24}

https://math.stackexchange.com/questions/698327/classification-of-triply-transitive-finite-groups

A master thesis reviewing its results: https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=5051&context=etd_theses

= Janko group
{c}
{parent=sporadic-group}
{wiki}

= Monster group
{parent=sporadic-group}
{wiki}

\Video[https://www.youtube.com/watch?v=mH0oCDa74tE]
{title=Group theory, abstraction, and the 196,883-dimensional monster by \x[3blue1brown] (2020)}
{description=Too basic, starts motivating groups themselves, therefore does not give anything new or rare.}

= Monstrous moonshine
{parent=monster-group}
{wiki}

TODO \x[clickbait], or is it that good?

= Jordan-Holder Theorem
{parent=classification-of-finite-simple-groups}
{wiki=Composition_series#Uniqueness:_Jordan–Hölder_theorem}

Uniqueness results for the \x[composition-series] of a group.

= Composition series
{parent=classification-of-finite-simple-groups}
{wiki}

= Group extension problem
{parent=classification-of-finite-groups}
{wiki=Group_extension}

Besides the understandable Wikipedia definition, \x[video-simple-groups-abstract-algebra-by-socratica-2018] gives an understandable one:
\Q[
Given a finite group $F$ and a simple group $S$, find all groups $G$ such that $N$ is a \x[normal-subgroup] of $G$ and $G/N = S$.
]

We don't really know how to make up larger groups from smaller simple groups, which would complete the \x[classification-of-finite-groups]:
* https://math.stackexchange.com/questions/25315/how-is-a-group-made-up-of-simple-groups

In particular, this is hard because you can't just take the \x[direct-product-of-groups] to retrieve the original group: \x[relationship-between-the-quotient-group-and-direct-products]{full}.

= Group operation
{parent=group-mathematics}

= Group product
{synonym}

= Group isomorphism
{parent=group-mathematics}
{wiki}

= Isomorphism
{parent=group-isomorphism}
{wiki}

= Isomorphic
{synonym}

Something analogous to a \x[group-isomorphism], but that preserves whatever properties the given algebraic object has. E.g. for a \x[field-mathematics], we also have to preserve multiplication in addition to addition.

Other common examples include isomorphisms of \x[vector-space]{p} and \x[field-mathematics]{p}. But since both of those two are much simpler than groups in \x[classification-mathematics], as they are both determined by number of elements/dimension alone, see:
* \x[classification-of-finite-fields]
* \x[all-vector-spaces-of-the-same-dimension-on-a-given-field-are-isomorphic]
we tend to not talk about isomorphisms so much in those contexts.

= Group homomorphism
{parent=group-isomorphism}
{wiki}

Like isomorphism, but does not have to be one-to-one: multiple different inputs can have the same output.

The image is as for any function smaller or equal in size as the domain of course.

This brings us to the key intuition about group homomorphisms: they are a way to split out a larger group into smaller groups that retains a subset of the original structure.

As shown by the \x[fundamental-theorem-on-homomorphisms], each group homomorphism is fully characterized by a \x[normal-subgroup] of the domain.

= Fundamental theorem on homomorphisms
{parent=group-homomorphism}
{wiki}

Ultimate explanation: https://math.stackexchange.com/questions/776039/intuition-behind-normal-subgroups/3732426#3732426

Links \x[group-homomorphism] and the \x[quotient-group] via \x[normal-subgroup]{p}.

= Kernel
{disambiguate=algebra}
{parent=group-homomorphism}
{wiki}

= Generating set of a group
{parent=group-mathematics}
{wiki}

= Cayley graph
{c}
{parent=generating-set-of-a-group}
{wiki}

You select a \x[generating-set-of-a-group], and then you name every node with them, and you specify:
* each node by a product of generators
* each edge by what happens when you apply a generator to each element

Not unique: different generating sets lead to different graphs, see e.g. two possible https://en.wikipedia.org/w/index.php?title=Cayley_graph&oldid=1028775401#Examples for the 

= Cycle graph
{disambiguate=algebra}
{parent=cayley-graph}
{wiki}

How to build it: https://math.stackexchange.com/questions/3137319/how-in-general-does-one-construct-a-cycle-graph-for-a-group/3162746#3162746 good answer with \x[ascii-art]. You basically just pick each element, and repeatedly apply it, and remove any path that has a longer version.

Immediately gives the \x[generating-set-of-a-group] by looking at elements adjacent to the origin, and more generally the \x[order-of-an-element-of-a-group][order of each element].

TODO \x[uniqueness]: can two different \x[group]{p} have the same cycle graph? It does not seem to tell us how every element interact with every other element, only with itself. This is in contrast with the \x[cayley-graph], which more accurately describes group structure (but does not give the order of elements as directly), so feels like it won't be unique.

= Cycle of an element of a group
{parent=generating-set-of-a-group}

Take the element and apply it to itself. Then again. And so on.

In the case of a \x[finite-group], you have to eventually reach the \x[identity-element] again sooner or later, giving you the \x[order-of-an-element-of-a-group].

The continuous analogue for the cycle of a group are the \x[one-parameter-subgroup]{p}. In the continuous case, you sometimes reach identity again and to around infinitely many times (which always happens in the finite case), but sometimes you don't.

= Order of an element of a group
{parent=cycle-of-an-element-of-a-group}

The length of its \x[cycle-of-an-element-of-a-group][cycle].

Bibliography:
* https://math.stackexchange.com/questions/972057/calculating-the-order-of-an-element-in-a-group

= Direct product of groups
{parent=group-mathematics}
{title2=$G \times H$}
{wiki}

= Product of group subsets
{parent=direct-product-of-groups}
{wiki}

= Semidirect product
{parent=direct-product-of-groups}
{title2=$N \rtimes H$}
{wiki}

As per https://en.wikipedia.org/w/index.php?title=Semidirect_product&oldid=1040813965#Properties[], unlike the \x[direct-product], the semidirect product of two goups is neither \x[unique], nor does it always \x[exist], and there is no known algorithmic way way to tell if one exists or not.

This is because reaching the "output" of the semidirect produt of two groups requires extra non-obvious information that might not exist. This is because the semi-direct product is based on the \x[product-of-group-subsets]. So you start with two small and completely independent groups, and it is not obvious how to join them up, i.e. how to define the group operation of the product group that is compatible with that of the two smaller input groups. Contrast this with the \x[direct-product], where the composition is simple: just use the group operation of each group on either side.

Product of group subsets

So in other words, it is not a \x[function-mathematics] like the \x[direct-product]. The semidiret product is therefore more like a property of three groups. 

The semidirect product is more general than the \x[direct-product-of-groups] when thinking about the \x[group-extension-problem], because with the \x[direct-product-of-groups], both subgroups of the larger group are necessarily also normal (trivial projection \x[group-homomorphism] on either side), while for the semidirect product, only one of them does.

Conversely, https://en.wikipedia.org/w/index.php?title=Semidirect_product&oldid=1040813965 explains that if $G = N \rtimes H$, and besides the implied requirement that N is normal, H is also normal, then $G = N \times H$.

Smallest example: $D_6 = C_3 \rtimes C_2$ where $D$ is a \x[dihedral-group] and $C$ are \x[cyclic-group]{p}. $C_3$ (the rotation) is a normal subgroup of $D_6$, but $C_2$ (the flip) is not.

Note that with the \x[direct-product] instead we get $C_6$ and not $D_6$, i.e. $C_3 \times C_2 = C_6$ as per \x[the-direct-product-of-two-cyclic-groups-of-coprime-order-is-another-cyclic-group].

TODO:
* why does one of the groups have to be normal in the definition?
* what is the smallest example of a non-\x[simple-group] that is neither a direct nor a semi-direct product of any two other groups?

Bibliography: https://math.stackexchange.com/questions/1726939/is-this-intuition-for-the-semidirect-product-of-groups-correct

= Subgroup
{parent=group-mathematics}
{wiki}

= Subgroup generated by a group
{parent=subgroup}

= Quotient group
{parent=subgroup}
{wiki}

Ultimate explanation: https://math.stackexchange.com/questions/776039/intuition-behind-normal-subgroups/3732426#3732426

Does not have to be isomorphic to a subgroup:
* https://www.mathcounterexamples.net/a-semi-continuous-function-with-a-dense-set-of-points-of-discontinuity/
* https://math.stackexchange.com/questions/2498922/is-a-quotient-group-a-subgroup
This is one of the reasons why the analogy between \x[simple-group]{p} of finite groups and \x[prime-number]{p} is limited.

= Subquotient
{parent=quotient-group}
{wiki}

Quotient of a subgroup H of G by a \x[normal-subgroup] of the subgroup H.

That \x[normal-subgroup] does not have have to be a normal subgroup of G.

As an overkill example, the happy family are subquotients of the \x[monster-group], but the monster group is simple.

= Relationship between the quotient group and direct products
{parent=quotient-group}

Although quotients look a bit real number division, there are some important differences with the "group analog of multiplication" of \x[direct-product-of-groups].

If a group is isomorphic to the \x[direct-product-of-groups], we can take a quotient of the product to retrieve one of the groups, which is somewhat analogous to division: https://math.stackexchange.com/questions/723707/how-is-the-quotient-group-related-to-the-direct-product-group

The "converse" is not always true however: a group does not need to be isomorphic to the product of one of its \x[normal-subgroup]{p} and the associated \x[quotient-group]. The wiki page provides an example:
\Q[
Given G and a normal subgroup N, then G is a group extension of G/N by N. One could ask whether this extension is trivial or split; in other words, one could ask whether G is a direct product or semidirect product of N and G/N. This is a special case of the extension problem. An example where the extension is not split is as follows: Let $G = Z4 = {0, 1, 2, 3}$, and $ = {0, 2}$ which is isomorphic to Z2. Then G/N is also isomorphic to Z2. But Z2 has only the trivial automorphism, so the only semi-direct product of N and G/N is the direct product. Since Z4 is different from Z2 × Z2, we conclude that G is not a semi-direct product of N and G/N.
]

TODO find a less minimal but possibly more important example.

This is also semi mentioned at: https://math.stackexchange.com/questions/1596500/when-is-a-group-isomorphic-to-the-product-of-normal-subgroup-and-quotient-group

I think this might be equivalent to why the \x[group-extension-problem] is hard. If this relation were true, then taking the direct product would be the only way to make larger groups from normal subgroups/quotients. But it's not.

= Normal subgroup
{parent=quotient-group}
{wiki}

Ultimate explanation: https://math.stackexchange.com/questions/776039/intuition-behind-normal-subgroups/3732426#3732426

Only normal subgroups can be used to form \x[quotient-group]{p}: their key definition is that they plus their cosets form a group.

Intuition:
* https://math.stackexchange.com/questions/776039/intuition-behind-normal-subgroups
* https://math.stackexchange.com/questions/1014535/is-there-any-intuitive-understanding-of-normal-subgroup/1014791

One key intuition is that "a normal subgroup is the \x[kernel-algebra]" of a \x[group-homomorphism], and the normal subgroup plus cosets are isomorphic to the image of the isomorphism, which is what the \x[fundamental-theorem-on-homomorphisms] says.

Therefore "there aren't that many \x[group-homomorphism]", and a normal subgroup it is a concrete and natural way to uniquely represent that homomorphism.

The best way to think about the, is to always think first: what is the homomorphism? And then work out everything else from there.

= Simple group
{parent=normal-subgroup}
{wiki}

Does not have any non-trivial \x[normal-subgroup].

And therefore, going back to our intuition that due to the \x[fundamental-theorem-on-homomorphisms] there is one normal group per homomorphism, a simple group is one that has no non-trivial homomorphisms.

= How to show that a group is simple
{parent=simple-group}
{wiki}

https://math.stackexchange.com/questions/203168/proving-a-group-is-simple

https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=5051&context=etd_theses proves that the \x[mathieu-group-m-24] is simple in just 200 pages. Nice.

Examples:
* \x[the-alternating-groups-of-degree-5-or-greater-are-simple]

= Ring
{disambiguate=mathematics}
{parent=group-mathematics}
{wiki}

A \x[group-mathematics] with an extra operation called multiplication which satisfies:
* \x[associative-property]
* distributes over addition (the default group operation)
* has an identity

Unlike addition, that multiplication does not need to satisfy:
* \x[commutative-property]. If this is satisfied, we can call it a \x[commutative-ring].
* existence of an inverse. If this is satisfied, we can call it a \x[division-ring].
If those are also satisfied, then we have a \x[field-mathematics].

The simplest example of a ring which is not a full fledged \x[field-mathematics] and with \x[commutative] multiplication are the \x[integer]{p}. Notably, no inverses exist except for the identity itself and -1. E.g. the inverse of 2 would be 1/2 which is not in the \x[set-mathematics].

A \x[polynomial-ring] is another example with the same properties as the \x[integer]{p}.

The simplest non-commutative ring that is not a \x[field-mathematics] is the set of all 2x2 \x[matrix]{p} of \x[real-number]{p}:
* we know that 2x2 matrix multiplication is non-commutative in general
* some 2x2 matrices have a multiplicative inverse, but others don't
Note that \x[gl-n] is not a ring because you can by addition reach the zero matrix. 

= Commutative ring
{parent=ring-mathematics}
{wiki}

= Division ring
{parent=ring-mathematics}
{wiki}

Two ways to see it:
* a \x[ring-mathematics] where \x[inverse]{p} exist
* a \x[field-mathematics] where multiplication is not necessarily \x[commutative]

= Field
{disambiguate=mathematics}
{parent=ring-mathematics}
{wiki}

A \x[ring-mathematics] where multiplication is \x[commutative] and there is always an inverse.

A field can be seen as an \x[abelian-group] that has two group operations: addition and multiplication, and they are compatible (distributive property).

Basically the nicest, least restrictive, 2-operation type of \x[algebra].

Examples:
* \x[real-number]{p}
* \x[rational-number]{p}

= Distributive property
{parent=field-mathematics}
{wiki}

One of the defining properties of \x[algebraic-structure] with two operations such as \x[ring-mathematics] and \x[field-mathematics]:
$$a(b + c) = ab + ac$$
This property shows how the two operations interact.

= Finite field
{parent=field-mathematics}
{title2=$GF(n)$}
{wiki}

A convenient notation for the elements of $GF(n)$ of prime order is to use \x[integer]{p}, e.g. for $GF(7)$ we could write:
$$
GR(7) = \{-3, -2, -1, 0, 1, 2, 3\}
$$
which makes it clear what is the additive inverse of each element, although sometimes a notation starting from 0 is also used:
$$
GR(7) = \{0, 1, 2, 3, 4, 5, 6\}
$$

For fields of \x[prime] order, regular \x[modular-arithmetic] works as the field operation.

For non-prime order, we see that \x[modular-arithmetic] does not work because the divisors have no inverse. E.g. at order 6, 2 and 3 have no inverse, e.g. for 2:
$$
0 \times 2 = 0
1 \times 2 = 2
2 \times 2 = 4
3 \times 2 = 0
4 \times 2 = 2
5 \times 2 = 4
$$
we see that things wrap around perfecly, and 1 is never reached.

For non-prime \x[prime-power] orders however, we can find a way, see \x[finite-field-of-non-prime-order].

\Video[https://www.youtube.com/watch?v=z9bTzjy4SCg]
{title=Finite fields made easy by Randell Heyman (2015)}
{description=Good introduction with examples}

= Classification of finite fields
{parent=finite-field}

There's exactly one field per \x[prime-power], So all we need to specify a field is give its order, notated e.g. as $GF(n)$.

Every element of a finite field satisfies $x^{order} = x$.

It is interesting to compare this result philosophically with the \x[classification-of-finite-groups]: fields are more constrained as they have to have two operations, and this leads to a much simpler classification!

= Finite field of non-prime order
{parent=finite-field}

As per \x[classification-of-finite-fields] those must be of \x[prime-power] order.

\x[video-finite-fields-made-easy-by-randell-heyman-2015] at https://youtu.be/z9bTzjy4SCg?t=159 shows how for order $9 = 3 \times 3$. Basically, for order $p^n$, we take:
* each element is a polynomial in $GF(p)[x]$, $GF(p)[x]$, the \x[polynomial-over-a-field][polynomial ring over the finite field $GF(p)$] with degree smaller than $n$. We've just seen how to construct $GF(p)$ for prime $p$ above, so we're good there.
* addition works element-wise modulo on $GF(p)$
* multiplication is done modulo an \x[irreducible-polynomial] of order $n$
For a worked out example, see: \x[gf-4].

= GF(2)
{c}
{parent=finite-field}
{wiki}

= GF(4)
{c}
{parent=finite-field}
{wiki}

\x[ciro-santilli] tried to https://en.wikipedia.org/w/index.php?title=Finite_field&type=revision&diff=1044934168&oldid=1044905041[add this example to Wikipedia], but revert, so here we are.

This is a good first example of a field of a \x[finite-field-of-non-prime-order], this one is a \x[prime-power] order instead.

$4 = 2^2$, so one way to represent the elements of the field will be the to use the 4 polynomials of degree 1 over \x[gf-2]:
* 0X + 0
* 0X + 1
* 1X + 0
* 1X + 1

Note that we refer in this definition to anther field, but that is fine, because we only refer to fields of \x[prime] order such as \x[gf-2], because we are dealing with \x[prime-power]{p} only. And we have already defined fields of prime order easily previously with \x[modular-arithmetic].

Over GF(2), there is only one \x[irreducible-polynomial] of degree 2:
$$X^2+X+1$$

Addition is defined element-wise with \x[modular-arithmetic] modulo 2 as defined over GF(2), e.g.:
$$(1X + 0) + (1X + 1) = (1 + 1)X + (0 + 1) = 0X + 1$$

Multiplication is done modulo $X^2+X+1$, which ensures that the result is also of degree 1.

For example first we do a regular multiplication:
$$(1X + 0) \times (1X + 1) = (1 \times 1)X^2 + (1 \times 1)X + (0 \times 1)X + (0 \times 1) = 1X^2 + 1X + 0$$

Without modulo, that would not be one of the elements of the field anymore due to the $1X^2$!

So we take the modulo, we note that:
$$1X^2 + 1X + 0 = 1(X^2+X+1) + (0X + 1)$$
and by the definition of modulo:
$$(1X^2 + 1X + 0) \mod (X^2+X+1) = (0X + 1)$$
which is the final result of the multiplication.

TODO show how taking a reducible polynomial for modulo fails. Presumably it is for a similar reason to why things fail for the prime case.

= Vector field
{parent=field-mathematics}
{wiki}

= Algebra over a field
{parent=vector-field}
{wiki}

A \x[vector-field] with a \x[bilinear-map] into itself, which we can also call a "vector product".

Note that the vector product does not have to be neither \x[associative] nor \x[commutative].

Examples: https://en.wikipedia.org/w/index.php?title=Algebra_over_a_field&oldid=1035146107#Motivating_examples
* \x[complex-number]{p}, i.e. \x[r-2] with complex number multiplication
* \x[r-3] with the \x[cross-product]
* \x[quaternion]{p}, i.e. \x[r-4] with the quaternion multiplication

= Division algebra
{parent=algebra-over-a-field}
{wiki}

An \x[algebra-over-a-field] where \x[division] exists.

= Frobenius theorem
{c}
{disambiguate=real division algebras}
{parent=division-algebra}
{wiki}

= Classification of associative real division algebras
{synonym}

There are 3: \x[real-number]{p}, \x[complex-number]{p} and \x[quaternion]{p}.

Notably, the \x[octonion]{p} are not \x[associative].

= Associative property
{parent=algebra}
{wiki}

= Associative
{synonym}
