= Artificial intelligence
{wiki}

= AI
{c}
{title2}
{synonym}

= Artificial general intelligence
{parent=artificial-intelligence}
{wiki}

= AGI
{c}
{synonym}
{title2}

Given enough computational power per dollar, AGI is inevitable, but it is not sure certain ever happen given the end of \x[moore-s-law][end of Moore's Law].

Alternatively, it could also be achieved genetically modified biological brains + \x[brain-in-a-vat].

Imagine a brain the size of a building, perfectly engineered to solve certain engineering problems, and giving hints to human operators + taking feedback from cameras and audio attached to the operators.

This likely implies \x[transhumanism], and \x[mind-uploading].

\x[ciro-santilli] joined the silicon industry at one point to help increase our computational capacity and reach AGI.

Ciro believes that the easiest route to full AI, if any, could involve \x[ciro-s-2d-reinforcement-learning-games].

= Instrumental goal
{c}
{parent=artificial-general-intelligence}

= Instrumental convergence
{c}
{parent=instrumental-goal}
{wiki}

= AI alignment
{c}
{parent=artificial-intelligence}
{wiki}

As highlighted e.g. at \x[human-compatible-by-stuart-j-russell-2019], this AI alignment intrisically linked to the idea of \x[utility] in \x[economy].

= AI safety
{c}
{parent=ai-alignment}

Basically ensuring that good \x[ai-alignment] allows us to survive the singularity.

= AI complete
{c}
{parent=artificial-intelligence}
{wiki}

= AI training game
{c}
{tag=serious-game}
{parent=artificial-intelligence}

\x[ciro-santilli] took a stab at: \x[ciro-s-2d-reinforcement-learning-games], but he didn't sink too much/enough into that project.

= gvgai
{c}
{parent=ai-training-game}

http://www.gvgai.net/

Similar goals to \x[ciro-s-2d-reinforcement-learning-games], but they were focusing mostly on discrete games.

The group kind of died circa 2020 it seems, a shame.

= Can AGI be trained in simulations?
{c}
{parent=ai-training-game}

Or is real word data necessary, e.g. with \x[robot]{p}?

Fundamental question related to \x[ciro-s-2d-reinforcement-learning-games].

Bibliograpy:
* https://youtu.be/i0UyKsAEaNI?t=120 How to Build AGI? Ilya Sutskever interview by Lex Fridman (2020)

= OpenAI
{c}
{parent=ai-training-game}
{wiki}

\Q[In 2019, OpenAI transitioned from non-profit to for-profit]
so what's that point of Open in the name anymore??
* https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/ "The AI moonshot was founded in the spirit of transparency. This is the inside story of how competitive pressure eroded that idealism."
* https://archive.ph/wXBtB How OpenAI Sold its Soul for \$1 Billion
* https://www.reddit.com/r/GPT3/comments/n2eo86/is_gpt3_open_source/

= OpenAI Gym
{c}
{parent=openai}

https://github.com/openai/gym

= Artificial intelligence bibliography
{c}
{parent=artificial-intelligence}
{wiki}

= Human compatible by Stuart J. Russell (2019)
{c}
{parent=artificial-intelligence-bibliography}
{tag=ai-alignment}
{tag=good-book}
{wiki=Human_compatible}

The key takeaway is that setting an explicit \x[value-function] to an \x[agi] entity is a good way to destroy the world due to poor \x[ai-alignment]. We are more likely to not destroy by creating an AI whose goals is to "do what humans what it to do", but in a way that it does not know before hand what it is that humans want, and it has to learn from them.

Some other cool ideas:
* a big thing that is missing for AGI in the 2010's is some kind of more hierarchical representation of the continuous input data of the world, e.g.:
  * when we behave, we do things in subroutines. E.g. life goal: save hunger. Subgoal: apply for some grant. Subsubgoal: eat, sleep, take shower. Subsub goal: move muscles to get me to table and open a can.
  * we can group continuous things into higher objects, e.g. all these pixels I'm seeing in front of me are a computer. So I treat all of them as a single object in my mind.
* \x[game-theory] can be seen as part of \x[artificial-intelligence] that deals with scenarios where multiple intelligent agents are involved
* \x[economy], and notably the study of the \x[utility], is intrinsically linked to \x[ai-alignment]

= Superintelligence by Nick Bostrom (2014)
{c}
{parent=artificial-intelligence-bibliography}
{wiki=Superintelligence:_Paths,_Dangers,_Strategies}

Good points:
* \x[post-mortem-connectome-extraction-with-microtome]{c}

Technological singularity

= Artificial general intelligence software
{parent=artificial-intelligence}

= OpenCog
{c}
{parent=artificial-general-intelligence-software}
{wiki}

= Ben Goertzel
{c}
{parent=opencog}
{wiki}

https://www.reddit.com/r/artificial/comments/b38hbk/what_do_my_fellow_ai_researchers_think_of_ben/ What do my fellow AI researchers think of Ben Goertzel and his research? 
